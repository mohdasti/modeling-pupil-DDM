# Validation Fix: Using Posterior Predictive Checks (PPC)

**Date:** 2025-11-26  
**Issue:** Aggregation bias (Jensen's Inequality) in validation  
**Solution:** Use PPC instead of analytical formula with mean parameters

---

## The Problem

Our previous validation used an **analytical formula with mean parameters**:

$$P(\text{upper}) = \frac{e^{-2va(1-z)} - 1}{e^{-2va} - 1}$$

Where we plugged in:
- $v = \text{mean drift} = -1.260$
- $a = \text{mean boundary} = 2.275$
- $z = \text{mean bias} = 0.567$

**Result:** Predicted 3.6% "Different", but observed 10.9% "Different" (7.3% mismatch).

---

## Why This Was Wrong: Aggregation Bias (Jensen's Inequality)

For **non-linear functions**, the average of probabilities is NOT equal to the probability of averages:

$$E[P(v, a, z)] \neq P(E[v], E[a], E[z])$$

**In hierarchical models:**
- We have **67 subjects** with different drift rates
- Some subjects have strong negative drift ($v \approx -1.26$)
- Some subjects have weaker drift ($v \approx -0.5$)
- A subject with $v=-0.5$ produces **exponentially more** "Different" errors

**The analytical formula simulates a single "average" human**, ignoring subject heterogeneity. The model correctly captures this heterogeneity through random effects, but our validation ignored it!

---

## The Solution: Posterior Predictive Checks (PPC)

**PPC properly validates the model by:**
1. Simulating data from the **full posterior** (including random effects)
2. Respecting subject-level heterogeneity
3. Aggregating simulated data to compare with observed

**This is the correct way to validate hierarchical models!**

---

## Implementation

Created `R/validate_ppc_proper.R` which:

1. **Loads model and data**
2. **Generates posterior predictions** for Standard trials using `posterior_predict()`
   - Uses 500 draws from posterior
   - Includes all subject-level random effects
3. **Extracts predicted choices** from RT values (sign indicates boundary)
4. **Calculates proportions** for each draw
5. **Compares observed to 95% credible interval** of predicted distribution

**If observed falls within 95% CI, the model fits correctly!**

---

## Expected Results

The PPC validation should show:
- **Observed:** 10.9% "Different"
- **Predicted 95% CI:** Should include 10.9%
- **Interpretation:** Model accurately captures data distribution

The 7.3% "mismatch" we saw before was just an artifact of using mean parameters instead of the full posterior.

---

## Files Created

1. **`R/validate_ppc_proper.R`** - Proper PPC validation script
2. **`VALIDATION_FIX_PPC.md`** - This document

---

## Next Steps

1. Run `R/validate_ppc_proper.R` to validate the model properly
2. If validation passes (observed within 95% CI), proceed with analysis
3. Update validation logic in model fitting scripts to use PPC
4. Document this in manuscript methods section

---

## Key Takeaway

**Never validate hierarchical models using mean parameters in non-linear formulas!**

Always use Posterior Predictive Checks (PPC) that respect the full posterior distribution and random effects.

---

**Status:** Ready to run PPC validation âœ…

