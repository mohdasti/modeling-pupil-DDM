---
title: "Chapter 3 — Diffusion Modeling with Pupil-Linked Arousal (Response-Signal Design)"
author: "Mohammad Dastgheib"
bibliography: references.bib
csl: apa-7th-edition.csl
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
    embed-resources: true
  pdf:
    toc: true
    toc-depth: 3
    number-sections: true
    documentclass: article
    geometry:
      - margin=1in
    fontsize: 11pt
    linestretch: 1.5
    fig-pos: "H"
  docx:
    toc: true
    number-sections: true

execute:
  echo: false
  warning: false
  message: false
---

```{r setup}
library(dplyr)
library(readr)
library(tidyr)
library(gt)
library(glue)
library(stringr)
library(knitr)
op <- options(width = 120)
on.exit(options(op), add = TRUE)

`%||%` <- function(x, y) if (is.null(x) || length(x) == 0) y else x

# Set working directory to project root (Quarto runs from reports/ directory)
if (basename(getwd()) == "reports") {
  setwd("..")
}
# Ensure we're in project root for file paths
proj_root <- getwd()
p <- file.path(proj_root, "output/publish")

# Helper: figure path resolution
# Returns PNG for HTML output, PDF for DOCX output
fig_path <- function(fig_name) {
  # Get base name without extension
  base_name <- tools::file_path_sans_ext(fig_name)
  
  # Detect output format - try multiple methods
  output_format <- NULL
  
  # Method 1: Check knitr's pandoc output format (most reliable)
  pandoc_to <- knitr::opts_knit$get("rmarkdown.pandoc.to")
  if (!is.null(pandoc_to)) {
    output_format <- pandoc_to
  }
  
  # Method 2: Check if we're in a DOCX rendering context
  # Quarto sets this in the knitr options
  if (is.null(output_format)) {
    # Check for DOCX-specific indicators
    docx_indicators <- c(
      knitr::opts_knit$get("quarto.document.format") == "docx",
      grepl("docx", knitr::opts_knit$get("out.format") %||% "", ignore.case = TRUE)
    )
    if (any(docx_indicators, na.rm = TRUE)) {
      output_format <- "docx"
    }
  }
  
  png_name <- paste0(base_name, ".png")
  pdf_name <- paste0(base_name, ".pdf")
  
  # Find the actual file location
  png_abs <- NULL
  pdf_abs <- NULL
  
  candidates_png <- c(
    file.path(proj_root, "output/figures", png_name),
    file.path("output/figures", png_name),
    file.path("../output/figures", png_name)
  )
  
  candidates_pdf <- c(
    file.path(proj_root, "output/figures", pdf_name),
    file.path("output/figures", pdf_name),
    file.path("../output/figures", pdf_name)
  )
  
  for (path in candidates_png) {
    if (file.exists(path)) {
      png_abs <- normalizePath(path)
      break
    }
  }
  
  for (path in candidates_pdf) {
    if (file.exists(path)) {
      pdf_abs <- normalizePath(path)
      break
    }
  }
  
  # Choose format based on output type
  # DOCX: prefer PDF (vector graphics work better in Word)
  # HTML: prefer PNG (better browser compatibility)
  if (!is.null(output_format) && grepl("docx|word", output_format, ignore.case = TRUE)) {
    if (!is.null(pdf_abs)) {
      # For DOCX, return relative path (Word handles it)
      return(file.path("../output/figures", pdf_name))
    } else if (!is.null(png_abs)) {
      return(file.path("../output/figures", png_name))
    }
  } else {
    # HTML: use relative path from reports/ directory
    # Quarto will handle copying if needed
    if (!is.null(png_abs)) {
      return(file.path("../output/figures", png_name))
    } else if (!is.null(pdf_abs)) {
      return(file.path("../output/figures", pdf_name))
    }
  }
  
  # Fallback
  warning("Figure not found: ", fig_name)
  return(file.path("../output/figures", if (!is.null(png_abs)) png_name else pdf_name))
}

# Helper: safe read
sread <- function(path) {
  if (file.exists(path)) {
    tryCatch(read_csv(path, show_col_types = FALSE), error = function(e) NULL)
  } else {
    NULL
  }
}

# --- Load all data files ---
# QA files
qa_exclusions <- sread(file.path(p, "qa_trial_exclusions.csv"))
qa_subj       <- sread(file.path(p, "qa_subject_inclusion.csv"))
qa_decision   <- sread(file.path(p, "qa_decision_coding_audit.csv"))
qa_mvc        <- sread(file.path(p, "qa_mvc_compliance.csv"))

# Manipulation checks
acc_glmm <- sread(file.path(p, "checks_accuracy_glmm.csv"))
rt_lmm   <- sread(file.path(p, "checks_rt_lmm.csv"))

# LOO comparison
loo_primary <- sread(file.path(p, "loo_summary_clean.csv")) %||% 
               sread(file.path(p, "table1_loo_primary.csv")) %||% 
               sread("loo_difficulty_all.csv")

# Convergence and publish gate
conv_primary <- sread(file.path(p, "table2_convergence_primary.csv"))
publish_gate <- sread(file.path(p, "publish_gate_primary_censored.csv"))

# PPC tables
ppc_subj_cens   <- sread(file.path(p, "table3_ppc_primary_subjectwise_censored.csv"))
ppc_uncond_cens <- sread(file.path(p, "table3_ppc_primary_unconditional_censored.csv"))
ppc_cond_cens   <- sread(file.path(p, "table3_ppc_primary_conditional_censored.csv"))
ppc_gate        <- sread(file.path(p, "ppc_gate_summary.csv"))
ppc_cells       <- sread(file.path(p, "ppc_cells_detail.csv"))

# Fixed effects and contrasts
fx_table        <- sread(file.path(p, "table_fixed_effects.csv"))
contrasts_table <- sread(file.path(p, "table_effect_contrasts.csv"))

# Heatmap tables
heatmap_wide <- sread(file.path(p, "ppc_heatmap_wide.csv"))
heatmap_long <- sread(file.path(p, "ppc_heatmap_long.csv"))

# Bias model results (response-side coding)
bias_levels_std <- sread(file.path(p, "bias_standard_only_levels.csv"))
bias_contr_std <- sread(file.path(p, "bias_standard_only_contrasts.csv"))
bias_contr_joint <- sread(file.path(p, "bias_joint_contrast.csv"))
ppc_joint_minimal <- sread(file.path(p, "ppc_joint_minimal.csv"))

# Sensitivity analyses
sensitivity_summary <- sread("output/checks/sensitivity_summary.csv") %||%
                      sread("output/sensitivity/sensitivity_summary.csv")

# Derive simple pass/fail summaries
pf_subj <- if (!is.null(ppc_subj_cens)) {
  if ("any_flag" %in% names(ppc_subj_cens)) {
    ppc_subj_cens %>% 
      summarise(n_cells = n(), n_flagged = sum(any_flag, na.rm=TRUE), 
                pct_flagged = round(100*mean(any_flag, na.rm=TRUE),1))
  } else {
    ppc_subj_cens %>% 
      mutate(any_flag = (qp_rmse_midbody > 0.09) | (ks_mean > 0.15)) %>%
      summarise(n_cells = n(), n_flagged = sum(any_flag, na.rm=TRUE), 
                pct_flagged = round(100*mean(any_flag, na.rm=TRUE),1))
  }
} else tibble(n_cells=NA, n_flagged=NA, pct_flagged=NA)

pf_uncond <- if (!is.null(ppc_uncond_cens)) {
  if ("any_flag" %in% names(ppc_uncond_cens)) {
    ppc_uncond_cens %>% 
      summarise(n_cells = n(), n_flagged = sum(any_flag, na.rm=TRUE), 
                pct_flagged = round(100*mean(any_flag, na.rm=TRUE),1))
  } else {
    ppc_uncond_cens %>% 
      mutate(any_flag = (qp_rmse_midbody > 0.09) | (ks_mean > 0.15)) %>%
      summarise(n_cells = n(), n_flagged = sum(any_flag, na.rm=TRUE), 
                pct_flagged = round(100*mean(any_flag, na.rm=TRUE),1))
  }
} else tibble(n_cells=NA, n_flagged=NA, pct_flagged=NA)
```

# Overview

This chapter presents a hierarchical Wiener diffusion decision model (DDM) for a response-signal change-detection task in older adults. The primary model maps task difficulty to drift rate (v), boundary separation (a), and starting-point bias (z), with small condition effects on non-decision time (t₀). We report comprehensive quality assurance checks, manipulation checks independent of the DDM, model comparison via LOO cross-validation, and extensive posterior predictive checks with emphasis on subject-wise mid-body RT quantiles.

# Sample & Experimental Design

## Participants

67 older adults (≥65 years; mean age = 71.3 years, SD = 4.8). This analysis uses the same dataset and participants as described in the LC behavioral report manuscript (see References). All participants provided written informed consent in accordance with the Institutional Review Board protocol.\
*Note*: 12 participants performed at or below chance (≤55%) in some conditions but were retained to maximize sample size, as hierarchical modeling borrows strength to stabilize their estimates. Sensitivity analyses confirmed their inclusion did not alter main effects.

## Tasks and Conditions

**Tasks**: Auditory Detection Task (ADT) and Visual Detection Task (VDT) were modeled jointly with 'task' as a fixed effect. This approach uses a single random effect variance parameter for subject-level variability across both tasks, allowing the model to share information between tasks and stabilize subject-specific estimates through hierarchical shrinkage while estimating task-specific offsets. *\[Detailed task descriptions, stimulus parameters, and equipment specifications are provided in the LC behavioral report manuscript; see References.\]*

**Conditions** (within-subjects, factorial design):

-   **Difficulty**: Standard (Δ=0), Easy, Hard
-   **Effort**: Low (5% MVC), High (40% MVC)

**Total design cells**: 2 tasks × 3 difficulty levels × 2 effort conditions = 12 cells per subject.

**Total trials analyzed**: 17,243 (after exclusions).

## Trial Timeline (Response-Signal Design)

```{r, fig.cap="Task design. Both Auditory and Visual tasks followed the same trial structure, but differed in stimuli. Each trial began with a 1.5-4.5 s jittered blank gray screen, followed by a 3 s grip gauge instructing low (5% MVC) or high (40% MVC) force with on-screen feedback. After a 0.25 s blank and 0.5 s fixation, a stimulus pair was presented. Auditory task: a 1000 Hz 0.1 s standard tone, a 0.5 s ISI, then a 0.1 s comparison tone that was either identical or increased by 8, 16, 32, or 64 Hz. Visual task: a central Gabor (1.5 cycles/degree, 0.2 Michelson contrast, 4°) followed by a 0.5 s blank and a second Gabor that was identical or increased in contrast by 0.06, 0.12, 0.24, or 0.48. Participants then released the grip and had 3 s to report \"same\" or \"different,\" followed by 3 s to rate their confidence on their same/different choice (1-4, low to high). All 5 stimulus levels and the 2 grip levels were presented in equal proportions across 150 trials per task, and were presented pseudorandomly.", out.width="100%", fig.align="center"}
knitr::include_graphics(fig_path("Trial_Structure.png"))
```

**RT definition**: Time from response-screen onset (response-signal design). This is a critical methodological detail: RTs are measured from when the response screen appears (after the stimulus presentation period), not from stimulus onset. This design constrains the interpretation of **t₀ (non-decision time) to primarily reflect motor execution and response selection** rather than the sum of encoding + motor time as in traditional RT tasks. The response-signal design rationale is described in detail in the LC behavioral report manuscript.

**Filtering**: RTs \< 0.250 s were excluded as anticipations. While a 150--200 ms cutoff is standard for young adult populations [@whelan2008effective], research consistently demonstrates that older adults exhibit significantly longer non-decision times ($T_{er}$), reflecting age-related slowing in stimulus encoding and motor execution. Specifically, drift diffusion modeling in aging populations estimates that $T_{er}$ is approximately 80--100 ms longer in older adults compared to their younger counterparts [@ratcliff2001aging; @ratcliff2004aging]. Consequently, a 250 ms threshold provides a conservative lower bound that adjusts for this physiological shift, ensuring that excluded trials represent genuine non-decisional reflexes rather than the leading edge of the valid decision distribution [@woods2015age]. The upper bound of 3.000 s reflects the maximum response window in the task design; no upper-bound filtering was applied post-experiment.

# Design & Data Quality Assurance

## Trial Exclusions

Trial exclusions were applied during data preprocessing. The following table summarizes exclusions by filter type:

```{r}
# Create trial exclusion summary table
trial_exclusions_summary <- tibble(
  Filter_Type = c("Starting trials", "RT < 200 ms", "Missed responses", "Invalid run performance", "Final trials (Preprocessing)", "Restored (Audit)", "Final Analysis N"),
  Trials_Remaining = c(19740, 19495, 19194, 16958, 16958, 17243, 17243),
  Trials_Removed = c(0, 245, 301, 2236, 2782, -285, 0),
  Percentage_Remaining = c(100.0, 98.76, 97.23, 85.91, 85.91, 87.35, 87.35)
) %>%
  mutate(
    Percentage_Removed = round(100 - Percentage_Remaining, 2)
  )

trial_exclusions_summary %>%
  select(Filter_Type, Trials_Remaining, Trials_Removed, Percentage_Remaining, Percentage_Removed) %>%
  gt() %>%
  tab_header(title = md("**Trial Exclusions Summary**")) %>%
  cols_label(
    Filter_Type = "Filter Applied",
    Trials_Remaining = "Trials Remaining",
    Trials_Removed = "Trials Removed",
    Percentage_Remaining = "% Remaining",
    Percentage_Removed = "% Removed"
  ) %>%
  fmt_number(columns = c(Trials_Remaining, Trials_Removed), decimals = 0) %>%
  fmt_number(columns = c(Percentage_Remaining, Percentage_Removed), decimals = 2) %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_body(rows = Filter_Type == "Final Analysis N")
  ) %>%
  tab_footnote(
    footnote = "RT < 200 ms: Anticipatory responses excluded during preprocessing. The DDM analysis used a 250 ms cutoff, but no additional trials were excluded. 285 trials were restored after a decision coding audit confirmed their validity.",
    locations = cells_body(rows = Filter_Type == "RT < 200 ms")
  )
```

**Summary**: Of 19,740 starting trials, 2,782 trials (14.1%) were excluded during preprocessing: - **245 trials (1.2%)** excluded for RT \< 200 ms (anticipatory responses) - **301 trials (1.5%)** excluded for missed responses - **2,236 trials (11.3%)** excluded for invalid run performance

**Final dataset after preprocessing**: 16,958 trials (85.9% retention) from 65 subjects. Additional data processing steps (e.g., decision coding verification, quality checks) resulted in the final analysis dataset of 17,243 trials from 67 subjects. Note: The DDM analysis applies a more conservative 250 ms lower-bound cutoff (see Filtering section below) based on age-related non-decision time shifts, but no additional trials were excluded as all remaining trials had RT ≥ 250 ms.

## Subject Inclusion & Decision Coding Audit

```{r}
# Subject inclusion summary
if (!is.null(qa_subj)) {
  n_subj <- nrow(qa_subj)
  n_sub_chance <- sum(qa_subj$sub_chance_flag, na.rm = TRUE)
  mean_acc <- mean(qa_subj$acc_overall, na.rm = TRUE) * 100
  cat("**Subject Inclusion:**\n")
  cat("- Total subjects:", n_subj, "\n")
  cat("- Sub-chance performers (≤55% accuracy):", n_sub_chance, "\n")
  cat("- Mean overall accuracy:", sprintf("%.1f%%", mean_acc), "\n\n")
}

# Decision coding audit
if (!is.null(qa_decision)) {
  cat("**Decision Coding Audit:**\n")
  cat("- Total trials:", format(qa_decision$n, big.mark=","), "\n")
  cat("- Decision coding mismatches:", qa_decision$mismatches, "\n")
  cat("- Mismatch rate:", sprintf("%.4f", qa_decision$mismatch_rate), "\n")
}
```

**Result**: All 67 subjects were retained; no sub-chance performers were excluded. Decision coding verification confirmed zero mismatches across all trials. Decision coding methodology is discussed in detail in the Model Specification section below.

# Manipulation Checks (Independent of DDM)

To confirm the experimental manipulations worked as intended, we conducted mixed-effects analyses on accuracy and RT *independent of any DDM assumptions*. **Important**: These analyses are restricted to Easy and Hard trials only (excluding Standard trials). Standard trials are "same" trials (Δ=0), while Easy and Hard are "different" trials with varying stimulus offsets. The difficulty manipulation is only meaningful within "different" trials, where Easy trials use large frequency/contrast offsets and Hard trials use small offsets.

For the manipulation check, we test whether both experimental manipulations work as intended by comparing (1) Easy vs Hard trials for the difficulty manipulation, and (2) Low vs High effort for the effort manipulation, pooled across both tasks (ADT and VDT). This approach validates both core experimental manipulations while maximizing statistical power. Task differences (VDT shows higher accuracy than ADT) are present but are secondary to validating the manipulations themselves.

## Accuracy: Generalized Linear Mixed Model

**Model**: `decision ~ difficulty + effort + (1 | subject)`, restricted to Easy and Hard trials only (N = 13,771 trials, pooled across ADT and VDT). Reference levels: Easy, Low_5_MVC.

```{r}
if (!is.null(acc_glmm)) {
  # Function to clean up term names for publication (vectorized)
  clean_term_name <- function(term) {
    term <- as.character(term)
    
    # Intercept
    term <- ifelse(term == "(Intercept)", "Intercept", term)
    
    # Step 1: Handle interactions FIRST (on raw R terms)
    # Difficulty × Task interactions
    term <- gsub("difficulty_levelEasy:taskVDT", "Difficulty: Easy × Task: VDT", term)
    term <- gsub("difficulty_levelEasy:taskADT", "Difficulty: Easy × Task: ADT", term)
    term <- gsub("difficulty_levelHard:taskVDT", "Difficulty: Hard × Task: VDT", term)
    term <- gsub("difficulty_levelHard:taskADT", "Difficulty: Hard × Task: ADT", term)
    
    # Effort × Task interactions
    term <- gsub("effort_conditionLow_5_MVC:taskVDT", "Effort: Low (5% MVC) × Task: VDT", term)
    term <- gsub("effort_conditionLow_5_MVC:taskADT", "Effort: Low (5% MVC) × Task: ADT", term)
    term <- gsub("effort_conditionHigh_MVC:taskVDT", "Effort: High (40% MVC) × Task: VDT", term)
    term <- gsub("effort_conditionHigh_MVC:taskADT", "Effort: High (40% MVC) × Task: ADT", term)
    term <- gsub("effort_conditionLow:taskVDT", "Effort: Low × Task: VDT", term)
    term <- gsub("effort_conditionLow:taskADT", "Effort: Low × Task: ADT", term)
    term <- gsub("effort_conditionHigh:taskVDT", "Effort: High × Task: VDT", term)
    term <- gsub("effort_conditionHigh:taskADT", "Effort: High × Task: ADT", term)
    
    # Difficulty × Effort interactions
    term <- gsub("difficulty_levelEasy:effort_conditionLow_5_MVC", "Difficulty: Easy × Effort: Low (5% MVC)", term)
    term <- gsub("difficulty_levelEasy:effort_conditionHigh_MVC", "Difficulty: Easy × Effort: High (40% MVC)", term)
    term <- gsub("difficulty_levelHard:effort_conditionLow_5_MVC", "Difficulty: Hard × Effort: Low (5% MVC)", term)
    term <- gsub("difficulty_levelHard:effort_conditionHigh_MVC", "Difficulty: Hard × Effort: High (40% MVC)", term)
    term <- gsub("difficulty_levelEasy:effort_conditionLow", "Difficulty: Easy × Effort: Low", term)
    term <- gsub("difficulty_levelEasy:effort_conditionHigh", "Difficulty: Easy × Effort: High", term)
    term <- gsub("difficulty_levelHard:effort_conditionLow", "Difficulty: Hard × Effort: Low", term)
    term <- gsub("difficulty_levelHard:effort_conditionHigh", "Difficulty: Hard × Effort: High", term)
    
    # Step 2: Handle main effects (only for terms NOT already processed as interactions)
    # Check if term is NOT already an interaction (doesn't contain ×)
    main_effect_mask <- !grepl(" × ", term)
    
    # Main effects (preserve colons - these separate factor name from level)
    term[main_effect_mask] <- gsub("difficulty_levelEasy", "Difficulty: Easy", term[main_effect_mask])
    term[main_effect_mask] <- gsub("difficulty_levelHard", "Difficulty: Hard", term[main_effect_mask])
    term[main_effect_mask] <- gsub("effort_conditionLow_5_MVC", "Effort: Low", term[main_effect_mask])
    term[main_effect_mask] <- gsub("effort_conditionHigh_MVC", "Effort: High", term[main_effect_mask])
    term[main_effect_mask] <- gsub("effort_conditionLow", "Effort: Low", term[main_effect_mask])
    term[main_effect_mask] <- gsub("effort_conditionHigh", "Effort: High", term[main_effect_mask])
    term[main_effect_mask] <- gsub("taskVDT", "Task: VDT", term[main_effect_mask])
    term[main_effect_mask] <- gsub("taskADT", "Task: ADT", term[main_effect_mask])
    
    # Step 3: For any remaining raw R interactions (not caught above), convert colon to ×
    # Only if term still has raw R format colons (between factor names, not in cleaned format)
    raw_interaction_mask <- grepl("_condition|_level|task", term) & grepl(":", term) & !grepl(" × ", term)
    term[raw_interaction_mask] <- gsub(":", " × ", term[raw_interaction_mask])
    
    return(term)
  }
  
  acc_glmm %>%
    filter(effect == "fixed") %>%
    mutate(
      term_clean = clean_term_name(term),
      estimate_fmt = sprintf("%.2f", estimate),
      p_value_fmt = ifelse(p.value < .001, "<.001", sprintf("%.3f", p.value)),
      ci_fmt = sprintf("[%.2f, %.2f]", conf.low, conf.high)
    ) %>%
    select(term_clean, estimate_fmt, std.error, statistic, p_value_fmt, ci_fmt) %>%
    rename(term = term_clean) %>%
    gt() %>%
    tab_header(title = md("**Accuracy GLMM Results**")) %>%
    cols_label(
      term = "Term",
      estimate_fmt = "β",
      std.error = "SE",
      p_value_fmt = "p",
      ci_fmt = "95% CI"
    )
}
```

**Key findings**:

-   **Hard vs. Easy**: Hard trials showed substantially lower accuracy than Easy (β = -2.97, *p* \< .001). Easy trials had 85.2% accuracy, while Hard trials had 30.5% accuracy (well below chance). This reflects the increased difficulty of detecting small frequency/contrast differences on "different" trials, demonstrating a strong effect of stimulus difference magnitude on discrimination performance.
-   **High vs. Low Effort**: High effort (40% MVC) showed slightly lower accuracy than Low effort (5% MVC) (β = -0.15, *p* = .001). Low effort had 58.5% accuracy, while High effort had 56.8% accuracy. This suggests that the increased physical effort required for High effort trials may interfere with cognitive performance, potentially due to dual-task resource competition between maintaining grip force and performing the discrimination task.

## RT: Linear Mixed Model on Median RT

**Model**: `rt_median ~ difficulty + effort + (1 | subject)`, restricted to Easy and Hard trials only (N = 13,771 trials, pooled across ADT and VDT). Reference levels: Easy, Low_5_MVC.

```{r}
if (!is.null(rt_lmm)) {
  # Function to clean up term names for publication (same as accuracy model, vectorized)
  clean_term_name <- function(term) {
    term <- as.character(term)
    
    # Intercept
    term <- ifelse(term == "(Intercept)", "Intercept", term)
    
    # Step 1: Handle interactions FIRST (on raw R terms)
    # Difficulty × Task interactions
    term <- gsub("difficulty_levelEasy:taskVDT", "Difficulty: Easy × Task: VDT", term)
    term <- gsub("difficulty_levelEasy:taskADT", "Difficulty: Easy × Task: ADT", term)
    term <- gsub("difficulty_levelHard:taskVDT", "Difficulty: Hard × Task: VDT", term)
    term <- gsub("difficulty_levelHard:taskADT", "Difficulty: Hard × Task: ADT", term)
    
    # Effort × Task interactions
    term <- gsub("effort_conditionLow_5_MVC:taskVDT", "Effort: Low (5% MVC) × Task: VDT", term)
    term <- gsub("effort_conditionLow_5_MVC:taskADT", "Effort: Low (5% MVC) × Task: ADT", term)
    term <- gsub("effort_conditionHigh_MVC:taskVDT", "Effort: High (40% MVC) × Task: VDT", term)
    term <- gsub("effort_conditionHigh_MVC:taskADT", "Effort: High (40% MVC) × Task: ADT", term)
    term <- gsub("effort_conditionLow:taskVDT", "Effort: Low × Task: VDT", term)
    term <- gsub("effort_conditionLow:taskADT", "Effort: Low × Task: ADT", term)
    term <- gsub("effort_conditionHigh:taskVDT", "Effort: High × Task: VDT", term)
    term <- gsub("effort_conditionHigh:taskADT", "Effort: High × Task: ADT", term)
    
    # Difficulty × Effort interactions
    term <- gsub("difficulty_levelEasy:effort_conditionLow_5_MVC", "Difficulty: Easy × Effort: Low (5% MVC)", term)
    term <- gsub("difficulty_levelEasy:effort_conditionHigh_MVC", "Difficulty: Easy × Effort: High (40% MVC)", term)
    term <- gsub("difficulty_levelHard:effort_conditionLow_5_MVC", "Difficulty: Hard × Effort: Low (5% MVC)", term)
    term <- gsub("difficulty_levelHard:effort_conditionHigh_MVC", "Difficulty: Hard × Effort: High (40% MVC)", term)
    term <- gsub("difficulty_levelEasy:effort_conditionLow", "Difficulty: Easy × Effort: Low", term)
    term <- gsub("difficulty_levelEasy:effort_conditionHigh", "Difficulty: Easy × Effort: High", term)
    term <- gsub("difficulty_levelHard:effort_conditionLow", "Difficulty: Hard × Effort: Low", term)
    term <- gsub("difficulty_levelHard:effort_conditionHigh", "Difficulty: Hard × Effort: High", term)
    
    # Step 2: Handle main effects (only for terms NOT already processed as interactions)
    # Check if term is NOT already an interaction (doesn't contain ×)
    main_effect_mask <- !grepl(" × ", term)
    
    # Main effects (preserve colons - these separate factor name from level)
    term[main_effect_mask] <- gsub("difficulty_levelEasy", "Difficulty: Easy", term[main_effect_mask])
    term[main_effect_mask] <- gsub("difficulty_levelHard", "Difficulty: Hard", term[main_effect_mask])
    term[main_effect_mask] <- gsub("effort_conditionLow_5_MVC", "Effort: Low", term[main_effect_mask])
    term[main_effect_mask] <- gsub("effort_conditionHigh_MVC", "Effort: High", term[main_effect_mask])
    term[main_effect_mask] <- gsub("effort_conditionLow", "Effort: Low", term[main_effect_mask])
    term[main_effect_mask] <- gsub("effort_conditionHigh", "Effort: High", term[main_effect_mask])
    term[main_effect_mask] <- gsub("taskVDT", "Task: VDT", term[main_effect_mask])
    term[main_effect_mask] <- gsub("taskADT", "Task: ADT", term[main_effect_mask])
    
    # Step 3: For any remaining raw R interactions (not caught above), convert colon to ×
    # Only if term still has raw R format colons (between factor names, not in cleaned format)
    raw_interaction_mask <- grepl("_condition|_level|task", term) & grepl(":", term) & !grepl(" × ", term)
    term[raw_interaction_mask] <- gsub(":", " × ", term[raw_interaction_mask])
    
    return(term)
  }
  
  rt_lmm %>%
    filter(effect == "fixed") %>%
    mutate(
      term_clean = clean_term_name(term),
      estimate_fmt = sprintf("%.3f", estimate),
      ci_fmt = sprintf("[%.3f, %.3f]", conf.low, conf.high)
    ) %>%
    select(term_clean, estimate_fmt, std.error, statistic, ci_fmt) %>%
    rename(term = term_clean) %>%
    gt() %>%
    tab_header(title = md("**RT LMM Results**")) %>%
    cols_label(
      term = "Term",
      estimate_fmt = "β (seconds)",
      std.error = "SE",
      ci_fmt = "95% CI"
    )
}
```

**Key findings**:

-   **Hard vs. Easy**: Hard trials were slower than Easy (β = 0.23 s, 95% CI \[0.20, 0.27\]). Easy trials had a median RT of 0.75 s (mean 0.90 s), while Hard trials had a median RT of 1.01 s (mean 1.12 s). This reflects slower decision-making when stimulus differences are smaller and harder to detect.
-   **High vs. Low Effort**: High effort showed no significant difference in RT compared to Low effort (β = 0.02 s, 95% CI \[-0.02, 0.05\]). Low effort had a median RT of 0.86 s (mean 1.00 s), while High effort had a median RT of 0.89 s (mean 1.02 s). The effort manipulation did not significantly affect reaction time, suggesting that the dual-task demands primarily affected accuracy rather than response speed.

**Conclusion**: Both experimental manipulations worked as intended. The **difficulty manipulation** (Easy vs. Hard within "different" trials) showed strong effects on both accuracy and RT in theoretically expected directions: larger stimulus differences (Easy) led to higher accuracy (85.2% vs. 30.5%) and faster RTs (0.75 s vs. 1.01 s median) compared to smaller differences (Hard). The **effort manipulation** (Low vs. High MVC) showed a small but significant effect on accuracy, with High effort slightly reducing accuracy (56.8% vs. 58.5%), likely due to dual-task resource competition. However, effort did not significantly affect RT. These results validate the experimental design prior to DDM analysis.

# Model Specification

## Decision Coding (Response-Side)

We redefined the decision boundary such that the **upper boundary corresponds to "different"** and the **lower boundary to "same"**. This response-side coding is critical for identifying bias independently of correctness. On Standard (Δ=0) trials, participants chose "same" on 87.8% of trials and "different" on 12.2%—consistent with a conservative response tendency. The transformation from accuracy-based coding (dec=1 = correct) to response-side coding (dec_upper=1 = "different") was verified across all trials with zero mismatches.

## Computational Environment

All analyses were performed using R version 4.5.2 (2025-10-31) "\[Not\] Part in a Rumble" [@R] on macOS (aarch64-apple-darwin20, Apple Silicon). Bayesian hierarchical models were fitted using `brms` [@burkner2017brms; @burkner2018brms] with CmdStan [@cmdstan2024] via `cmdstanr` [@cmdstanr] as the backend. Model comparison was conducted using leave-one-out cross-validation via the `loo` package [@vehtari2017loo]. Data manipulation and visualization used `dplyr` [@dplyr], `tidyr` [@tidyr], `readr` [@readr], and `ggplot2` [@wickham2016ggplot2]. Tables were generated using `gt` [@gt]. Posterior analysis and diagnostics used the `posterior` package [@burkner2022posterior]. Code development and debugging were performed using Cursor (AI-assisted code editor), and the document was rendered using Quarto [@quarto].

**MCMC Sampling Specifications:**
- Algorithm: NUTS (No-U-Turn Sampler)
- Chains: 4
- Iterations: 8,000 per chain (4,000 warmup, 4,000 sampling)
- Convergence criteria: $\hat{R}$ ≤ 1.01 [@gelman1992inference; @vehtari2021rank], minimum bulk/tail ESS ≥ 400

## DDM Family and Links

**Family**: `wiener(link_bs="log", link_ndt="log", link_bias="logit")`

**Links**:

-   Drift rate (v): identity link
-   Boundary separation (a/bs): log link
-   Non-decision time (t₀/ndt): log link
-   Starting-point bias (z): logit link

## Mathematical Specification

The drift diffusion model (DDM) models decision-making as a noisy evidence accumulation process described by a Wiener diffusion process [@ratcliff1978theory; @ratcliff2008diffusion]. The evidence accumulation process follows the stochastic differential equation:

$$dX(t) = v \, dt + dW(t)$$

where $X(t)$ is the accumulated evidence at time $t$, $v$ is the drift rate (evidence strength), and $dW(t)$ is a Wiener noise process with unit variance. The process starts at $X(0) = z \cdot a$, where $z \in [0,1]$ is the starting-point bias (expressed as a proportion of boundary separation) and $a > 0$ is the boundary separation.

The decision process terminates when $X(t)$ reaches either the upper boundary ($X(t) = a$, corresponding to "different" responses) or the lower boundary ($X(t) = 0$, corresponding to "same" responses). The total reaction time (RT) is the sum of the decision time (time to reach a boundary) and the non-decision time ($t_0$):

$$\text{RT} = t_{\text{decision}} + t_0$$

where $t_0$ reflects motor execution and response selection time (in the response-signal design, this primarily captures post-signal processing and motor output).

**Parameter Transformations (Link Functions):**

To ensure parameter constraints, we apply link functions:

-   **Drift rate**: $v = \beta_v$ (identity link)
-   **Boundary separation**: $a = \exp(\beta_{\text{bs}})$ (log link, ensures $a > 0$)
-   **Non-decision time**: $t_0 = \exp(\beta_{\text{ndt}})$ (log link, ensures $t_0 > 0$)
-   **Starting-point bias**: $z = \text{logit}^{-1}(\beta_{\text{bias}}) = \frac{\exp(\beta_{\text{bias}})}{1 + \exp(\beta_{\text{bias}})}$ (logit link, ensures $z \in [0,1]$)

**Hierarchical Structure:**

For subject $i$ and trial $j$, the model parameters are:

$$v_{ij} = \beta_{v,0} + \sum_k \beta_{v,k} X_{k,ij} + u_{v,i}$$

$$a_{ij} = \exp\left(\beta_{\text{bs},0} + \sum_k \beta_{\text{bs},k} X_{k,ij} + u_{\text{bs},i}\right)$$

$$t_{0,ij} = \exp\left(\beta_{\text{ndt},0} + \sum_k \beta_{\text{ndt},k} X_{k,ij}\right)$$

$$z_{ij} = \text{logit}^{-1}\left(\beta_{\text{bias},0} + \sum_k \beta_{\text{bias},k} X_{k,ij} + u_{\text{bias},i}\right)$$

where $\beta_{0}$ are population-level intercepts, $\beta_k$ are population-level coefficients for predictors $X_k$ (e.g., task, effort condition, difficulty level), and $u_i \sim \mathcal{N}(0, \sigma^2_u)$ are subject-level random effects. Note that $t_0$ is modeled without subject-level random effects to maintain model stability in the response-signal design.

**Likelihood Function:**

The likelihood for a single trial with RT $t$ and decision $d \in \{\text{"same"}, \text{"different"}\}$ follows the Wiener first-passage time distribution [@feller1968introduction]:

$$p(t, d | v, a, t_0, z) = \text{Wiener}(t - t_0 | v, a, z)$$

where the Wiener distribution gives the probability density of the first-passage time to boundary $d$ given drift $v$, boundary separation $a$, and starting point $z \cdot a$.

## DDM Process Overview

```{r, fig.cap="Drift Diffusion Model: Evidence Accumulation Process. The DDM models decision-making as a noisy evidence accumulation process. Evidence accumulates from a starting point (z, starting-point bias) toward one of two decision boundaries: the upper boundary corresponds to \"different\" responses, and the lower boundary to \"same\" responses. The rate of evidence accumulation is determined by the drift rate (v), which reflects the quality of the stimulus evidence. The distance between boundaries is the boundary separation (a), which reflects response caution. The total reaction time (RT) is the sum of non-decision time (t₀, reflecting motor execution and response selection in the response-signal design) and the decision time (time to reach a boundary). In this example, the starting point is biased toward the upper boundary (z > 0.5), and positive drift leads to a \"different\" response.", out.width="90%", fig.align="center"}
knitr::include_graphics(fig_path("fig_ddm_process.pdf"))
```

## Standard-Only Bias Calibration

To isolate bias identification from drift, we fit a hierarchical Wiener DDM to **Standard trials only** (3,472 trials from 67 subjects) with a tight drift prior to enforce near-zero evidence:

-   **Drift (v)**: `rt | dec(decision) ~ 1 + (1|subject_id)` with prior `normal(0, 0.03)` to enforce v ≈ 0
-   **Boundary (a/bs)**: `bs ~ 1 + (1|subject_id)` — intercept + subject random effects
-   **Non-decision time (t₀/ndt)**: `ndt ~ 1` — intercept-only (response-signal design)
-   **Bias (z)**: `bias ~ task + effort_condition + (1|subject_id)` — task/effort effects + subject random effects

This isolates bias identification from drift, as Standard (Δ=0) trials should have zero evidence accumulation.

## Joint Model (Confirmation)

A full hierarchical model using all trials (17,243 trials) constrained Standard drift to ≈0 (tight prior `normal(0, 0.04)`) and allowed drift differences only for non-Standard trials (Easy/Hard) via an `is_nonstd` indicator:

-   **Drift (v)**: `rt | dec(decision) ~ 0 + difficulty_level + task:is_nonstd + effort_condition:is_nonstd + (1|subject_id)` — separate coefficients per difficulty, task/effort effects only for non-Standard
-   **Boundary (a/bs)**: `bs ~ difficulty_level + task + (1|subject_id)` — difficulty + task effects + subject random effects
-   **Non-decision time (t₀/ndt)**: `ndt ~ task + effort_condition` — task/effort effects, no random effects
-   **Bias (z)**: `bias ~ difficulty_level + task + (1|subject_id)` — difficulty + task effects + subject random effects

This joint model confirms the bias estimates from the Standard-only model while providing additional information about difficulty effects.

## Formulas (Primary Model - Original Analysis)

The primary model includes difficulty effects on v, a, and z, with task and effort as additive factors:

-   **Drift (v)**: `rt | dec(decision) ~ difficulty_level + task + effort_condition + (1 + difficulty_level | subject_id)`
-   **Boundary (a/bs)**: `bs ~ difficulty_level + task + (1 | subject_id)`
-   **Non-decision time (t₀/ndt)**: `ndt ~ task + effort_condition` *(no random effects)*
-   **Bias (z)**: `bias ~ difficulty_level + task + (1 | subject_id)`

**Rationale for ndt formula**: In the response-signal design, t₀ primarily reflects motor execution. To avoid identifiability issues and maintain model stability, we modeled t₀ with group-level task and effort effects only, omitting subject-level random effects. The response-signal task design and its implications for DDM parameter interpretation are described in the LC behavioral report manuscript (see References).

## Priors (Link Scale)

All priors are weakly informative and set on the link scale:

**Intercepts**:

-   v Intercept \~ Normal(0, 1)
-   bs Intercept \~ Normal(log(1.7), 0.30) → a ≈ 1.7 on natural scale
-   ndt Intercept \~ Normal(log(0.23), 0.12) → t₀ ≈ 230 ms on natural scale
-   bias Intercept \~ Normal(0, 0.5) → z ≈ 0.5 (no bias) on probability scale

**Slopes**:

-   v slopes: Normal(0, 0.6–0.7)
-   bs slopes: Normal(0, 0.25–0.30)
-   bias slopes: Normal(0, 0.35)

**Random effects**:

-   Standard deviations: Student-t(3, 0, 0.30)
-   Correlations: LKJ(2)

**Sampling controls**: NUTS with `adapt_delta = 0.995`, `max_treedepth = 15`. Four chains, 8,000 iterations (4,000 warmup).

## Prior vs. Posterior for Non-Decision Time

```{r, fig.cap="t₀ Prior vs Posterior. Prior (gray line) and posterior (blue shaded density) distributions for the t₀ intercept. The prior is Normal(log(0.23), 0.12) on the log scale (≈0.23 s on natural scale). This figure documents prior influence for the response-signal design, where t₀ primarily reflects motor execution rather than encoding time.", out.width="85%", fig.align="center"}
knitr::include_graphics(fig_path("fig_ndt_prior_posterior.pdf"))
```

**Interpretation**: The posterior for t₀ is well-informed by the data while remaining compatible with the weakly informative prior, confirming adequate identifiability for the group-level intercept despite the response-signal design.

# Model Comparison (LOO Cross-Validation)

We compared 10 candidate models varying in how difficulty, task, and effort map onto DDM parameters. Leave-one-out cross-validation (LOO-CV) was used to select the best-fitting model.

## LOO Summary Table

```{r}
if (!is.null(loo_primary)) {
  # Function to clean model names for publication
  clean_model_name <- function(model) {
    model <- as.character(model)
    
    # Single parameter models
    model <- ifelse(model == "v", "Difficulty → v (drift)", model)
    model <- ifelse(model == "z", "Difficulty → z (bias)", model)
    model <- ifelse(model == "a", "Difficulty → a (boundary)", model)
    
    # Two-parameter combinations
    model <- ifelse(model == "v_z", "Difficulty → v + z", model)
    model <- ifelse(model == "v_a", "Difficulty → v + a", model)
    model <- ifelse(model == "z_a", "Difficulty → z + a", model)
    
    # Three-parameter combination
    model <- ifelse(model == "v_z_a", "Difficulty → v + a + z", model)
    model <- ifelse(model == "v_a_z", "Difficulty → v + a + z", model)
    
    # Model number formats
    model <- gsub("Model3_Difficulty", "Difficulty → v (drift)", model)
    model <- gsub("Model4_Additive", "Additive (v + a + z)", model)
    model <- gsub("Model5_Interaction", "Interaction", model)
    model <- gsub("Model10_Param_v_bs", "v + a parameterized", model)
    
    # Remove common prefixes
    model <- gsub("^Difficulty_on_", "", model)
    model <- gsub("^difficulty_", "", model)
    
    # Format underscores as separators
    model <- gsub("_", " ", model)
    
    return(model)
  }
  
  loo_primary %>%
    mutate(
      model_clean = clean_model_name(model),
      across(where(is.numeric), ~round(.x, 2))
    ) %>%
    select(model_clean, elpd, se, p_loo) %>%
    rename(model = model_clean) %>%
    gt() %>%
    tab_header(title = md("**Model Comparison: LOO-CV Results**")) %>%
    cols_label(
      model = "Model",
      elpd = "ELPD",
      se = "SE",
      p_loo = "P_loo"
    ) %>%
    tab_style(
      style = cell_fill(color = "#E8F4F8"),
      locations = cells_body(rows = 1)
    )
} else {
  cat("LOO comparison data not available.")
}
```

**Winner**: The model with **difficulty → (v + a + z)** is strongly favored.

-   **ΔELPD vs. v-only**: ≈ +185 (SE ≈ 20)
-   **Stacking weight**: ≈ 0.89
-   **PBMA weight**: ≈ 1.0

**Pareto-k diagnostics**: 1/17,243 observations had k \> 0.7; moment matching was not required.

```{r, fig.cap="Model Comparison: Leave-One-Out Cross-Validation. ELPD (Expected Log-Predictive Density) with 95% SE bars by model. The best model (highest ELPD) is indicated with a dashed red line. ΔELPD values (difference from best) are annotated above each point. Larger ELPD indicates better out-of-sample predictive accuracy.", out.width="90%", fig.align="center"}
knitr::include_graphics(fig_path("fig_loo.pdf"))
```

**Interpretation**: The data strongly support a model in which task difficulty modulates drift rate, boundary separation, *and* starting-point bias simultaneously. Simpler models (e.g., difficulty affecting only drift) are decisively rejected by cross-validation.

# Convergence & Diagnostics

```{r}
if (!is.null(publish_gate)) {
  publish_gate %>%
    mutate(across(where(is.numeric), ~round(.x, 3))) %>%
    gt() %>%
    tab_header(title = md("**Convergence & PPC Gate (Primary Model)**"))
} else {
  cat("Publish gate data not available.")
}
```

**Convergence criteria**:

-   Max $\hat{R}$ ≤ 1.01 ✓
-   Min bulk ESS ≥ 400 ✓
-   Min tail ESS ≥ 400 ✓
-   Divergent transitions = 0 ✓

**PPC thresholds** (pre-declared):

-   Subject-wise mid-body QP RMSE ≤ 0.09 s
-   \|Δ accuracy\| ≤ 0.05
-   KS statistic ≤ 0.15
-   ≤ 15% of cells flagged

**Result**: The primary model passes all MCMC convergence gates ($\hat{R}$, ESS, divergent transitions). PPC performance is discussed in detail below.

# Fixed Effects & Posterior Contrasts

## Bias Results (Standard-Only Model)

```{r}
if (!is.null(bias_levels_std)) {
  bias_levels_std %>%
    filter(scale == "prob") %>%
    mutate(
      param_label = case_when(
        param == "bias_ADT_Low" ~ "ADT, Low effort",
        param == "bias_ADT_High" ~ "ADT, High effort",
        param == "bias_VDT_Low" ~ "VDT, Low effort",
        param == "bias_VDT_High" ~ "VDT, High effort",
        TRUE ~ param
      )
    ) %>%
    select(param_label, mean, q2.5, q97.5) %>%
    gt() %>%
    tab_header(title = md("**Bias Levels (z parameter, natural scale)**")) %>%
    cols_label(
      param_label = "Condition",
      mean = "Mean",
      q2.5 = "2.5%",
      q97.5 = "97.5%"
    ) %>%
    fmt_number(columns = c(mean, q2.5, q97.5), decimals = 3)
} else {
  cat("Bias levels data not available.")
}
```

```{r}
if (!is.null(bias_contr_std)) {
  # Function to clean contrast names
  clean_contrast_name <- function(contrast) {
    contrast <- as.character(contrast)
    
    # Common contrast patterns
    contrast <- gsub("difficulty_levelHard", "Difficulty: Hard", contrast)
    contrast <- gsub("difficulty_levelEasy", "Difficulty: Easy", contrast)
    contrast <- gsub("difficulty_levelStandard", "Difficulty: Standard", contrast)
    contrast <- gsub("taskVDT", "Task: VDT", contrast)
    contrast <- gsub("taskADT", "Task: ADT", contrast)
    contrast <- gsub("effort_conditionLow_Force_5pct", "Effort: Low", contrast)
    contrast <- gsub("effort_conditionHigh_Force_40pct", "Effort: High", contrast)
    contrast <- gsub("effort_conditionLow", "Effort: Low", contrast)
    contrast <- gsub("effort_conditionHigh", "Effort: High", contrast)
    
    # Replace operators
    contrast <- gsub(" - ", " vs. ", contrast)
    contrast <- gsub(" -", " vs.", contrast)
    contrast <- gsub("- ", "vs. ", contrast)
    
    return(contrast)
  }
  
  bias_contr_std %>%
    mutate(
      contrast_clean = clean_contrast_name(contrast),
      across(where(is.numeric), ~round(.x, 3))
    ) %>%
    select(contrast_clean, mean, q2.5, q97.5, Pr_gt_0) %>%
    rename(contrast = contrast_clean) %>%
    gt() %>%
    tab_header(title = md("**Bias Contrasts (Standard-Only Model)**")) %>%
    cols_label(
      contrast = "Contrast",
      mean = "Mean Δ (logit)",
      q2.5 = "2.5%",
      q97.5 = "97.5%",
      Pr_gt_0 = "P(Δ>0)"
    )
} else {
  cat("Bias contrasts data not available.")
}
```

## Fixed Effects: Forest Plots by Task

```{r, fig.cap="Fixed Effects: ADT (Auditory Detection Task). Posterior means (link scale) with 95% CrIs for drift (v), boundary separation (a/bs), and starting-point bias (z). In the additive model, difficulty and effort contrasts are identical for both tasks; only the intercepts differ.", out.width="90%", fig.align="center"}
knitr::include_graphics(fig_path("fig_fixed_effects_ADT.pdf"))
```

```{r, fig.cap="Fixed Effects: VDT (Visual Detection Task). Posterior means (link scale) with 95% CrIs for drift (v), boundary separation (a/bs), and starting-point bias (z). In the additive model, difficulty and effort contrasts are identical for both tasks; only the intercepts differ.", out.width="90%", fig.align="center"}
knitr::include_graphics(fig_path("fig_fixed_effects_VDT.pdf"))
```

## Fixed Effects Summary Table

```{r}
if (!is.null(fx_table)) {
  # Function to clean DDM parameter names for publication (robust, vectorized version)
  clean_ddm_parameter <- function(param) {
    param <- as.character(param)
    original <- param
    n <- length(param)
    result <- character(n)
    
    for (i in seq_len(n)) {
      p <- param[i]
      orig <- original[i]
      
      # Determine parameter type from prefix
      param_type <- ""
      if (grepl("^bs_", p)) {
        param_type <- "Boundary (a)"
        p <- gsub("^bs_", "", p)
      } else if (grepl("^bias_", p)) {
        param_type <- "Bias (z)"
        p <- gsub("^bias_", "", p)
      } else if (grepl("^ndt_", p)) {
        param_type <- "Non-decision time (t₀)"
        p <- gsub("^ndt_", "", p)
      } else if (grepl("^b_", p)) {
        param_type <- "Drift (v)"
        p <- gsub("^b_", "", p)
      } else {
        # Try to infer from content
        if (grepl("bs|boundary", orig, ignore.case = TRUE)) {
          param_type <- "Boundary (a)"
        } else if (grepl("ndt|non.decision", orig, ignore.case = TRUE)) {
          param_type <- "Non-decision time (t₀)"
        } else if (grepl("bias", orig, ignore.case = TRUE)) {
          param_type <- "Bias (z)"
        } else {
          param_type <- "Drift (v)"  # Default
        }
      }
      
      # Handle Intercept
      if (p == "Intercept" || orig == "Intercept") {
        result[i] <- paste0(param_type, ": Intercept")
        next
      }
      
      # Step 1: Check if this is an interaction (has colon between different factors in raw R format)
      # Interactions have colons between factor patterns like difficulty_level:task or effort_condition:task
      is_raw_interaction <- grepl("_level:", p) || grepl("_condition:", p) || 
                           (grepl("difficulty_level", p) && grepl(":task", p)) ||
                           (grepl("effort_condition", p) && grepl(":task", p)) ||
                           (grepl("difficulty_level", p) && grepl(":effort_condition", p))
      
      if (is_raw_interaction) {
        # Handle interactions: split by colon between factors, clean each part, join with ×
        # Pattern: difficulty_levelHard:taskVDT -> split -> clean -> "Difficulty: Hard × Task: VDT"
        if (grepl(":", p) && !grepl(" × ", p)) {
          # Split by colon (assuming only one colon between factors for interactions)
          parts <- strsplit(p, ":")[[1]]
          # Clean each part
          cleaned_parts <- character(length(parts))
          for (j in seq_along(parts)) {
            part <- parts[j]
            # Clean difficulty
            part <- gsub("difficulty_levelEasy", "Difficulty: Easy", part, ignore.case = TRUE)
            part <- gsub("difficulty_levelHard", "Difficulty: Hard", part, ignore.case = TRUE)
            part <- gsub("difficulty_levelStandard", "Difficulty: Standard", part, ignore.case = TRUE)
            # Clean task
            part <- gsub("^taskVDT$", "Task: VDT", part, ignore.case = TRUE)
            part <- gsub("^taskADT$", "Task: ADT", part, ignore.case = TRUE)
            # Clean effort
            part <- gsub("^effort_conditionLow_Force_5pct$", "Effort: Low", part, ignore.case = TRUE)
            part <- gsub("^effort_conditionLow_MVC$", "Effort: Low", part, ignore.case = TRUE)
            part <- gsub("^effort_conditionLow$", "Effort: Low", part, ignore.case = TRUE)
            part <- gsub("^effort_conditionHigh_Force_40pct$", "Effort: High", part, ignore.case = TRUE)
            part <- gsub("^effort_conditionHigh_MVC$", "Effort: High", part, ignore.case = TRUE)
            part <- gsub("^effort_conditionHigh$", "Effort: High", part, ignore.case = TRUE)
            cleaned_parts[j] <- part
          }
          # Join with ×
          p <- paste(cleaned_parts, collapse = " × ")
        }
      } else {
        # Step 2: Main effects - clean normally (preserve colons)
        # Clean difficulty levels (flexible matching)
        p <- gsub("difficulty_levelEasy", "Difficulty: Easy", p, ignore.case = TRUE)
        p <- gsub("difficulty_levelHard", "Difficulty: Hard", p, ignore.case = TRUE)
        p <- gsub("difficulty_levelStandard", "Difficulty: Standard", p, ignore.case = TRUE)
        
        # Clean task
        p <- gsub("taskVDT", "Task: VDT", p, ignore.case = TRUE)
        p <- gsub("taskADT", "Task: ADT", p, ignore.case = TRUE)
        
        # Clean effort conditions (handle all variations - order matters!)
        p <- gsub("effort_conditionLow_Force_5pct", "Effort: Low", p, ignore.case = TRUE)
        p <- gsub("effort_conditionLow_MVC", "Effort: Low", p, ignore.case = TRUE)
        p <- gsub("effort_conditionLow", "Effort: Low", p, ignore.case = TRUE)
        p <- gsub("effort_conditionHigh_Force_40pct", "Effort: High", p, ignore.case = TRUE)
        p <- gsub("effort_conditionHigh_MVC", "Effort: High", p, ignore.case = TRUE)
        p <- gsub("effort_conditionHigh", "Effort: High", p, ignore.case = TRUE)
      }
      
      # Add parameter type prefix if we have meaningful content
      if (p != orig && param_type != "") {
        # Check if param already starts with a parameter type (avoid duplication)
        if (!grepl("^(Drift|Boundary|Non-decision|Bias)", p)) {
          result[i] <- paste0(param_type, ": ", p)
        } else {
          result[i] <- p
        }
      } else {
        result[i] <- p
      }
    }
    
    return(result)
  }
  
  fx_table %>%
    mutate(
      parameter_clean = clean_ddm_parameter(parameter),
      across(where(is.numeric), ~round(.x, 3))
    ) %>%
    mutate(
      rhat = ifelse(is.na(rhat), "-", sprintf("%.2f", rhat)),
      ess = ifelse(is.na(ess), "-", format(round(as.numeric(ess)), big.mark = ","))
    ) %>%
    select(parameter_clean, estimate, conf.low, conf.high, rhat, ess) %>%
    rename(parameter = parameter_clean) %>%
    gt() %>%
    tab_header(title = md("**Table: Fixed Effects Summary (Link Scale)**")) %>%
    cols_label(
      parameter = "Parameter",
      estimate = "Mean",
      conf.low = "2.5%",
      conf.high = "97.5%",
      rhat = "Rhat",
      ess = "ESS Bulk"
    ) %>%
    tab_style(
      style = cell_text(weight = "bold"),
      locations = cells_column_labels()
    )
} else {
  cat("Fixed effects table not available.")
}
```

## Posterior Contrasts with Directional Evidence

```{r}
if (!is.null(contrasts_table)) {
  # Function to clean contrast and parameter names
  clean_contrast_name <- function(contrast) {
    contrast <- as.character(contrast)
    
    # Common contrast patterns
    contrast <- gsub("difficulty_levelHard", "Difficulty: Hard", contrast)
    contrast <- gsub("difficulty_levelEasy", "Difficulty: Easy", contrast)
    contrast <- gsub("difficulty_levelStandard", "Difficulty: Standard", contrast)
    contrast <- gsub("taskVDT", "Task: VDT", contrast)
    contrast <- gsub("taskADT", "Task: ADT", contrast)
    contrast <- gsub("effort_conditionLow_Force_5pct", "Effort: Low", contrast)
    contrast <- gsub("effort_conditionHigh_Force_40pct", "Effort: High", contrast)
    
    # Replace operators
    contrast <- gsub(" - ", " vs. ", contrast)
    contrast <- gsub(" -", " vs.", contrast)
    contrast <- gsub("- ", "vs. ", contrast)
    
    return(contrast)
  }
  
  # Function to clean DDM parameter names (same as Fixed Effects table, vectorized)
  clean_ddm_parameter <- function(param) {
    param <- as.character(param)
    original <- param
    n <- length(param)
    result <- character(n)
    
    for (i in seq_len(n)) {
      p <- param[i]
      orig <- original[i]
      
      # Determine parameter type from prefix
      param_type <- ""
      if (grepl("^bs_", p)) {
        param_type <- "Boundary (a)"
        p <- gsub("^bs_", "", p)
      } else if (grepl("^bias_", p)) {
        param_type <- "Bias (z)"
        p <- gsub("^bias_", "", p)
      } else if (grepl("^ndt_", p)) {
        param_type <- "Non-decision time (t₀)"
        p <- gsub("^ndt_", "", p)
      } else if (grepl("^b_", p)) {
        param_type <- "Drift (v)"
        p <- gsub("^b_", "", p)
      } else {
        # Try to infer from content
        if (grepl("bs|boundary", orig, ignore.case = TRUE)) {
          param_type <- "Boundary (a)"
        } else if (grepl("ndt|non.decision", orig, ignore.case = TRUE)) {
          param_type <- "Non-decision time (t₀)"
        } else if (grepl("bias", orig, ignore.case = TRUE)) {
          param_type <- "Bias (z)"
        } else {
          param_type <- "Drift (v)"  # Default
        }
      }
      
      # Handle Intercept
      if (p == "Intercept" || orig == "Intercept") {
        result[i] <- paste0(param_type, ": Intercept")
        next
      }
      
      # Step 1: Check if this is an interaction (has colon between different factors in raw R format)
      # Interactions have colons between factor patterns like difficulty_level:task or effort_condition:task
      is_raw_interaction <- grepl("_level:", p) || grepl("_condition:", p) || 
                           (grepl("difficulty_level", p) && grepl(":task", p)) ||
                           (grepl("effort_condition", p) && grepl(":task", p)) ||
                           (grepl("difficulty_level", p) && grepl(":effort_condition", p))
      
      if (is_raw_interaction) {
        # Handle interactions: split by colon between factors, clean each part, join with ×
        # Pattern: difficulty_levelHard:taskVDT -> split -> clean -> "Difficulty: Hard × Task: VDT"
        if (grepl(":", p) && !grepl(" × ", p)) {
          # Split by colon (assuming only one colon between factors for interactions)
          parts <- strsplit(p, ":")[[1]]
          # Clean each part
          cleaned_parts <- character(length(parts))
          for (j in seq_along(parts)) {
            part <- parts[j]
            # Clean difficulty
            part <- gsub("difficulty_levelEasy", "Difficulty: Easy", part, ignore.case = TRUE)
            part <- gsub("difficulty_levelHard", "Difficulty: Hard", part, ignore.case = TRUE)
            part <- gsub("difficulty_levelStandard", "Difficulty: Standard", part, ignore.case = TRUE)
            # Clean task
            part <- gsub("^taskVDT$", "Task: VDT", part, ignore.case = TRUE)
            part <- gsub("^taskADT$", "Task: ADT", part, ignore.case = TRUE)
            # Clean effort
            part <- gsub("^effort_conditionLow_Force_5pct$", "Effort: Low", part, ignore.case = TRUE)
            part <- gsub("^effort_conditionLow_MVC$", "Effort: Low", part, ignore.case = TRUE)
            part <- gsub("^effort_conditionLow$", "Effort: Low", part, ignore.case = TRUE)
            part <- gsub("^effort_conditionHigh_Force_40pct$", "Effort: High", part, ignore.case = TRUE)
            part <- gsub("^effort_conditionHigh_MVC$", "Effort: High", part, ignore.case = TRUE)
            part <- gsub("^effort_conditionHigh$", "Effort: High", part, ignore.case = TRUE)
            cleaned_parts[j] <- part
          }
          # Join with ×
          p <- paste(cleaned_parts, collapse = " × ")
        }
      } else {
        # Step 2: Main effects - clean normally (preserve colons)
        # Clean difficulty levels (flexible matching)
        p <- gsub("difficulty_levelEasy", "Difficulty: Easy", p, ignore.case = TRUE)
        p <- gsub("difficulty_levelHard", "Difficulty: Hard", p, ignore.case = TRUE)
        p <- gsub("difficulty_levelStandard", "Difficulty: Standard", p, ignore.case = TRUE)
        
        # Clean task
        p <- gsub("taskVDT", "Task: VDT", p, ignore.case = TRUE)
        p <- gsub("taskADT", "Task: ADT", p, ignore.case = TRUE)
        
        # Clean effort conditions (handle all variations - order matters!)
        p <- gsub("effort_conditionLow_Force_5pct", "Effort: Low", p, ignore.case = TRUE)
        p <- gsub("effort_conditionLow_MVC", "Effort: Low", p, ignore.case = TRUE)
        p <- gsub("effort_conditionLow", "Effort: Low", p, ignore.case = TRUE)
        p <- gsub("effort_conditionHigh_Force_40pct", "Effort: High", p, ignore.case = TRUE)
        p <- gsub("effort_conditionHigh_MVC", "Effort: High", p, ignore.case = TRUE)
        p <- gsub("effort_conditionHigh", "Effort: High", p, ignore.case = TRUE)
      }
      
      # Add parameter type prefix if we have meaningful content
      if (p != orig && param_type != "") {
        # Check if param already starts with a parameter type (avoid duplication)
        if (!grepl("^(Drift|Boundary|Non-decision|Bias)", p)) {
          result[i] <- paste0(param_type, ": ", p)
        } else {
          result[i] <- p
        }
      } else {
        result[i] <- p
      }
    }
    
    return(result)
  }
  
  contrasts_table %>%
    mutate(
      contrast_clean = clean_contrast_name(contrast),
      parameter_clean = clean_ddm_parameter(parameter),
      across(where(is.numeric), ~round(.x, 3))
    ) %>%
    rename(q2.5 = q05, q97.5 = q95) %>%
    select(contrast_clean, parameter_clean, mean, q2.5, q97.5, p_gt0, p_lt0, p_in_rope) %>%
    rename(contrast = contrast_clean, parameter = parameter_clean) %>%
    gt() %>%
    tab_header(title = md("**Table: Posterior Contrasts (Directional Probabilities)**")) %>%
    cols_label(
      contrast = "Contrast",
      parameter = "Parameter",
      mean = "Mean Δ",
      q2.5 = "2.5%",
      q97.5 = "97.5%",
      p_gt0 = "P(Δ>0)",
      p_lt0 = "P(Δ<0)",
      p_in_rope = "P(in ROPE)"
    ) %>%
    tab_style(
      style = cell_text(weight = "bold"),
      locations = cells_column_labels()
    ) %>%
    tab_footnote(
      footnote = "ROPE (Region of Practical Equivalence): |Δ| < 0.02 for drift (v), |Δ| < 0.05 for boundary (bs) and bias (z) on link scales.",
      locations = cells_column_labels(columns = p_in_rope)
    )
} else {
  cat("Contrasts table not available.")
}
```

**Key contrasts interpreted**:

-   **Easy vs. Hard on drift (v)**: Strong positive effect in both tasks (P(Δ\>0) \> 0.99), indicating faster evidence accumulation for easier discriminations (Mean Δ ≈ +1.50 units/s).
-   **Easy vs. Hard on boundary (a)**: Negative effect (Mean Δ ≈ -0.04 on log scale, or \~4% reduction), consistent with reduced caution.
-   **Task differences**: VDT shows systematically different parameter values than ADT, supporting task-specific processing.
-   **Effort on drift and t₀**: High effort shows small but credible effects on information accumulation and motor execution time (t₀ increase of ~0.03 log-units or ~7.5 ms).

# Posterior Predictive Checks

## Primary PPC Gate: Subject-Wise Mid-Body Quantiles

Our **primary gate** for model acceptance is the subject-wise mid-body PPC (conditional on response, 2% censored). This metric respects individual differences and focuses on the core of the RT distribution, avoiding the Simpson's paradox issues inherent in pooled metrics and the known fast-tail limitations of the base Wiener DDM.

**Thresholds** (pre-declared):

-   QP RMSE fail \> 0.12 s (warn \> 0.09 s)
-   KS statistic fail \> 0.20 (warn \> 0.15)
-   Target: ≤ 15% of cells flagged

```{r}
if (!is.null(ppc_subj_cens)) {
  ppc_subj_cens %>%
    mutate(across(where(is.numeric), ~round(.x, 3))) %>%
    gt() %>%
    tab_header(title = md("**Subject-Wise Mid-Body PPC (30/50/70% quantiles; censored 2%)**"))
} else {
  cat("Subject-wise PPC data not available.")
}
```

```{r}
if (!is.null(pf_subj) && !is.na(pf_subj$n_cells)) {
  tibble(
    Metric = c("N Cells", "N Flagged", "% Flagged"),
    Value = c(
      as.character(pf_subj$n_cells),
      as.character(pf_subj$n_flagged),
      sprintf("%.1f%%", pf_subj$pct_flagged)
    )
  ) %>%
    gt() %>%
    tab_header(title = md("**Subject-Wise PPC Summary**"))
}
```

**Result**: `r if (!is.null(pf_subj)) sprintf("%.1f%%", pf_subj$pct_flagged)` of cells flagged. The subject-wise mid-body PPC gate (based on strict pooled quantiles) was not met due to fast-tail deviations. However, as detailed below, the joint model cell-wise PPCs show that the model captures the central tendencies for the majority of conditions (Standard/Easy), with misfit primarily concentrated in VDT-Hard.

## Visual Diagnostics

### 1. RT Distribution Overlays

```{r, fig.cap="Posterior Predictive Check: RT Distributions. Empirical (black solid) vs. posterior predictive (blue solid) RT densities by Task × Effort × Difficulty. Overall model fit is good for central tendencies, with some misfit in fast tails (especially Easy/VDT).", out.width="95%", fig.align="center"}
knitr::include_graphics(fig_path("fig_ppc_rt_overlay.pdf"))
```

### 2. Quantile-Probability (QP) Plots

```{r, fig.cap="Quantile-Probability (QP) Plot. Empirical vs. predicted RT quantiles by difficulty level, with separate panels for Task × Effort. Points colored by difficulty (Standard=gray, Easy=blue, Hard=red) and shaped by response type (Correct/Error). Dashed diagonal = perfect prediction. Deviations primarily occur in fast tails for Easy/VDT conditions.", out.width="95%", fig.align="center"}
knitr::include_graphics(fig_path("fig_qp.pdf"))
```

## Sensitivity Analyses

We conducted additional sensitivity analyses (Unconditional Pooled PPC, Conditional Pooled PPC) which confirmed that the core findings are robust, though strict pooled metrics flag more cells due to fast-tail misfit. These additional checks are detailed in the Supplementary Figures.

# Interpretation & Key Findings

## Bias (Standard-Only Model)

The starting-point bias was above 0.5 (no bias), with posterior mean z = 0.567, 95% CrI \[0.534, 0.601\], indicating a slight bias toward "different" responses. VDT showed less bias toward "different" than ADT on the logit scale, with contrast Δ = -0.179, 95% CrI \[-0.259, -0.101\], P(Δ\>0) \< 0.001. Effort (High vs Low) was negligible, with contrast Δ = 0.048, 95% CrI \[-0.025, 0.120\], P(Δ\>0) = 0.903. Drift on Standard trials was effectively zero, with posterior mean v = -0.036, 95% CrI \[-0.094, 0.022\], validating the use of Standard trials for bias identification. Non-decision time was 233 ms, 95% CrI \[226, 240\], consistent with response-signal motor execution.

```{r, fig.cap="Bias (z) by task and effort with 95% CrIs; reference line at z=0.5 (no bias).", out.width="85%", fig.align="center"}
knitr::include_graphics(fig_path("fig_bias_forest.png"))
```

```{r, fig.cap="Posterior of drift on Standard trials with prior overlay (v≈0). The tight prior Normal(0, 0.03) successfully constrained drift to near-zero, validating the Standard-only bias calibration approach.", out.width="85%", fig.align="center"}
knitr::include_graphics(fig_path("fig_v_standard_posterior.png"))
```

## Joint Model (Confirmation)

Standard drift remained near zero (posterior mean ≈ -0.10, 95% CrI \[-0.179, -0.017\]). Easy showed strong positive drift (≈1.77), Hard moderate positive drift (≈0.25). Bias intercept closely matched the Standard-only estimate (z = 0.572 vs. 0.567); the task effect replicated (VDT \< ADT, Δ = -0.100, 95% CrI \[-0.141, -0.059\]).

## Convergence & Model Selection

All parameters converged well (max $\hat{R}$ ≤ 1.01; min bulk/tail ESS ≥ 400; no divergent transitions). Leave-one-out cross-validation strongly favored a model in which **difficulty modulates drift, boundary separation, and starting-point bias jointly** (v+a+z), relative to drift-only or simpler models (ΔELPD ≈ +185, SE ≈ 20).

## Difficulty Effects

**Drift rate (v)**: Easy trials show faster evidence accumulation than Hard trials (strong positive contrast, P(Δ\>0) \> 0.99 for both tasks).

**Boundary separation (a)**: Easy trials have narrower decision boundaries, consistent with reduced caution when discrimination is easier.

## Task Differences (ADT vs. VDT)

ADT and VDT are separate experimental conditions with distinct parameter profiles. VDT shows systematically different drift rates and boundary settings compared to ADT, supporting modality-specific processing strategies.

## Effort Effects

High effort (40% MVC) produces small but credible effects on drift rate and non-decision time, suggesting that physical effort modulates both information accumulation and motor execution speed.

## Model Fit

**Absolute fit**: Subject-wise mid-body PPCs show acceptable error magnitudes (QP RMSE ≤ 0.09 s for most cells; ≤15% flagged). The model captures central RT tendencies and accuracy well.

**PPC Summary (Joint Model)**: PPCs were good for Standard and Easy cells (QP RMSE \< 0.13, KS \< 0.08), with modest misfit in VDT-Hard (worst QP RMSE ≈ 0.206). This pattern suggests some residual fast-tail behavior not captured by a constant-drift Wiener process.

```{r, fig.cap="Observed vs. model-predicted p('different') across 12 cells (Task × Effort × Difficulty).", out.width="90%", fig.align="center"}
knitr::include_graphics(fig_path("fig_pdiff_heatmap.png"))
```

```{r, fig.cap="PPC best/median/worst cells (QP RMSE and KS with thresholds).", out.width="90%", fig.align="center"}
knitr::include_graphics(fig_path("fig_ppc_small_multiples.png"))
```

**Known limitation**: Pooled conditional PPCs reveal residual fast-tail misfit, most pronounced in Easy/VDT conditions. This is a known limitation of constant-drift Wiener DDMs without across-trial variability (sv, sz, st₀) or explicit contaminant/lapse processes.

# Ethics, Precision, and Data Availability

## Ethics Statement

All participants provided written informed consent. The study was approved by the Institutional Review Board and conducted in accordance with the Declaration of Helsinki.

## Sample Size & Precision

With N=67 subjects and \~260 trials per subject (17,243 total), hierarchical estimation provides adequate precision for group-level and subject-level effects. Effective sample sizes (ESS) for all parameters exceeded 400, indicating stable posterior estimates.

## Data & Code Availability

All analysis code and de-identified data are available in the project repository:\
**Repository**: [modeling-pupil-DDM](https://github.com/mohdasti/modeling-pupil-DDM)\
**Analysis scripts**: `R/`, `scripts/`\
**Report source**: `reports/chap3_ddm_results.qmd`

**Note**: The behavioral dataset and detailed task methodology are described in the LC behavioral report manuscript (see References). This DDM analysis uses the same dataset and participants.

# Limitations & Future Directions

## Model Family Limitations

1.  **Constant-drift Wiener DDM**: The base Wiener DDM assumes constant drift within each trial and no across-trial variability in drift (sv), starting point (sz), or non-decision time (st₀). This can underfit fast tails, especially in VDT-Hard conditions. The constant-drift Wiener DDM underfits fast RT tails, especially in VDT-Hard. Response-signal timing limits identifiability of across-trial variability. Future work could add a small contaminant mixture, across-trial variability (sv, sz), or urgency/collapsing bounds; LBA/race models may better capture fast-tail dynamics in the Easy/VDT regime.

2.  **Non-decision time (t₀) random effects omitted**: In the response-signal design, t₀ primarily reflects motor execution. We modeled t₀ with group-level intercepts and small task/effort effects but omitted subject-level random effects due to identifiability concerns and initialization failures in pilot models. This may underestimate individual differences in motor execution speed.

3.  **Alternative model families**: Linear Ballistic Accumulator (LBA) or race models may provide better fit for fast-tail dynamics, particularly for Easy/VDT. These models allow for more flexible RT distributions and may better accommodate the response-signal design.

## Design-Specific Limitations

4.  **Response-signal RT measurement**: RTs are measured from response-screen onset, not stimulus onset. This constrains the interpretation of t₀ to motor execution and response selection, excluding early perceptual/encoding processes. While this is appropriate for the current design, it limits generalizability to traditional RT paradigms.

5.  **Effort manipulation**: Physical effort (grip force) may interact with motor execution in complex ways not fully captured by small fixed effects on t₀. Future work integrating EMG or kinematic measures could provide richer insights into effort-motor interactions.

## Misfit in Easy/VDT

6.  **Fast-tail misfit**: The most pronounced misfit occurs in Easy/VDT conditions, where the model underpredicts the frequency of very fast correct responses. This suggests a subset of trials may reflect:

    -   Anticipatory responses (partially captured by 2% censoring)
    -   A "fast-guess" process not represented in the base DDM
    -   Extremely high drift rates that are incompatible with the assumed Wiener process for a small subset of trials

    Sensitivity analyses (2% censoring, unconditional PPCs) confirm that substantive conclusions are robust, but future work should explore mixture models or urgency signals to better account for these fast responses.

# Conclusions

This chapter presents a comprehensive hierarchical Wiener DDM analysis of a response-signal change-detection task in older adults. The primary model, in which task difficulty modulates drift rate, boundary separation, and starting-point bias, is strongly supported by LOO cross-validation and shows acceptable fit to subject-wise mid-body RT quantiles. Key findings—difficulty effects on v, a, and z; task-specific processing differences; and small effort effects—are robust across multiple sensitivity analyses. While the base Wiener DDM shows localized misfit in fast tails (especially Easy/VDT), this does not undermine the core substantive conclusions. Future extensions incorporating across-trial variability, urgency, or mixture models may further improve absolute fit.

# Supplementary Figures

## S1. Conditional Accuracy Function (CAF)

```{r, fig.cap="Conditional Accuracy Function (CAF). Empirical accuracy by RT bin for each Task × Effort × Difficulty combination. Shows the speed–accuracy tradeoff: faster responses (lower bins) tend toward chance accuracy, while slower responses show higher accuracy, consistent with evidence accumulation over time.", out.width="95%", fig.align="center"}
knitr::include_graphics(fig_path("fig_caf.pdf"))
```

## S2. PPC Residual Heatmaps

```{r, fig.cap="PPC Residual Heatmaps. KS statistic and QP RMSE by Task × Effort × Difficulty for all models (top panel) and primary model only (bottom panel). Darker red indicates larger residuals (poorer fit). The primary model shows acceptable fit across most cells, with notable misfit in Easy/VDT conditions.", out.width="100%", fig.align="center"}
knitr::include_graphics(fig_path("fig_ppc_heatmaps.pdf"))
```

### Heatmap Detail Tables

```{r}
if (!is.null(heatmap_wide)) {
  # Function to clean factor level values
  clean_factor_levels <- function(df) {
    df <- df %>%
      mutate(
        # Clean task values
        task = case_when(
          task == "VDT" ~ "VDT",
          task == "ADT" ~ "ADT",
          TRUE ~ as.character(task)
        ),
        # Clean effort condition values
        effort_condition = case_when(
          effort_condition == "Low_Force_5pct" ~ "Low",
          effort_condition == "High_Force_40pct" ~ "High",
          effort_condition == "Low" ~ "Low",
          effort_condition == "High" ~ "High",
          TRUE ~ as.character(effort_condition)
        ),
        # Clean difficulty level values
        difficulty_level = case_when(
          difficulty_level == "Standard" ~ "Standard",
          difficulty_level == "Easy" ~ "Easy",
          difficulty_level == "Hard" ~ "Hard",
          TRUE ~ as.character(difficulty_level)
        )
      )
    return(df)
  }
  
  heatmap_wide %>%
    clean_factor_levels() %>%
    mutate(across(where(is.numeric), ~round(.x, 3))) %>%
    gt() %>%
    tab_header(title = md("**PPC Residual Heatmap (Wide Format)**")) %>%
    cols_label(
      task = "Task",
      effort_condition = "Effort",
      difficulty_level = "Difficulty",
      ks_mean_max = "KS Statistic",
      qp_rmse_max = "QP RMSE"
    )
}
```

## S3. Unconditional Pooled PPC Metrics (Reference)

This table reports metrics from the strict unconditional pooled test (censored 2%), provided for completeness. As noted in the text, this pooled test is overly sensitive to small deviations in fast tails and is superseded by the subject-wise gate (≤15% flagged) and the joint model cell-wise PPCs (Standard/Easy good, VDT-Hard modest misfit).

```{r}
if (!is.null(ppc_gate)) {
  ppc_gate %>%
    mutate(across(where(is.numeric), ~round(.x, 3))) %>%
    gt() %>%
    tab_header(title = md("**Pooled PPC Gate Summary (Strict Test)**")) %>%
    cols_label(
      n_cells = "N Cells",
      pct_flagged = "% Flagged",
      max_qp = "Max QP RMSE",
      max_ks = "Max KS"
    )
}
```

## S4. Sensitivity Analysis: Exclusion of Sub-Chance Participants

To verify that the inclusion of 12 participants who performed at or below chance (≤55% accuracy) in some conditions did not bias our main findings, we conducted sensitivity analyses comparing the primary model (N=67) with models fit after excluding these participants (N=55).

**Method**: We refit the primary model (Model3_Difficulty) and an additive model (Model4_Additive) on the reduced dataset excluding sub-chance participants. Parameter estimates were compared using delta (sensitivity - baseline) with conservative 95% credible intervals. If the delta CI includes zero and the baseline and sensitivity CIs overlap, we conclude the parameter is robust to exclusion.

```{r}
if (!is.null(sensitivity_summary)) {
  # Function to clean model names (reuse from LOO table)
  clean_model_name <- function(model) {
    model <- as.character(model)
    
    # Single parameter models
    model <- ifelse(model == "v", "Difficulty → v (drift)", model)
    model <- ifelse(model == "z", "Difficulty → z (bias)", model)
    model <- ifelse(model == "a", "Difficulty → a (boundary)", model)
    
    # Two-parameter combinations
    model <- ifelse(model == "v_z", "Difficulty → v + z", model)
    model <- ifelse(model == "v_a", "Difficulty → v + a", model)
    model <- ifelse(model == "z_a", "Difficulty → z + a", model)
    
    # Three-parameter combination
    model <- ifelse(model == "v_z_a", "Difficulty → v + a + z", model)
    model <- ifelse(model == "v_a_z", "Difficulty → v + a + z", model)
    
    # Model number formats
    model <- gsub("Model3_Difficulty", "Difficulty → v (drift)", model)
    model <- gsub("Model4_Additive", "Additive (v + a + z)", model)
    model <- gsub("Model5_Interaction", "Interaction", model)
    model <- gsub("Model10_Param_v_bs", "v + a parameterized", model)
    
    # Remove common prefixes
    model <- gsub("^Difficulty_on_", "", model)
    model <- gsub("^difficulty_", "", model)
    
    # Format underscores as separators
    model <- gsub("_", " ", model)
    
    return(model)
  }
  
  # Function to clean parameter names (reuse from DDM parameter function)
  clean_ddm_parameter <- function(param) {
    param <- as.character(param)
    original <- param
    n <- length(param)
    result <- character(n)
    
    for (i in seq_len(n)) {
      p <- param[i]
      orig <- original[i]
      
      # Determine parameter type from prefix
      param_type <- ""
      if (grepl("^bs_", p)) {
        param_type <- "Boundary (a)"
        p <- gsub("^bs_", "", p)
      } else if (grepl("^bias_", p)) {
        param_type <- "Bias (z)"
        p <- gsub("^bias_", "", p)
      } else if (grepl("^ndt_", p)) {
        param_type <- "Non-decision time (t₀)"
        p <- gsub("^ndt_", "", p)
      } else if (grepl("^b_", p)) {
        param_type <- "Drift (v)"
        p <- gsub("^b_", "", p)
      } else {
        # Try to infer from content
        if (grepl("bs|boundary", orig, ignore.case = TRUE)) {
          param_type <- "Boundary (a)"
        } else if (grepl("ndt|non.decision", orig, ignore.case = TRUE)) {
          param_type <- "Non-decision time (t₀)"
        } else if (grepl("bias", orig, ignore.case = TRUE)) {
          param_type <- "Bias (z)"
        } else {
          param_type <- "Drift (v)"  # Default
        }
      }
      
      # Handle Intercept
      if (p == "Intercept" || orig == "Intercept") {
        result[i] <- paste0(param_type, ": Intercept")
        next
      }
      
      # Step 1: Check if this is an interaction (has colon between different factors in raw R format)
      # Interactions have colons between factor patterns like difficulty_level:task or effort_condition:task
      is_raw_interaction <- grepl("_level:", p) || grepl("_condition:", p) || 
                           (grepl("difficulty_level", p) && grepl(":task", p)) ||
                           (grepl("effort_condition", p) && grepl(":task", p)) ||
                           (grepl("difficulty_level", p) && grepl(":effort_condition", p))
      
      if (is_raw_interaction) {
        # Handle interactions: split by colon between factors, clean each part, join with ×
        # Pattern: difficulty_levelHard:taskVDT -> split -> clean -> "Difficulty: Hard × Task: VDT"
        if (grepl(":", p) && !grepl(" × ", p)) {
          # Split by colon (assuming only one colon between factors for interactions)
          parts <- strsplit(p, ":")[[1]]
          # Clean each part
          cleaned_parts <- character(length(parts))
          for (j in seq_along(parts)) {
            part <- parts[j]
            # Clean difficulty
            part <- gsub("difficulty_levelEasy", "Difficulty: Easy", part, ignore.case = TRUE)
            part <- gsub("difficulty_levelHard", "Difficulty: Hard", part, ignore.case = TRUE)
            part <- gsub("difficulty_levelStandard", "Difficulty: Standard", part, ignore.case = TRUE)
            # Clean task
            part <- gsub("^taskVDT$", "Task: VDT", part, ignore.case = TRUE)
            part <- gsub("^taskADT$", "Task: ADT", part, ignore.case = TRUE)
            # Clean effort
            part <- gsub("^effort_conditionLow_Force_5pct$", "Effort: Low", part, ignore.case = TRUE)
            part <- gsub("^effort_conditionLow_MVC$", "Effort: Low", part, ignore.case = TRUE)
            part <- gsub("^effort_conditionLow$", "Effort: Low", part, ignore.case = TRUE)
            part <- gsub("^effort_conditionHigh_Force_40pct$", "Effort: High", part, ignore.case = TRUE)
            part <- gsub("^effort_conditionHigh_MVC$", "Effort: High", part, ignore.case = TRUE)
            part <- gsub("^effort_conditionHigh$", "Effort: High", part, ignore.case = TRUE)
            cleaned_parts[j] <- part
          }
          # Join with ×
          p <- paste(cleaned_parts, collapse = " × ")
        }
      } else {
        # Step 2: Main effects - clean normally (preserve colons)
        # Clean difficulty levels (flexible matching)
        p <- gsub("difficulty_levelEasy", "Difficulty: Easy", p, ignore.case = TRUE)
        p <- gsub("difficulty_levelHard", "Difficulty: Hard", p, ignore.case = TRUE)
        p <- gsub("difficulty_levelStandard", "Difficulty: Standard", p, ignore.case = TRUE)
        
        # Clean task
        p <- gsub("taskVDT", "Task: VDT", p, ignore.case = TRUE)
        p <- gsub("taskADT", "Task: ADT", p, ignore.case = TRUE)
        
        # Clean effort conditions (handle all variations - order matters!)
        p <- gsub("effort_conditionLow_Force_5pct", "Effort: Low", p, ignore.case = TRUE)
        p <- gsub("effort_conditionLow_MVC", "Effort: Low", p, ignore.case = TRUE)
        p <- gsub("effort_conditionLow", "Effort: Low", p, ignore.case = TRUE)
        p <- gsub("effort_conditionHigh_Force_40pct", "Effort: High", p, ignore.case = TRUE)
        p <- gsub("effort_conditionHigh_MVC", "Effort: High", p, ignore.case = TRUE)
        p <- gsub("effort_conditionHigh", "Effort: High", p, ignore.case = TRUE)
      }
      
      # Add parameter type prefix if we have meaningful content
      if (p != orig && param_type != "") {
        # Check if param already starts with a parameter type (avoid duplication)
        if (!grepl("^(Drift|Boundary|Non-decision|Bias)", p)) {
          result[i] <- paste0(param_type, ": ", p)
        } else {
          result[i] <- p
        }
      } else {
        result[i] <- p
      }
    }
    
    return(result)
  }
  
  sens_subchance <- sensitivity_summary %>%
    filter(sensitivity == "exclude_subchance") %>%
    mutate(
      model_clean = clean_model_name(model),
      parameter_clean = clean_ddm_parameter(parameter),
      across(where(is.numeric), ~round(.x, 3))
    ) %>%
    select(model_clean, parameter_clean, baseline_estimate, sensitivity_estimate, delta, 
           delta_ci_lower, delta_ci_upper, ci_overlap, delta_contains_zero) %>%
    rename(model = model_clean, parameter = parameter_clean) %>%
    arrange(model, parameter)
  
  if (nrow(sens_subchance) > 0) {
    sens_subchance %>%
      gt() %>%
      tab_header(title = md("**Sensitivity Analysis: Excluding Sub-Chance Participants**")) %>%
      cols_label(
        model = "Model",
        parameter = "Parameter",
        baseline_estimate = "Baseline",
        sensitivity_estimate = "Excluded",
        delta = "Δ",
        delta_ci_lower = "Δ CI Lower",
        delta_ci_upper = "Δ CI Upper",
        ci_overlap = "CI Overlap",
        delta_contains_zero = "Δ Contains 0"
      ) %>%
      tab_footnote(
        footnote = "Baseline: N=67 (includes sub-chance). Excluded: N=55 (excludes sub-chance). Δ = Excluded - Baseline. If Δ CI contains 0 and CIs overlap, parameter is robust.",
        locations = cells_column_labels(columns = delta_contains_zero)
      )
  } else {
    cat("Sensitivity analysis results for sub-chance exclusion not available.")
  }
} else {
  cat("Sensitivity analysis summary not available.")
}
```

**Results**: Most key parameters showed robust estimates when excluding sub-chance participants. For Model3_Difficulty and Model4_Additive, the drift intercept and boundary separation showed delta CIs that included zero, indicating no meaningful change. The Easy difficulty effect was also robust. The Hard difficulty effect showed a small shift (Δ ≈ -0.10, delta CI did not include zero), but this represents a small change in magnitude (\~6.5% of the baseline estimate) and does not alter the substantive conclusion that Hard trials show negative drift relative to Standard. **Conclusion**: The inclusion of sub-chance participants did not meaningfully alter main effects or substantive conclusions, supporting our decision to retain all 67 participants to maximize sample size and leverage hierarchical modeling's ability to stabilize estimates through shrinkage.

# References

::: {#refs}
:::

*Note: The LC behavioral report manuscript (in preparation/published) describes the behavioral dataset and methodology used in this analysis. Full citation details will be added when available.*

------------------------------------------------------------------------

**End of Report**
