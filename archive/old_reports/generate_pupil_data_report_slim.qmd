---
title: "Quick-Share Pupil QC Snapshot (Slim)"
author: "BAP Pupillometry Analysis Pipeline"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
format:
  html:
    toc: false
    number-sections: false
    code-fold: true
    code-tools: false
    embed-resources: false
    theme: flatly
execute:
  echo: false
  warning: false
  message: false
---

```{r setup}
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.width = 6,
  fig.height = 4
)

suppressPackageStartupMessages({
  library(dplyr)
  library(readr)
  library(tidyr)
  library(ggplot2)
})

# Determine paths
REPO_ROOT <- if (file.exists("02_pupillometry_analysis")) {
  normalizePath(getwd())
} else if (file.exists("../02_pupillometry_analysis")) {
  normalizePath("..")
} else {
  normalizePath(getwd())
}

qc_dir <- file.path(REPO_ROOT, "data", "qc", "quick_share_latest")

# Run the export script first
script_path <- file.path(REPO_ROOT, "02_pupillometry_analysis", "quick_share_export.R")
if (file.exists(script_path)) {
  cat("Running quick_share_export.R...\n")
  old_wd <- getwd()
  setwd(REPO_ROOT)
  tryCatch({
    source(script_path)
  }, finally = {
    setwd(old_wd)
  })
} else {
  stop("Cannot find quick_share_export.R")
}

# Load CSVs
files_expected <- c(
  "01_file_provenance.csv",
  "02_design_expected_vs_observed.csv",
  "03_trials_per_subject_task_ses.csv",
  "04_run_level_counts.csv",
  "05_window_validity_summary.csv",
  "06_gate_pass_rates_by_threshold.csv",
  "07_bias_checks_key_gates.csv",
  "08_prestim_dip_summary.csv"
)

missing <- files_expected[!file.exists(file.path(qc_dir, files_expected))]
if (length(missing) > 0) {
  stop("Missing expected quick-share CSVs:\n", paste(missing, collapse = "\n"))
}

trials_per_sub <- read_csv(file.path(qc_dir, "03_trials_per_subject_task_ses.csv"), show_col_types = FALSE)
gate_rates <- read_csv(file.path(qc_dir, "06_gate_pass_rates_by_threshold.csv"), show_col_types = FALSE)
window_summary <- read_csv(file.path(qc_dir, "05_window_validity_summary.csv"), show_col_types = FALSE)
run_level <- read_csv(file.path(qc_dir, "04_run_level_counts.csv"), show_col_types = FALSE)
design_table <- read_csv(file.path(qc_dir, "02_design_expected_vs_observed.csv"), show_col_types = FALSE)
prestim_dip <- read_csv(file.path(qc_dir, "08_prestim_dip_summary.csv"), show_col_types = FALSE)
```

## Executive Summary

```{r exec-summary}
n_subjects <- length(unique(trials_per_sub$sub))
total_trials <- sum(trials_per_sub$observed_trials, na.rm = TRUE)
median_trials_per_run <- median(run_level$n_trials, na.rm = TRUE)
mean_coverage <- mean(design_table$pct_coverage, na.rm = TRUE)

cat("1. **N subjects**: ", n_subjects, "\n", sep = "")
cat("2. **Total trials**: ", total_trials, "\n", sep = "")
cat("3. **Median trials per run**: ", sprintf("%.1f", median_trials_per_run), " (expected 28-30)\n", sep = "")
cat("4. **Design coverage**: ", sprintf("%.1f%%", 100 * mean_coverage), "\n", sep = "")

if (nrow(window_summary) > 0 && !all(is.na(window_summary$baseline_cog_mean))) {
  cat("5. **Baseline cog validity (mean)**: ", sprintf("%.2f", mean(window_summary$baseline_cog_mean, na.rm = TRUE)), "\n", sep = "")
  cat("6. **Post target validity (mean)**: ", sprintf("%.2f", mean(window_summary$post_target_mean, na.rm = TRUE)), "\n", sep = "")
} else {
  cat("5. **Window validity**: Not computed\n", sep = "")
}
```

## Design Compliance (Sample)

```{r design-table}
# Show only first 10 rows to keep HTML small
design_display <- design_table %>%
  select(sub, task, session_used, expected_trials, observed_trials, pct_coverage) %>%
  slice_head(n = 10)

knitr::kable(design_display, 
             col.names = c("Subject", "Task", "Session", "Expected", "Observed", "Coverage %"),
             format = "simple", digits = 1)

cat("\n*Showing first 10 rows. Full data in `02_design_expected_vs_observed.csv`*\n")
```

## Run-Level Trial Counts Summary

```{r run-counts}
run_summary <- run_level %>%
  group_by(task) %>%
  summarise(
    n_runs = n(),
    median_trials = median(n_trials, na.rm = TRUE),
    min_trials = min(n_trials, na.rm = TRUE),
    max_trials = max(n_trials, na.rm = TRUE),
    runs_with_60 = sum(n_trials == 60, na.rm = TRUE),
    .groups = "drop"
  )

knitr::kable(run_summary,
             col.names = c("Task", "N Runs", "Median Trials", "Min", "Max", "Runs with 60"),
             format = "simple")

if (any(run_summary$runs_with_60 > 0)) {
  cat("\n**WARNING**: Some runs have exactly 60 trials (possible double-counting bug)\n")
}
```

## Gate Pass Rates

```{r gate-plot}
if (nrow(gate_rates) > 0 && any(!is.na(gate_rates$pass_rate))) {
  gate_plot_data <- gate_rates %>%
    filter(!is.na(pass_rate)) %>%
    mutate(
      gate_label = case_when(
        gate == "baseline_cog" ~ "Baseline Cog",
        gate == "post_target" ~ "Post Target",
        gate == "overall" ~ "Overall",
        TRUE ~ gate
      )
    )
  
  if (nrow(gate_plot_data) > 0) {
    gate_plot <- gate_plot_data %>%
      ggplot(aes(x = threshold, y = pass_rate, color = gate_label, linetype = task)) +
      geom_line(size = 1) +
      geom_point(size = 2) +
      scale_x_continuous(breaks = unique(gate_plot_data$threshold)) +
      scale_y_continuous(labels = scales::percent_format(), limits = c(0, 1)) +
      labs(
        x = "Threshold",
        y = "Pass Rate",
        color = "Gate",
        linetype = "Task",
        title = "Gate Pass Rates by Threshold"
      ) +
      theme_minimal() +
      theme(legend.position = "bottom")
    
    print(gate_plot)
  } else {
    cat("No valid pass rate data to plot.\n")
  }
} else {
  cat("Gate pass rates data not available.\n")
}
```

## Window Validity Summary

```{r window-table}
if (nrow(window_summary) > 0 && !all(is.na(window_summary$baseline_cog_mean))) {
  window_display <- window_summary %>%
    select(task, baseline_cog_mean, baseline_cog_median, 
           post_target_mean, post_target_median,
           overall_mean, overall_median) %>%
    mutate(across(where(is.numeric), ~ sprintf("%.3f", .x)))
  
  knitr::kable(window_display,
               col.names = c("Task", "Baseline Cog Mean", "Baseline Cog Median", 
                            "Post Target Mean", "Post Target Median",
                            "Overall Mean", "Overall Median"),
               format = "simple")
} else {
  cat("Window validity data not available.\n")
}
```

## Prestim Dip Summary

```{r prestim-table}
if (nrow(prestim_dip) > 0 && !all(is.na(prestim_dip$mean_dip_amp_overall))) {
  prestim_display <- prestim_dip %>%
    select(task, n_trials_total, mean_dip_amp_overall, pct_negative_dip_overall) %>%
    mutate(across(where(is.numeric), ~ if_else(is.na(.x), "N/A", sprintf("%.3f", .x))))
  
  knitr::kable(prestim_display,
               col.names = c("Task", "N Trials", "Mean Dip Amp", "% Negative Dip"),
               format = "simple")
} else {
  cat("Prestim dip data not available.\n")
}
```

## Data Files

All detailed outputs are available in `data/qc/quick_share_latest/`:

- `01_file_provenance.csv` - File paths, git hash, timestamps
- `02_design_expected_vs_observed.csv` - Design compliance (full data)
- `03_trials_per_subject_task_ses.csv` - Per-subject trial counts and gate pass counts
- `04_run_level_counts.csv` - Run-level statistics (n_trials should be ~30, not 60)
- `05_window_validity_summary.csv` - Window validity distributions
- `06_gate_pass_rates_by_threshold.csv` - Gate pass rates
- `07_bias_checks_key_gates.csv` - Bias checks (if behavioral data available)
- `08_prestim_dip_summary.csv` - Prestim dip analysis

**Note**: This report shows only summary tables. Full data is in the CSV files above.
