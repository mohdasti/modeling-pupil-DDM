---
title: "Comprehensive Pupil Data Report"
author: "BAP Pupillometry Analysis Pipeline"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
params:
  processed_dir: "/Users/mohdasti/Documents/LC-BAP/BAP/BAP_Pupillometry/BAP/BAP_processed"
  behavioral_file: "/Users/mohdasti/Documents/LC-BAP/BAP/Nov2025/bap_beh_trialdata_v2.csv"
  analysis_ready_dir: "data/analysis_ready"
  output_dir: "06_visualization/publication_figures"
  recompute_raw_coverage: false
  threshold_grid: [0.50, 0.60, 0.70, 0.80, 0.90, 0.95]
  min_trials_per_cell: 10
  export_qc_csvs: true
  export_dir: "quality_control/exports"
  export_thresholds: [0.50, 0.60, 0.70, 0.80, 0.90]
  export_default_thr: 0.60
format:
  html:
    toc: true
    toc-depth: 3
    toc-location: left
    number-sections: true
    code-fold: false
    code-tools: true
    embed-resources: true
    theme: cosmo
    fig-width: 10
    fig-height: 6
    fig-dpi: 300
execute:
  echo: true
  warning: false
  message: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.width = 10,
  fig.height = 6,
  dpi = 300
)

# Load required libraries
suppressPackageStartupMessages({
  library(dplyr)
  library(tidyr)
  library(readr)
  library(purrr)
  library(ggplot2)
  library(knitr)
  library(kableExtra)
  library(DT)
  library(patchwork)
  if (requireNamespace("broom", quietly = TRUE)) {
    library(broom)
  }
})

# Helper function
`%||%` <- function(x, y) if (is.null(x) || length(x) == 0) y else x

normalize_path_from_doc <- function(path) {
  # If path is absolute, use as-is; otherwise treat as project-root relative
  if (nzchar(path) && substring(path, 1, 1) == "/") {
    path
  } else {
    file.path("..", path)
  }
}

# Configuration (paths can be supplied via Quarto params or environment variables)
processed_dir <- params$processed_dir %||% Sys.getenv("BAP_PROCESSED_DIR", unset = "")
behavioral_file <- params$behavioral_file %||% Sys.getenv("BAP_BEHAVIORAL_FILE", unset = "")

if (!nzchar(processed_dir) || !nzchar(behavioral_file)) {
  stop(
    "Processed pupil directory and behavioral file must be specified.\n",
    "Set Quarto params 'processed_dir' and 'behavioral_file' in the YAML header\n",
    "or define environment variables BAP_PROCESSED_DIR and BAP_BEHAVIORAL_FILE."
  )
}

analysis_ready_dir <- normalize_path_from_doc(
  params$analysis_ready_dir %||% "data/analysis_ready"
)
output_dir <- normalize_path_from_doc(
  params$output_dir %||% "06_visualization/publication_figures"
)
```

# Introduction

This report provides a comprehensive overview of the pupil data available in the BAP (Brain Aging and Perception) study. It documents:

- **Data inventory**: Files available for each subject
- **Subject-level statistics**: Trials, runs, and data availability
- **Quality control**: Data quality metrics and filtering
- **Feature extraction**: AUC metrics and processing methods
- **Visualizations**: Quality distributions and pupil waveforms

The report is generated from the pupillometry analysis pipeline located in `02_pupillometry_analysis/`.

::: {.callout-note}
## QC Data Exports

To export QC decision data to CSV files, set `export_qc_csvs: true` in the YAML parameters when rendering this report. The exports will be written to `quality_control/exports/` and include up to 9 files with trial-level coverage, threshold sweeps, loss reasons, availability summaries, and analysis-ready subject statistics.
:::

# Data Inventory

## File Discovery

```{r file-discovery}
# Find all flat files
flat_files_merged <- list.files(processed_dir, pattern = "_flat_merged\\.csv$", full.names = TRUE)
flat_files_reg <- list.files(processed_dir, pattern = "_flat\\.csv$", full.names = TRUE)

# Remove duplicates (prefer merged versions)
if (length(flat_files_merged) > 0 && length(flat_files_reg) > 0) {
  merged_ids <- gsub("_flat_merged\\.csv$", "", basename(flat_files_merged))
  reg_ids <- gsub("_flat\\.csv$", "", basename(flat_files_reg))
  reg_to_keep <- !reg_ids %in% merged_ids
  flat_files <- c(flat_files_merged, flat_files_reg[reg_to_keep])
} else {
  flat_files <- c(flat_files_merged, flat_files_reg)
}

cat("**Total flat files found:**", length(flat_files), "\n")
cat("**Merged files:**", length(flat_files_merged), "\n")
cat("**Regular files:**", length(flat_files_reg), "\n")
cat("**Unique files (after deduplication):**", length(flat_files), "\n")
```

## Subject File Inventory

```{r subject-inventory}
# Extract subject IDs from file names
extract_subject_id <- function(filename) {
  # Remove path and extension
  base <- basename(filename)
  # Remove _flat or _flat_merged suffix
  base <- gsub("_flat(_merged)?\\.csv$", "", base)
  return(base)
}

file_inventory <- tibble(
  filename = basename(flat_files),
  full_path = flat_files,
  subject_id = extract_subject_id(flat_files),
  file_type = ifelse(grepl("_merged", filename), "merged_flat", "raw_flat"),
  file_size_mb = round(file.size(flat_files) / 1024^2, 2),
  modified_date = format(file.mtime(flat_files), "%Y-%m-%d %H:%M")
) %>%
  arrange(subject_id, file_type)

# Summary by subject
subject_file_summary <- file_inventory %>%
  group_by(subject_id) %>%
  summarise(
    n_files = n(),
    has_raw = any(file_type == "raw_flat"),
    has_merged = any(file_type == "merged_flat"),
    total_size_mb = sum(file_size_mb),
    latest_modification = max(modified_date),
    .groups = "drop"
  ) %>%
  arrange(subject_id)

cat("**Total unique subjects with files:**", nrow(subject_file_summary), "\n")
```

### File Inventory Table

```{r file-inventory-table}
file_inventory %>%
  select(subject_id, filename, file_type, file_size_mb, modified_date) %>%
  kable(
    col.names = c("Subject ID", "Filename", "Type", "Size (MB)", "Modified"),
    caption = "Complete file inventory for all subjects",
    digits = 2
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = TRUE,
    font_size = 11
  ) %>%
  scroll_box(height = "400px")
```

### Subjects Missing Merged Files

```{r subjects-missing-merged, results='asis'}
# Identify subjects without merged files
subjects_without_merged <- subject_file_summary %>%
  filter(!has_merged) %>%
  arrange(subject_id)

if (nrow(subjects_without_merged) > 0) {
  cat("The following", nrow(subjects_without_merged), "subjects do **not** have merged flat files (`*_flat_merged.csv`):\n\n")
  
  # Get details about why they might be missing
  missing_details <- subjects_without_merged %>%
    left_join(
      file_inventory %>%
        filter(file_type == "raw_flat") %>%
        group_by(subject_id) %>%
        summarise(
          raw_file_count = n(),
          raw_total_size_mb = sum(file_size_mb),
          raw_files = paste(basename(filename), collapse = "; "),
          .groups = "drop"
        ),
      by = "subject_id"
    ) %>%
    mutate(
      raw_file_count = ifelse(is.na(raw_file_count), 0, raw_file_count),
      raw_total_size_mb = ifelse(is.na(raw_total_size_mb), 0, raw_total_size_mb)
    )
  
  missing_details %>%
    select(subject_id, raw_file_count, raw_total_size_mb, total_size_mb) %>%
    kable(
      col.names = c("Subject ID", "Raw Files Count", "Raw Files Size (MB)", "Total Size (MB)"),
      caption = "Subjects missing merged files - these subjects only have raw_flat files",
      digits = 2
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed", "warning"),
      full_width = FALSE
    ) %>%
    print()
  
  cat("\n**Why merged files might be missing:**\n")
  cat("- Merged files (`*_flat_merged.csv`) are created by a preprocessing step that merges pupil data with behavioral data (RT, stimulus conditions, etc.)\n")
  cat("- If a subject only has `*_flat.csv` files, it means:\n")
  cat("  1. The merge preprocessing step hasn't been run for this subject yet, OR\n")
  cat("  2. **No matching behavioral data exists** (most common reason) - the pupil file exists but there's no corresponding behavioral data in the behavioral CSV file, OR\n")
  cat("  3. The raw pupil file was too small or had quality issues that prevented merging\n")
  cat("- **Note**: The report will still use raw_flat files if merged files aren't available, but merged files are preferred because they include behavioral variables (RT, stimulus conditions) needed for proper trial-level analysis.\n\n")
  
  # Diagnostic: Check if behavioral data exists for these subjects
  cat("#### Diagnostic: Checking Behavioral Data Availability\n\n")
  
  if (file.exists(behavioral_file)) {
    library(readr)
    library(dplyr)
    
    beh_data <- read_csv(behavioral_file, show_col_types = FALSE)
    
    # Extract subject IDs from missing subjects (handle task suffix)
    missing_subjects_base <- unique(gsub("_(ADT|VDT)$", "", subjects_without_merged$subject_id))
    missing_tasks <- gsub(".*_", "", subjects_without_merged$subject_id)
    
    diagnostic_results <- subjects_without_merged %>%
      mutate(
        subject_base = gsub("_(ADT|VDT)$", "", subject_id),
        task_suffix = gsub(".*_", "", subject_id),
        task_modality_expected = case_when(
          task_suffix == "ADT" ~ "aud",
          task_suffix == "VDT" ~ "vis",
          TRUE ~ NA_character_
        )
      ) %>%
      rowwise() %>%
      mutate(
        has_behavioral = if ("subject_id" %in% names(beh_data) && "task_modality" %in% names(beh_data)) {
          nrow(beh_data %>% filter(
            grepl(subject_base, subject_id, fixed = TRUE) & 
            task_modality == task_modality_expected
          )) > 0
        } else NA,
        behavioral_rows = if ("subject_id" %in% names(beh_data) && "task_modality" %in% names(beh_data)) {
          nrow(beh_data %>% filter(
            grepl(subject_base, subject_id, fixed = TRUE) & 
            task_modality == task_modality_expected
          ))
        } else NA_integer_,
        other_tasks_in_behav = if ("subject_id" %in% names(beh_data) && "task_modality" %in% names(beh_data)) {
          other_tasks <- beh_data %>% 
            filter(grepl(subject_base, subject_id, fixed = TRUE)) %>%
            pull(task_modality) %>%
            unique()
          paste(other_tasks, collapse = ", ")
        } else NA_character_
      ) %>%
      ungroup()
    
    # Improve diagnostic to show clearer categories
    diagnostic_summary <- diagnostic_results %>%
      select(subject_id, has_behavioral, behavioral_rows, other_tasks_in_behav) %>%
      mutate(
        has_other_tasks = !is.na(other_tasks_in_behav) & nchar(other_tasks_in_behav) > 0,
        reason = case_when(
          !has_behavioral & has_other_tasks ~ 
            paste0("Missing task: behavioral data exists for ", other_tasks_in_behav, " but not this task"),
          !has_behavioral & !has_other_tasks ~ 
            "No behavioral data: subject not found in behavioral CSV at all",
          TRUE ~ "Behavioral data exists - merge may have failed for other reasons"
        ),
        status = case_when(
          !has_behavioral & has_other_tasks ~ "Partial data",
          !has_behavioral ~ "No data",
          TRUE ~ "Data exists"
        )
      ) %>%
      select(subject_id, status, reason, behavioral_rows, other_tasks_in_behav) %>%
      arrange(status, subject_id)
    
    diagnostic_summary %>%
      kable(
        col.names = c("Subject ID", "Status", "Reason", "Behavioral Rows Found", "Other Tasks Available"),
        caption = "Diagnostic: Why merged files are missing (grouped by status)"
      ) %>%
      kable_styling(
        bootstrap_options = c("striped", "hover", "condensed"),
        full_width = FALSE
      ) %>%
      print()
    
    # Summary counts
    cat("\n**Summary:**\n")
    cat("- **No behavioral data at all**:", sum(diagnostic_summary$status == "No data"), "subject-task(s)\n")
    cat("- **Partial data (one task missing)**:", sum(diagnostic_summary$status == "Partial data"), "subject-task(s)\n")
    cat("- **Data exists but merge failed**:", sum(diagnostic_summary$status == "Data exists"), "subject-task(s)\n\n")
    
    cat("**Recommendations:**\n")
    if (sum(diagnostic_summary$status == "No data") > 0) {
      cat("- For subjects with **no behavioral data**: Check if behavioral data exists in other files/locations, or if these sessions were not recorded in the behavioral system.\n")
    }
    if (sum(diagnostic_summary$status == "Partial data") > 0) {
      cat("- For subjects with **partial data**: Verify if the missing task session was actually run. If yes, check if behavioral data was exported/included in the CSV.\n")
    }
    cat("\n")
  } else {
    cat("Cannot check behavioral data availability - behavioral file not found at:", behavioral_file, "\n\n")
  }
  
  # Show which raw files exist for these subjects
  if (any(missing_details$raw_file_count > 0)) {
    cat("\n\n#### Raw Files for Subjects Missing Merged Files\n\n")
    missing_details %>%
      filter(raw_file_count > 0) %>%
      select(subject_id, raw_files) %>%
      mutate(raw_files = strsplit(raw_files, "; ")) %>%
      tidyr::unnest(raw_files) %>%
      kable(
        col.names = c("Subject ID", "Raw Filename"),
        caption = "Raw flat files available for subjects without merged files"
      ) %>%
      kable_styling(
        bootstrap_options = c("striped", "hover", "condensed"),
        full_width = FALSE,
        font_size = 10
      ) %>%
      print()
  }
} else {
  cat("### ✅ All Subjects Have Merged Files\n\n")
  cat("All", nrow(subject_file_summary), "subjects have merged flat files (`*_flat_merged.csv`).\n")
  cat("This means the preprocessing merge step has been completed for all subjects.\n\n")
}
```

# RAW PUPIL TRIAL COVERAGE (PRE-FILTER)

```{r raw-coverage-build, include=FALSE}
# Build or load trial-level pre-filter coverage, aligned to task timing and AUC windows

coverage_cache_dir <- "quality_control/output"
dir.create(coverage_cache_dir, recursive = TRUE, showWarnings = FALSE)

coverage_cache_file <- file.path(coverage_cache_dir, "pupil_trial_coverage_prefilter.rds")
threshold_sweep_cache_file <- file.path(coverage_cache_dir, "pupil_threshold_sweep.rds")

safe_first <- function(x) {
  x <- x[!is.na(x)]
  if (length(x) == 0) NA else x[1]
}

window_stats <- function(time, pupil, start, end) {
  in_win <- !is.na(time) & time >= start & time <= end
  n_rows <- sum(in_win)
  if (n_rows == 0) {
    return(list(prop = NA_real_, n_rows = 0L))
  }
  prop_valid <- mean(!is.na(pupil[in_win]))
  list(prop = prop_valid, n_rows = n_rows)
}

build_trial_coverage <- function(files) {
  purrr::map_dfr(files, function(f) {
    df <- readr::read_csv(f, show_col_types = FALSE, progress = FALSE)

    if (!all(c("time", "pupil") %in% names(df))) {
      return(tibble())
    }

    df <- df %>%
      mutate(
        sub = if ("sub" %in% names(.)) sub else if ("subject_id" %in% names(.)) as.character(subject_id) else NA_character_,
        task = if ("task" %in% names(.)) {
          dplyr::if_else(task == "aud", "ADT",
                         dplyr::if_else(task == "vis", "VDT", as.character(task)))
        } else if ("task_modality" %in% names(.)) {
          dplyr::case_when(
            task_modality == "aud" ~ "ADT",
            task_modality == "vis" ~ "VDT",
            TRUE ~ as.character(task_modality)
          )
        } else NA_character_,
        run = if ("run" %in% names(.)) run else if ("run_num" %in% names(.)) run_num else NA_integer_,
        trial_index = dplyr::coalesce(
          if ("trial_index" %in% names(.)) trial_index else NA_integer_,
          if ("trial_in_run" %in% names(.)) trial_in_run else NA_integer_,
          if ("trial" %in% names(.)) trial else NA_integer_,
          if ("trial_num" %in% names(.)) trial_num else NA_integer_
        ),
        pupil = if ("pupil" %in% names(.)) pupil else NA_real_,
        resp1RT = if ("resp1RT" %in% names(.)) resp1RT else if ("rt" %in% names(.)) rt else NA_real_,
        gf_trPer = dplyr::coalesce(
          if ("gf_trPer" %in% names(.)) gf_trPer else NA_real_,
          if ("grip_targ_prop_mvc" %in% names(.)) grip_targ_prop_mvc else NA_real_
        ),
        force_condition = if ("force_condition" %in% names(.)) force_condition else NA_character_,
        stimLev = if ("stimLev" %in% names(.)) stimLev else if ("stim_level_index" %in% names(.)) stim_level_index else NA_real_,
        isOddball = if ("isOddball" %in% names(.)) isOddball else if ("stim_is_diff" %in% names(.)) as.integer(stim_is_diff) else NA_integer_
      ) %>%
      filter(!is.na(sub), !is.na(task), !is.na(run), !is.na(trial_index))

    if (nrow(df) == 0) {
      return(tibble())
    }

    df$pupil[df$pupil == 0] <- NA_real_

    df %>%
      group_by(sub, task, run, trial_index) %>%
      summarise(
        subject_id = safe_first(as.character(sub)),
        task = safe_first(as.character(task)),
        run = safe_first(run),
        trial_index = safe_first(trial_index),

        has_any_pupil = any(!is.na(pupil)),

        recording_max_time = max(time, na.rm = TRUE),
        last_valid_time = {
          tv <- time[!is.na(pupil)]
          if (length(tv) == 0) NA_real_ else max(tv, na.rm = TRUE)
        },
        has_response_window = !is.na(recording_max_time) & recording_max_time >= 7.7 - 0.05,

        rt = safe_first(resp1RT),
        response_onset = {
          rt_val <- safe_first(resp1RT)
          if (!is.na(rt_val) && rt_val > 0 && rt_val < 5.0) 4.7 + rt_val else 7.7
        },

        # Baseline 500 ms and full ITI
        valid_prop_baseline_500ms = window_stats(time, pupil, -0.5, 0.0)$prop,
        n_rows_baseline_500ms     = window_stats(time, pupil, -0.5, 0.0)$n_rows,
        valid_prop_iti_full       = window_stats(time, pupil, -3.0, 0.0)$prop,
        n_rows_iti_full           = window_stats(time, pupil, -3.0, 0.0)$n_rows,

        # Prestim (3.25–3.75)
        valid_prop_prestim = window_stats(time, pupil, 3.25, 3.75)$prop,
        n_rows_prestim     = window_stats(time, pupil, 3.25, 3.75)$n_rows,

        # Total AUC window (0–response_onset)
        valid_prop_total_auc = window_stats(time, pupil, 0.0, response_onset)$prop,
        n_rows_total_auc     = window_stats(time, pupil, 0.0, response_onset)$n_rows,

        # Cognitive AUC window (4.65–response_onset)
        valid_prop_cognitive_auc = window_stats(time, pupil, 4.65, response_onset)$prop,
        n_rows_cognitive_auc     = window_stats(time, pupil, 4.65, response_onset)$n_rows,
        
        # Fixed cognitive windows (for salvage/robustness analysis)
        valid_prop_cognitive_fixed_1s = window_stats(time, pupil, 4.65, 5.65)$prop,
        n_rows_cognitive_fixed_1s     = window_stats(time, pupil, 4.65, 5.65)$n_rows,
        valid_prop_cognitive_fixed_1p5s = window_stats(time, pupil, 4.65, 6.15)$prop,
        n_rows_cognitive_fixed_1p5s     = window_stats(time, pupil, 4.65, 6.15)$n_rows,

        # Store raw source values for diagnostics (before condition assignment)
        raw_gf_trPer = safe_first(gf_trPer),
        raw_force_condition = safe_first(force_condition),
        raw_isOddball = safe_first(isOddball),
        raw_stimLev = safe_first(stimLev),

        effort_condition = factor(dplyr::case_when(
          !is.na(gf_trPer) & gf_trPer == 0.05 ~ "Low_5_MVC",
          !is.na(gf_trPer) & gf_trPer == 0.4 ~ "High_40_MVC",  # Note: 0.4 (not 0.40) as per design
          !is.na(gf_trPer) & gf_trPer == 0.40 ~ "High_40_MVC",  # Also accept 0.40 for compatibility
          force_condition == "Low_Force_5pct" ~ "Low_5_MVC",
          force_condition == "High_Force_40pct" ~ "High_40_MVC",
          TRUE ~ NA_character_
        ), levels = c("Low_5_MVC", "High_40_MVC")),

        difficulty_level = factor(dplyr::case_when(
          # Standard trials: isOddball == 0 or stimLev == 0
          isOddball == 0 ~ "Standard",
          !is.na(stimLev) & stimLev == 0 ~ "Standard",
          # Oddball trials: isOddball == 1 with stimLev values
          # Based on actual data: stim_level_index 1,2,3,4 are oddballs
          # Lower values (1,2) typically harder, higher values (3,4) typically easier
          # But if we don't have that info, we can use: all oddballs with stimLev > 0
          isOddball == 1 & !is.na(stimLev) & stimLev %in% c(1, 2) ~ "Hard",
          isOddball == 1 & !is.na(stimLev) & stimLev %in% c(3, 4) ~ "Easy",
          # Fallback: if isOddball == 1 but stimLev doesn't match, check stimLev directly
          !is.na(stimLev) & stimLev %in% c(1, 2) ~ "Hard",
          !is.na(stimLev) & stimLev %in% c(3, 4) ~ "Easy",
          # Legacy values (in case some files still have them)
          isOddball == 1 & !is.na(stimLev) & stimLev %in% c(8, 16, 0.06, 0.12) ~ "Hard",
          isOddball == 1 & !is.na(stimLev) & stimLev %in% c(32, 64, 0.24, 0.48) ~ "Easy",
          !is.na(stimLev) & stimLev %in% c(8, 16, 0.06, 0.12) ~ "Hard",
          !is.na(stimLev) & stimLev %in% c(32, 64, 0.24, 0.48) ~ "Easy",
          TRUE ~ NA_character_
        ), levels = c("Standard", "Easy", "Hard")),

        .groups = "drop"
      )
  })
}

if (file.exists(coverage_cache_file) && !isTRUE(params$recompute_raw_coverage)) {
  trial_coverage_prefilter <- readRDS(coverage_cache_file)
} else {
  # Prefer merged flat files when available
  merged_files <- flat_files_merged[grepl("_flat_merged\\.csv$", flat_files_merged)]
  files_to_use <- if (length(merged_files) > 0) merged_files else flat_files

  trial_coverage_prefilter <- build_trial_coverage(files_to_use)
  saveRDS(trial_coverage_prefilter, coverage_cache_file)
}

# RECOVERY: Backfill missing conditions from behavioral file
# This attempts to recover trials that have missing conditions due to merge failures
n_missing_before_recovery <- sum(is.na(trial_coverage_prefilter$effort_condition) | is.na(trial_coverage_prefilter$difficulty_level))

if (file.exists(behavioral_file) && n_missing_before_recovery > 0) {
  cat(sprintf("Attempting to recover missing conditions from behavioral file...\n"))
  cat(sprintf("Found %d trials with missing conditions before recovery.\n\n", n_missing_before_recovery))
  
  recovery_attempted <- TRUE
  recovery_error <- NULL
  
  tryCatch({
    # Load behavioral data
    behavioral_data_recovery <- readr::read_csv(behavioral_file, show_col_types = FALSE)
    
    # Normalize subject ID
    if (!"subject_id" %in% names(behavioral_data_recovery)) {
      stop("Column 'subject_id' not found in behavioral file. Available columns: ", 
           paste(names(behavioral_data_recovery), collapse = ", "))
    }
    
    # Check for task column and create normalized version
    if ("task_modality" %in% names(behavioral_data_recovery)) {
      behavioral_data_recovery$task_behav <- case_when(
        behavioral_data_recovery$task_modality == "aud" ~ "ADT",
        behavioral_data_recovery$task_modality == "vis" ~ "VDT",
        TRUE ~ as.character(behavioral_data_recovery$task_modality)
      )
    } else if ("task" %in% names(behavioral_data_recovery)) {
      behavioral_data_recovery$task_behav <- case_when(
        behavioral_data_recovery$task == "aud" ~ "ADT",
        behavioral_data_recovery$task == "vis" ~ "VDT",
        TRUE ~ as.character(behavioral_data_recovery$task)
      )
    } else {
      stop("Neither 'task_modality' nor 'task' column found in behavioral file")
    }
    
    # Normalize run (check which columns exist)
    if ("run_num" %in% names(behavioral_data_recovery)) {
      behavioral_data_recovery$run_behav <- behavioral_data_recovery$run_num
    } else if ("run" %in% names(behavioral_data_recovery)) {
      behavioral_data_recovery$run_behav <- behavioral_data_recovery$run
    } else {
      behavioral_data_recovery$run_behav <- NA_integer_
    }
    
    # Normalize trial index (check which columns exist)
    trial_cols <- c("trial_num", "trial_in_run", "trial", "trial_index")
    trial_col_found <- trial_cols[trial_cols %in% names(behavioral_data_recovery)]
    if (length(trial_col_found) > 0) {
      behavioral_data_recovery$trial_index_behav <- behavioral_data_recovery[[trial_col_found[1]]]
    } else {
      behavioral_data_recovery$trial_index_behav <- NA_integer_
    }
    
    # Extract condition variables (check which columns exist and create them)
    if ("grip_targ_prop_mvc" %in% names(behavioral_data_recovery)) {
      behavioral_data_recovery$gf_trPer_behav <- behavioral_data_recovery$grip_targ_prop_mvc
    } else if ("gf_trPer" %in% names(behavioral_data_recovery)) {
      behavioral_data_recovery$gf_trPer_behav <- behavioral_data_recovery$gf_trPer
    } else {
      behavioral_data_recovery$gf_trPer_behav <- NA_real_
    }
    
    if ("force_condition" %in% names(behavioral_data_recovery)) {
      behavioral_data_recovery$force_condition_behav <- behavioral_data_recovery$force_condition
    } else {
      behavioral_data_recovery$force_condition_behav <- NA_character_
    }
    
    if ("stim_level_index" %in% names(behavioral_data_recovery)) {
      behavioral_data_recovery$stimLev_behav <- behavioral_data_recovery$stim_level_index
    } else if ("stimLev" %in% names(behavioral_data_recovery)) {
      behavioral_data_recovery$stimLev_behav <- behavioral_data_recovery$stimLev
    } else {
      behavioral_data_recovery$stimLev_behav <- NA_real_
    }
    
    if ("stim_is_diff" %in% names(behavioral_data_recovery)) {
      behavioral_data_recovery$isOddball_behav <- as.integer(behavioral_data_recovery$stim_is_diff)
    } else if ("isOddball" %in% names(behavioral_data_recovery)) {
      behavioral_data_recovery$isOddball_behav <- as.integer(behavioral_data_recovery$isOddball)
    } else {
      behavioral_data_recovery$isOddball_behav <- NA_integer_
    }
    
    # Final cleanup and filtering - ensure trial-level data
    behavioral_data_recovery <- behavioral_data_recovery %>%
      mutate(
        # Normalize subject ID
        subject_id_behav = as.character(subject_id)
      ) %>%
      filter(!is.na(subject_id_behav), !is.na(task_behav), !is.na(run_behav), !is.na(trial_index_behav))
    
    # CRITICAL: Ensure behavioral data is trial-level (one row per trial)
    # If there are duplicates, take the first non-NA value for each condition variable
    # Helper function to safely get first non-NA value
    safe_first_na_omit <- function(x) {
      x_clean <- x[!is.na(x)]
      if (length(x_clean) > 0) {
        return(x_clean[1])
      } else {
        return(NA)
      }
    }
    
    behavioral_data_recovery <- behavioral_data_recovery %>%
      group_by(subject_id_behav, task_behav, run_behav, trial_index_behav) %>%
      summarise(
        # Take first non-NA value for each condition variable
        gf_trPer_behav = safe_first_na_omit(gf_trPer_behav),
        force_condition_behav = safe_first_na_omit(force_condition_behav),
        stimLev_behav = safe_first_na_omit(stimLev_behav),
        isOddball_behav = safe_first_na_omit(isOddball_behav),
        .groups = "drop"
      ) %>%
      # Ensure proper types
      mutate(
        gf_trPer_behav = as.numeric(gf_trPer_behav),
        force_condition_behav = as.character(force_condition_behav),
        stimLev_behav = as.numeric(stimLev_behav),
        isOddball_behav = as.integer(isOddball_behav)
      )
    
    # Verify no duplicates remain - force to one row per trial
    n_before <- nrow(behavioral_data_recovery)
    behavioral_data_recovery <- behavioral_data_recovery %>%
      distinct(subject_id_behav, task_behav, run_behav, trial_index_behav, .keep_all = TRUE)
    n_after <- nrow(behavioral_data_recovery)
    
    if (n_before != n_after) {
      cat(sprintf("Warning: Removed %d duplicate trial keys from behavioral data.\n", n_before - n_after))
    }
    
    # Identify trials with missing conditions
    trials_to_recover <- trial_coverage_prefilter %>%
      filter(is.na(effort_condition) | is.na(difficulty_level)) %>%
      select(subject_id, task, run, trial_index, effort_condition, difficulty_level)
    
    if (nrow(trials_to_recover) > 0 && nrow(behavioral_data_recovery) > 0) {
      # Ensure trials_to_recover has no duplicates
      trials_to_recover <- trials_to_recover %>%
        distinct(subject_id, task, run, trial_index, .keep_all = TRUE)
      
      # Try to match with behavioral data
      # Ensure behavioral data has no duplicates before joining
      behavioral_data_recovery_join <- behavioral_data_recovery %>%
        group_by(subject_id_behav, task_behav, run_behav, trial_index_behav) %>%
        slice(1) %>%  # Take first row if duplicates exist
        ungroup()
      
      # Perform the join
      recovery_matches <- trials_to_recover %>%
        left_join(
          behavioral_data_recovery_join,
          by = c(
            "subject_id" = "subject_id_behav",
            "task" = "task_behav",
            "run" = "run_behav",
            "trial_index" = "trial_index_behav"
          )
        )
      
      # Verify join didn't create duplicates
      if (nrow(recovery_matches) > nrow(trials_to_recover)) {
        warning(sprintf("Join created %d rows from %d trials. Deduplicating...", 
                       nrow(recovery_matches), nrow(trials_to_recover)))
        recovery_matches <- recovery_matches %>%
          distinct(subject_id, task, run, trial_index, .keep_all = TRUE)
      }
      
      # Extract recoverable conditions (only for trials that are actually missing them)
      recovery_data <- recovery_matches %>%
        mutate(
          # Recover effort condition (only if missing)
          recovered_effort = factor(case_when(
            !is.na(gf_trPer_behav) & gf_trPer_behav == 0.05 ~ "Low_5_MVC",
            !is.na(gf_trPer_behav) & gf_trPer_behav == 0.4 ~ "High_40_MVC",
            !is.na(gf_trPer_behav) & gf_trPer_behav == 0.40 ~ "High_40_MVC",
            force_condition_behav == "Low_Force_5pct" ~ "Low_5_MVC",
            force_condition_behav == "High_Force_40pct" ~ "High_40_MVC",
            TRUE ~ NA_character_
          ), levels = c("Low_5_MVC", "High_40_MVC")),
          
          # Recover difficulty level (only if missing)
          recovered_difficulty = factor(case_when(
            isOddball_behav == 0 ~ "Standard",
            !is.na(stimLev_behav) & stimLev_behav == 0 ~ "Standard",
            isOddball_behav == 1 & !is.na(stimLev_behav) & stimLev_behav %in% c(1, 2) ~ "Hard",
            isOddball_behav == 1 & !is.na(stimLev_behav) & stimLev_behav %in% c(3, 4) ~ "Easy",
            !is.na(stimLev_behav) & stimLev_behav %in% c(1, 2) ~ "Hard",
            !is.na(stimLev_behav) & stimLev_behav %in% c(3, 4) ~ "Easy",
            isOddball_behav == 1 & !is.na(stimLev_behav) & stimLev_behav %in% c(8, 16, 0.06, 0.12) ~ "Hard",
            isOddball_behav == 1 & !is.na(stimLev_behav) & stimLev_behav %in% c(32, 64, 0.24, 0.48) ~ "Easy",
            !is.na(stimLev_behav) & stimLev_behav %in% c(8, 16, 0.06, 0.12) ~ "Hard",
            !is.na(stimLev_behav) & stimLev_behav %in% c(32, 64, 0.24, 0.48) ~ "Easy",
            TRUE ~ NA_character_
          ), levels = c("Standard", "Easy", "Hard"))
        ) %>%
        select(subject_id, task, run, trial_index, recovered_effort, recovered_difficulty) %>%
        distinct(subject_id, task, run, trial_index, .keep_all = TRUE)  # Ensure no duplicates
      
      # Update trial_coverage_prefilter with recovered conditions
      # Ensure recovery_data has no duplicates before joining
      recovery_data_dedup <- recovery_data %>%
        distinct(subject_id, task, run, trial_index, .keep_all = TRUE)
      
      trial_coverage_prefilter <- trial_coverage_prefilter %>%
        left_join(recovery_data_dedup, by = c("subject_id", "task", "run", "trial_index")) %>%
        mutate(
          # Backfill effort condition where missing
          effort_condition = factor(
            ifelse(is.na(effort_condition) & !is.na(recovered_effort),
                   as.character(recovered_effort),
                   as.character(effort_condition)),
            levels = c("Low_5_MVC", "High_40_MVC")
          ),
          # Backfill difficulty level where missing
          difficulty_level = factor(
            ifelse(is.na(difficulty_level) & !is.na(recovered_difficulty),
                   as.character(recovered_difficulty),
                   as.character(difficulty_level)),
            levels = c("Standard", "Easy", "Hard")
          )
        ) %>%
        select(-recovered_effort, -recovered_difficulty)
      
      # Calculate recovery statistics
      n_recovered_effort <- sum(!is.na(recovery_data$recovered_effort) & is.na(trials_to_recover$effort_condition))
      n_recovered_difficulty <- sum(!is.na(recovery_data$recovered_difficulty) & is.na(trials_to_recover$difficulty_level))
      n_recovered_both <- sum(!is.na(recovery_data$recovered_effort) & !is.na(recovery_data$recovered_difficulty) &
                              is.na(trials_to_recover$effort_condition) & is.na(trials_to_recover$difficulty_level))
      n_attempted_recovery <- nrow(trials_to_recover)
      
      # Store recovery stats for later reporting
      recovery_stats <- list(
        attempted = n_attempted_recovery,
        recovered_effort = n_recovered_effort,
        recovered_difficulty = n_recovered_difficulty,
        recovered_both = n_recovered_both
      )
      assign("condition_recovery_stats", recovery_stats, envir = .GlobalEnv)
      
      cat(sprintf("✓ Recovery attempt completed:\n"))
      cat(sprintf("  - Attempted recovery for %d trials with missing conditions\n", n_attempted_recovery))
      cat(sprintf("  - Recovered %d effort conditions (%.1f%% of attempted)\n", 
                 n_recovered_effort, 100 * n_recovered_effort / max(n_attempted_recovery, 1)))
      cat(sprintf("  - Recovered %d difficulty levels (%.1f%% of attempted)\n", 
                 n_recovered_difficulty, 100 * n_recovered_difficulty / max(n_attempted_recovery, 1)))
      cat(sprintf("  - Recovered both conditions for %d trials\n\n", n_recovered_both))
    } else {
      cat("No trials with missing conditions found to recover.\n\n")
      # Still create stats for reporting
      recovery_stats <- list(
        attempted = 0,
        recovered_effort = 0,
        recovered_difficulty = 0,
        recovered_both = 0
      )
      assign("condition_recovery_stats", recovery_stats, envir = .GlobalEnv)
    }
  }, error = function(e) {
    recovery_error_msg <- e$message
    cat(sprintf("Warning: Could not recover conditions from behavioral file: %s\n", recovery_error_msg))
    # Create empty stats to show recovery was attempted but failed
    recovery_stats <- list(
      attempted = n_missing_before_recovery,
      recovered_effort = 0,
      recovered_difficulty = 0,
      recovered_both = 0,
      error = recovery_error_msg
    )
    assign("condition_recovery_stats", recovery_stats, envir = .GlobalEnv)
  })
} else {
  if (!file.exists(behavioral_file)) {
    cat("Skipping recovery: Behavioral file not found at:", behavioral_file, "\n")
  } else if (n_missing_before_recovery == 0) {
    cat("Skipping recovery: No missing conditions found.\n")
  }
}

# Add canonical trial_id for all counting operations
trial_coverage_prefilter <- trial_coverage_prefilter %>%
  mutate(
    trial_id = paste(subject_id, task, run, trial_index, sep = ":")
  )

# Derive a strict trial-level table (one row per trial_id)
trial_coverage_trial <- trial_coverage_prefilter %>%
  group_by(trial_id) %>%
  slice(1) %>%
  ungroup()

# Hard assertions to guarantee 1 row per trial
n_unique_trials <- n_distinct(trial_coverage_prefilter$trial_id)
if (nrow(trial_coverage_trial) != n_unique_trials) {
  stop("trial_coverage_trial is not truly trial-level: nrow != n_distinct(trial_id).")
}
if (any(duplicated(trial_coverage_trial$trial_id))) {
  stop("trial_coverage_trial contains duplicated trial_id values.")
}

# Validation: Check if we're accidentally counting samples instead of trials
# Check 1: Row count should equal distinct trial count (if not, we have duplicates)
# Check 2: Trial counts should be reasonable (typically 50-200 per subject-task)
validation_check <- trial_coverage_prefilter %>%
  group_by(subject_id, task) %>%
  summarise(
    n_trials_actual = n_distinct(trial_id),
    n_rows = n(),
    .groups = "drop"
  ) %>%
  mutate(
    # Flag if row count != trial count (indicates duplicate rows)
    has_duplicates = n_rows != n_trials_actual,
    # Flag if trial count is suspiciously high (>500 suggests counting samples)
    suspiciously_high = n_trials_actual > 500,
    suspicious = has_duplicates | suspiciously_high
  )

# Store validation results for display in report (will be enriched with expected_trials later)
validation_warnings <- validation_check %>%
  filter(suspicious) %>%
  select(subject_id, task, n_trials_actual, n_rows, has_duplicates, suspiciously_high)

# Build threshold sweep (for gates) based on params$threshold_grid
threshold_grid <- params$threshold_grid %||% c(0.50, 0.60, 0.70, 0.80, 0.90, 0.95)

gate_flags <- function(df, thr) {
  # Independent analysis-specific gates (NOT nested)
  # Each gate is appropriate for different analysis types
  
  # Use event-relative prestim if available, otherwise fall back to hard-coded window
  prestim_col <- if ("valid_prestim_fix_interior" %in% names(df)) {
    df$valid_prestim_fix_interior
  } else if ("valid_prop_prestim_fix_interior" %in% names(df)) {
    df$valid_prop_prestim_fix_interior
  } else {
    df$valid_prop_prestim  # Fallback to hard-coded 3.25-3.75
  }
  
  tibble(
    # DEPRECATED: Old nested gates (kept for backwards compatibility only)
    gate_A = !is.na(df$valid_prop_baseline_500ms) & !is.na(df$valid_prop_prestim) &
      df$valid_prop_baseline_500ms >= thr & df$valid_prop_prestim >= thr,
    gate_B = !is.na(df$valid_prop_total_auc) &
      df$valid_prop_total_auc >= thr &
      !is.na(df$valid_prop_baseline_500ms) & !is.na(df$valid_prop_prestim) &
      df$valid_prop_baseline_500ms >= thr & df$valid_prop_prestim >= thr,
    gate_C = !is.na(df$valid_prop_cognitive_auc) &
      df$valid_prop_cognitive_auc >= thr &
      !is.na(df$valid_prop_baseline_500ms) & !is.na(df$valid_prop_prestim) &
      df$valid_prop_baseline_500ms >= thr & df$valid_prop_prestim >= thr,
    # NEW: Independent analysis-specific gates
    gate_stimlocked = !is.na(df$valid_prop_baseline_500ms) & !is.na(prestim_col) &
      df$valid_prop_baseline_500ms >= thr & prestim_col >= thr,
    gate_total_auc = !is.na(df$valid_prop_total_auc) &
      df$valid_prop_total_auc >= thr,
    gate_cog_auc = !is.na(df$valid_prop_cognitive_auc) & !is.na(df$valid_prop_baseline_500ms) &
      df$valid_prop_cognitive_auc >= thr & df$valid_prop_baseline_500ms >= thr
  )
}

threshold_sweep <- purrr::map_dfr(threshold_grid, function(thr) {
  gf <- gate_flags(trial_coverage_prefilter, thr)
  tibble(
    trial_id = trial_coverage_prefilter$trial_id,
    subject_id = trial_coverage_prefilter$subject_id,
    task = trial_coverage_prefilter$task,
    effort_condition = trial_coverage_prefilter$effort_condition,
    difficulty_level = trial_coverage_prefilter$difficulty_level,
    threshold = thr,
    # DEPRECATED: Old nested gates
    gate_A = gf$gate_A,
    gate_B = gf$gate_B,
    gate_C = gf$gate_C,
    # NEW: Independent analysis-specific gates
    gate_stimlocked = gf$gate_stimlocked,
    gate_total_auc = gf$gate_total_auc,
    gate_cog_auc = gf$gate_cog_auc
  ) %>%
    tidyr::pivot_longer(cols = starts_with("gate_"), names_to = "gate", values_to = "retain") %>%
    group_by(subject_id, task, effort_condition, difficulty_level, threshold, gate) %>%
    summarise(n_trials_retained = n_distinct(trial_id[retain]), .groups = "drop")
})

saveRDS(threshold_sweep, threshold_sweep_cache_file)
```

## Methods and Quality Gates

This section defines the quality gates used to determine trial usability for different research questions. Each gate requires a minimum proportion of valid pupil samples within specific time windows.

### Quality Gates: Analysis-Specific Gates (Independent, Not Nested)

**Stimulus-locked gate (`gate_stimlocked`)**:
- **Required Windows**: 
  - Baseline: -0.5s to 0s (last 500ms of ITI_Baseline)
  - Prestimulus: Event-relative fixation interior window (fixST + 0.10s to fixOFSTP - 0.10s)
    - Falls back to 3.25s to 3.75s if event timestamps unavailable
- **Purpose**: Ensures trials have valid baseline correction and prestimulus quality for stimulus-locked analyses
- **Use Case**: Stimulus-locked baseline-corrected measures, waveform visualizations
- **Research Questions**: All analyses requiring baseline correction and stimulus-locked timing

**Total-AUC gate (`gate_total_auc`)**:
- **Required Windows**: 
  - Total AUC window: 0s to response onset (4.7s + RT)
- **Purpose**: Ensures trials have valid pupil data throughout the entire task-evoked period
- **Use Case**: Analyses requiring Total AUC (full task-evoked pupil response)
- **Research Questions**: 
  - **Chapter 2**: Effort–pupil manipulation check (Total AUC as global arousal marker)
  - **Chapter 2**: Subject-level coupling analyses (Effort-induced changes in Total AUC)
  - Any analysis examining the combined effects of physical and cognitive effort

**Cognitive-AUC gate (`gate_cog_auc`)**:
- **Required Windows**: 
  - Baseline: -0.5s to 0s (for baseline correction)
  - Cognitive AUC window: 4.65s to response onset (4.7s + RT)
- **Purpose**: Ensures trials have valid pupil data during the cognitive task period (post-target stimulus) with baseline correction
- **Use Case**: Analyses requiring Cognitive AUC (isolated cognitive TEPR) with baseline correction
- **Research Questions**: 
  - **Chapter 2**: Primary psychometric–pupil analysis (trial-wise Cognitive AUC tertiles)
  - **Chapter 2**: Secondary continuous-pupil analyses (Cognitive AUC as continuous predictor)
  - **Chapter 2**: Subject-level coupling (Effort-induced changes in Cognitive AUC)
  - **Chapter 3**: DDM with Pupil Predictors (response-locked cognitive window)
  - Any analysis examining cognitive effort effects independent of physical effort

### Gate Requirements by Research Question

| Research Question | Required Gates | Rationale |
|------------------|----------------|-----------|
| **Chapter 2: Psychometric–Pupil Coupling** | `gate_cog_auc` (minimum) | Cognitive AUC needed for trial-wise and subject-wise arousal markers |
| **Chapter 2: Effort–Pupil Manipulation** | `gate_total_auc` + `gate_cog_auc` | Both Total AUC (global) and Cognitive AUC (cognitive) needed |
| **Chapter 3: DDM with Pupil Predictors** | `gate_cog_auc` (minimum) | Pupil data during response period (4.65s to response) needed for response-locked analyses |
| **Chapter 3: Behavior-Only DDM** | None (behavioral gates only) | No pupil quality gates required |
| **Waveform Visualizations** | `gate_stimlocked` (minimum) | Baseline correction needed for interpretable waveforms |

**Important Note**: These gates are **independent** (not nested). Each gate is appropriate for different analysis types:
- Use `gate_stimlocked` for stimulus-locked analyses requiring baseline correction
- Use `gate_total_auc` for total AUC analyses (does not require baseline/prestim)
- Use `gate_cog_auc` for cognitive AUC analyses (includes baseline for correction)

**DEPRECATED**: The old nested gate system (Stimulus-locked gate/B/C) is preserved for backwards compatibility but should not be used for new analyses.

### Quality Thresholds

Trials are evaluated at multiple validity thresholds (default: 0.50, 0.60, 0.70, 0.80, 0.90, 0.95) to assess sensitivity. The default threshold of **0.80** (80% valid samples) is used for:
- **Chapter 2**: High and Medium quality trials (overall quality ≥ 0.60 for primary analysis, ≥ 0.50 for secondary)
- **Chapter 3**: Conservative subset for DDM with pupil predictors (overall quality ≥ 0.60)

Lower thresholds (0.50–0.70) may be used for exploratory analyses or when sample size is limited, but should be justified based on the threshold sensitivity analyses shown in this report.

### Gate Sensitivity: Prestimulus as the Limiter

```{r gate-sensitivity-prestim, message=FALSE, warning=FALSE, fig.width=10, fig.height=5}
if (exists("trial_coverage_trial") && !is.null(trial_coverage_trial)) {
  thr_080 <- 0.80

  # Work from strict trial-level table
  df_080 <- trial_coverage_trial %>%
    dplyr::mutate(
      baseline_ok   = !is.na(valid_prop_baseline_500ms)   & valid_prop_baseline_500ms   >= thr_080,
      prestim_ok    = !is.na(valid_prop_prestim)          & valid_prop_prestim          >= thr_080,
      cognitive_ok  = !is.na(valid_prop_cognitive_auc)    & valid_prop_cognitive_auc    >= thr_080,
      has_rt        = !is.na(rt),
      has_resp_win  = isTRUE(has_response_window)
    )

  # Mutually exclusive buckets at T = 0.80
  df_080 <- df_080 %>%
    dplyr::mutate(
      bucket_080 = dplyr::case_when(
        baseline_ok & cognitive_ok & prestim_ok ~ "baseline_ok & cognitive_ok & prestim_ok",
        baseline_ok & cognitive_ok & !prestim_ok ~ "baseline_ok & cognitive_ok & prestim_fail",
        baseline_ok & !cognitive_ok ~ "baseline_ok & cognitive_fail",
        !baseline_ok ~ "baseline_fail",
        TRUE ~ "other_or_incomplete"
      )
    )

  bucket_counts_080 <- df_080 %>%
    dplyr::count(bucket_080, name = "n_trials") %>%
    dplyr::mutate(
      bucket_080 = factor(
        bucket_080,
        levels = c(
          "baseline_ok & cognitive_ok & prestim_ok",
          "baseline_ok & cognitive_ok & prestim_fail",
          "baseline_ok & cognitive_fail",
          "baseline_fail",
          "other_or_incomplete"
        )
      )
    )

  p_buckets_080 <- ggplot(bucket_counts_080, aes(x = bucket_080, y = n_trials)) +
    geom_col(fill = "#3182bd") +
    labs(
      title = "Gate sensitivity at T = 0.80: Prestimulus as the limiter",
      x = "Mutually exclusive bucket",
      y = "Number of trials"
    ) +
    theme_bw() +
    theme(
      axis.text.x = element_text(angle = 30, hjust = 1)
    )

  # Cognitive-AUC gate variants across thresholds (analysis-specific gates are independent)
  gate_thresholds <- params$export_thresholds %||% c(0.50, 0.60, 0.70, 0.80, 0.90)

  gate_sensitivity <- purrr::map_dfr(gate_thresholds, function(thr) {
    df_thr <- trial_coverage_trial %>%
      dplyr::mutate(
        baseline_ok   = !is.na(valid_prop_baseline_500ms)   & valid_prop_baseline_500ms   >= thr,
        prestim_ok    = !is.na(valid_prop_prestim)          & valid_prop_prestim          >= thr,
        cognitive_ok  = !is.na(valid_prop_cognitive_auc)    & valid_prop_cognitive_auc    >= thr,
        has_rt        = !is.na(rt),
        has_resp_win  = isTRUE(has_response_window),
        # Cognitive-AUC gate (baseline + cognitive) - independent gate
        gate_cog_auc_T = baseline_ok & cognitive_ok,
        # Baseline-only + cognitive + response window + RT present (variant for robustness)
        gate_cog_auc_baseline_only_T = baseline_ok & cognitive_ok & has_resp_win & has_rt
      )

    tibble::tibble(
      threshold = thr,
      gate = c("gate_cog_auc_T", "gate_cog_auc_baseline_only_T"),
      n_trials_retained = c(
        sum(df_thr$gate_cog_auc_T, na.rm = TRUE),
        sum(df_thr$gate_cog_auc_baseline_only_T, na.rm = TRUE)
      )
    )
  })

  p_gate_sensitivity <- gate_sensitivity %>%
    dplyr::mutate(
      threshold = as.factor(threshold),
      gate = factor(gate, levels = c("gate_cog_auc_T", "gate_cog_auc_baseline_only_T"))
    ) %>%
    ggplot(aes(x = threshold, y = n_trials_retained, fill = gate)) +
    geom_col(position = "dodge") +
    scale_fill_manual(
      values = c("gate_cog_auc_T" = "#08519c", "gate_cog_auc_baseline_only_T" = "#6baed6"),
      name = "Gate definition",
      labels = c(
        "gate_cog_auc_T" = "Cognitive-AUC gate (baseline + cognitive)",
        "gate_cog_auc_baseline_only_T" = "Cognitive-AUC gate variant (baseline + cognitive + RT window)"
      )
    ) +
    labs(
      title = "Cognitive-AUC gate (gate_cog_auc) retention across thresholds (analysis-specific gates are independent)",
      x = "Validity threshold T",
      y = "Number of trials retained"
    ) +
    theme_bw()

  p_buckets_080 + p_gate_sensitivity
} else {
  cat("Gate sensitivity plots require `trial_coverage_trial`. Run the raw coverage section first.\n")
}
```

### Processing Methods

#### Baseline Correction

- **Baseline Window**: -0.5s to 0s (last 500ms of ITI_Baseline)
- **Baseline (B₀)**: Mean pupil diameter in baseline window
- **Correction**: `pupil_isolated = pupil - baseline_B0`
- **Purpose**: Converge all conditions at squeeze onset (time = 0) to isolate task-evoked responses

#### AUC Calculation (Zenon et al. 2014 Method)

**Total AUC**:
- **Data**: Raw pupil diameter (no baseline correction)
- **Window**: From trial onset (0s) to trial-specific response onset
- **Response Onset**: `4.7s + RT` (response window start + RT)
- **Method**: Trapezoidal integration
- **Interpretation**: Full task-evoked pupil response (TEPR) including physical and cognitive demands

**Cognitive AUC**:
- **Data**: Baseline-corrected pupil (`pupil_isolated`)
- **Window**: From 300ms after target stimulus onset (4.65s) to trial-specific response onset
- **Target Stimulus Onset**: 4.35s
- **Method**: Trapezoidal integration
- **Interpretation**: Isolated cognitive TEPR, controlling for physical effort and baseline differences

#### Difficulty Level Mapping

- **Standard**: `isOddball == 0` OR `stimLev == 0`
- **Easy**: `isOddball == 1` AND `stimLev` in `[3, 4]` (higher stimulus levels)
- **Hard**: `isOddball == 1` AND `stimLev` in `[1, 2]` (lower stimulus levels)

#### Effort Condition Mapping

- **Low Effort**: `gf_trPer == 0.05` → `effort_condition = "Low_5_MVC"`
- **High Effort**: `gf_trPer == 0.40` → `effort_condition = "High_40_MVC"`

#### Trial Structure

**Time Reference**: Squeeze onset = 0s

| Phase | Time Window | Duration | Description |
|-------|-------------|----------|-------------|
| ITI_Baseline | -3.0 to 0s | 3s | Pre-trial baseline |
| Squeeze | 0 to 3.0s | 3s | Handgrip force manipulation |
| Post_Squeeze_Blank | 3.0 to 3.25s | 250ms | Post-squeeze blank |
| Pre_Stimulus_Fixation | 3.25 to 3.75s | 500ms | Pre-stimulus fixation |
| Stimulus | 3.75 to 4.45s | 700ms | Standard (100ms) + ISI (500ms) + Target (100ms) |
| Post_Stimulus_Fixation | 4.45 to 4.7s | 250ms | Post-stimulus fixation |
| Response_Different | 4.7 to 7.7s | 3000ms | Response period |

**Key Time Points**:
- **Trial Onset (Squeeze)**: 0s
- **Baseline Window**: -0.5s to 0s (for B₀ calculation)
- **Target Onset**: 4.35s
- **Response Window Start**: 4.7s
- **Cognitive AUC Start**: 4.65s (4.35s + 0.3s latency)

## Subject Overview (Pre-Filter)

```{r raw-status-subject-overview, results='asis'}
if (!is.null(trial_coverage_prefilter)) {
  # Expected trials from behavioral data (if available).
  # We load directly from the behavioral CSV defined in the setup chunk,
  # rather than relying on analysis-ready files which are post-filter.
  if (file.exists(behavioral_file)) {
    behav_expected <- readr::read_csv(behavioral_file, show_col_types = FALSE)

    expected_trials <- behav_expected %>%
      mutate(
        sub = if ("sub" %in% names(.)) sub else if ("subject_id" %in% names(.)) as.character(subject_id) else NA_character_,
        task = if ("task" %in% names(.)) {
          dplyr::if_else(task == "aud", "ADT",
                        dplyr::if_else(task == "vis", "VDT", as.character(task)))
        } else if ("task_modality" %in% names(.)) {
          dplyr::case_when(
            task_modality == "aud" ~ "ADT",
            task_modality == "vis" ~ "VDT",
            TRUE ~ as.character(task_modality)
          )
        } else NA_character_
      ) %>%
      filter(!is.na(sub), !is.na(task)) %>%
      transmute(subject_id = as.character(sub), task) %>%
      group_by(subject_id) %>%
      summarise(expected_trials = n(), .groups = "drop")
  } else {
    expected_trials <- NULL
    behav_expected <- NULL
  }

  # Gate logic at threshold 0.80
  t0 <- 0.80

  # Expected trials by subject AND task
  if (!is.null(expected_trials) && !is.null(behav_expected)) {
    expected_trials_by_task <- behav_expected %>%
      mutate(
        sub = if ("sub" %in% names(.)) sub else if ("subject_id" %in% names(.)) as.character(subject_id) else NA_character_,
        task = if ("task" %in% names(.)) {
          dplyr::if_else(task == "aud", "ADT",
                        dplyr::if_else(task == "vis", "VDT", as.character(task)))
        } else if ("task_modality" %in% names(.)) {
          dplyr::case_when(
            task_modality == "aud" ~ "ADT",
            task_modality == "vis" ~ "VDT",
            TRUE ~ as.character(task_modality)
          )
        } else NA_character_
      ) %>%
      filter(!is.na(sub), !is.na(task)) %>%
      transmute(subject_id = as.character(sub), task) %>%
      group_by(subject_id, task) %>%
      summarise(expected_trials = n(), .groups = "drop")
  } else {
    expected_trials_by_task <- tibble(subject_id = character(), task = character(), expected_trials = integer())
  }

  # Display validation warnings if any (now that expected_trials_by_task is available)
  if (exists("validation_warnings") && nrow(validation_warnings) > 0) {
    validation_warnings_enriched <- validation_warnings %>%
      left_join(
        if (!is.null(expected_trials_by_task)) expected_trials_by_task else
          tibble(subject_id = character(), task = character(), expected_trials = integer()),
        by = c("subject_id", "task")
      ) %>%
      mutate(
        expected_trials = ifelse(is.na(expected_trials), n_trials_actual, expected_trials),
        ratio = n_trials_actual / expected_trials,
        issue = case_when(
          has_duplicates & suspiciously_high ~ "Duplicates + High count",
          has_duplicates ~ "Duplicate rows detected",
          suspiciously_high ~ "Suspiciously high count (>500)",
          TRUE ~ "Unknown"
        )
      )
    
    cat("\n\n### ⚠️ VALIDATION WARNING: Possible Sample vs Trial Counting Issue\n\n")
    cat("The following subject-task combinations show suspicious trial counts:\n\n")
    
    validation_warnings_enriched %>%
      select(subject_id, task, n_trials_actual, expected_trials, ratio, n_rows, issue) %>%
      kable(
        col.names = c("Subject", "Task", "Actual Trials", "Expected", "Ratio", "Rows", "Issue"),
        caption = "⚠️ Likely counting samples instead of trials",
        digits = 1
      ) %>%
      kable_styling(
        bootstrap_options = c("striped", "hover", "condensed"),
        full_width = FALSE
      ) %>%
      print()
    
    cat("\n**Action**: Check that `trial_coverage_prefilter` is collapsed to one row per trial.\n")
    cat("**Expected**: Rows should equal Actual Trials, and Ratio should be close to 1.0.\n\n")
  }

  subj_overview <- trial_coverage_prefilter %>%
    group_by(subject_id, task) %>%
    summarise(
      trials_with_any_pupil = sum(has_any_pupil, na.rm = TRUE),
      n_trials = n_distinct(trial_id),  # Use distinct trial_id to ensure we count trials, not samples
      .groups = "drop"
    ) %>%
    left_join(
      if (!is.null(expected_trials_by_task)) expected_trials_by_task else
        tibble(subject_id = character(), task = character(), expected_trials = integer()),
      by = c("subject_id", "task")
    )

  # Attach gate-based usable trial counts at 0.80 (by subject and task)
  if (!is.null(threshold_sweep)) {
    gate_stats_080 <- threshold_sweep %>%
      filter(abs(threshold - t0) < 1e-6) %>%
      mutate(
        gate = dplyr::case_when(
          # DEPRECATED: Old nested gates
          gate == "gate_A" ~ "gate_A_iti_prestim_DEPRECATED",
          gate == "gate_B" ~ "gate_B_total_auc_DEPRECATED",
          gate == "gate_C" ~ "gate_C_cognitive_auc_DEPRECATED",
          # NEW: Analysis-specific gates
          gate == "gate_stimlocked" ~ "gate_stimlocked",
          gate == "gate_total_auc" ~ "gate_total_auc",
          gate == "gate_cog_auc" ~ "gate_cog_auc",
          TRUE ~ gate
        )
      ) %>%
      group_by(subject_id, task, gate) %>%
      summarise(n_trials_retained = sum(n_trials_retained, na.rm = TRUE), .groups = "drop") %>%
      pivot_wider(names_from = gate, values_from = n_trials_retained, values_fill = 0)

    subj_overview <- subj_overview %>%
      left_join(gate_stats_080, by = c("subject_id", "task"))
  }

  subj_overview %>%
    mutate(
      expected_trials = ifelse(is.na(expected_trials), n_trials, expected_trials),
      pct_any_pupil = round(100 * trials_with_any_pupil / expected_trials, 1)
    ) %>%
    arrange(subject_id, task) %>%
    kable(
      caption = "Subject-level overview (pre-filter) by task: expected vs any-pupil vs usable trials at 0.80 (analysis-specific gates are independent)",
      col.names = c(
        "Subject", "Task", "Trials with Any Pupil", "Total Trials (Coverage Table)",
        "Expected Trials (Behavioral)", "Stimulus-locked gate", "Total-AUC gate",
        "Cognitive-AUC gate", "% Any Pupil / Expected"
      )
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      full_width = TRUE,
      font_size = 10
    ) %>%
    scroll_box(height = "400px")
}
```

## Condition Labeling Diagnostics

```{r condition-labeling-diagnostics, results='asis'}
if (!is.null(trial_coverage_prefilter)) {
  # Check for missing condition labels (after recovery attempt)
  # Prefer strict trial-level table to guarantee 1 row per trial
  coverage_source <- if (exists("trial_coverage_trial") && !is.null(trial_coverage_trial)) {
    trial_coverage_trial
  } else {
    trial_coverage_prefilter
  }

  missing_conditions <- coverage_source %>%
    summarise(
      total_trials = n(),
      missing_effort = sum(is.na(effort_condition)),
      missing_difficulty = sum(is.na(difficulty_level)),
      missing_both = sum(is.na(effort_condition) & is.na(difficulty_level)),
      missing_either = sum(is.na(effort_condition) | is.na(difficulty_level)),
      .groups = "drop"
    )
  
  cat("### Condition Labeling Summary (After Recovery Attempt)\n\n")
  
  # Show recovery statistics if available
  if (exists("condition_recovery_stats")) {
    rec_stats <- condition_recovery_stats
    cat("#### Recovery Statistics\n\n")
    cat("- **Trials with missing conditions (before recovery)**:", format(rec_stats$attempted, big.mark = ","), "\n")
    if (rec_stats$attempted > 0) {
      cat("- **Effort conditions recovered**:", format(rec_stats$recovered_effort, big.mark = ","),
          sprintf("(%.1f%% of attempted)\n", 100 * rec_stats$recovered_effort / max(rec_stats$attempted, 1)))
      cat("- **Difficulty levels recovered**:", format(rec_stats$recovered_difficulty, big.mark = ","),
          sprintf("(%.1f%% of attempted)\n", 100 * rec_stats$recovered_difficulty / max(rec_stats$attempted, 1)))
      cat("- **Both conditions recovered**:", format(rec_stats$recovered_both, big.mark = ","), "\n")
      if (!is.null(rec_stats$error)) {
        cat("\n**Recovery Error**:", rec_stats$error, "\n")
      }
      cat("\n")
    } else {
      cat("- **No recovery attempted** (no missing conditions found before recovery)\n\n")
    }
  } else {
    cat("#### Recovery Statistics\n\n")
    cat("- **Recovery was not attempted** (recovery statistics not available)\n")
    cat("  This may indicate the recovery code did not run or encountered an issue.\n\n")
  }
  
  cat("#### Current Status (After Recovery)\n\n")
  cat("- **Total trials**:", format(missing_conditions$total_trials, big.mark = ","), "\n")
  cat("- **Missing effort condition**:", format(missing_conditions$missing_effort, big.mark = ","), 
      "(", round(100 * missing_conditions$missing_effort / missing_conditions$total_trials, 1), "%)\n")
  cat("- **Missing difficulty level**:", format(missing_conditions$missing_difficulty, big.mark = ","),
      "(", round(100 * missing_conditions$missing_difficulty / missing_conditions$total_trials, 1), "%)\n")
  cat("- **Missing both**:", format(missing_conditions$missing_both, big.mark = ","),
      "(", round(100 * missing_conditions$missing_both / missing_conditions$total_trials, 1), "%)\n")
  cat("- **Missing either condition**:", format(missing_conditions$missing_either, big.mark = ","),
      "(", round(100 * missing_conditions$missing_either / missing_conditions$total_trials, 1), "%)\n")
  cat("\n**Note**: A recovery attempt was made to backfill missing conditions from the behavioral file.\n")
  cat("Trials that still have missing conditions after recovery should be investigated in the source data.\n\n")
  
  # Sample of trials with missing conditions to diagnose
  if (missing_conditions$missing_either > 0) {
    # Check if raw columns exist (they might not if cache was built before this update)
    has_raw_cols <- all(c("raw_gf_trPer", "raw_force_condition", "raw_isOddball", "raw_stimLev") %in% names(trial_coverage_prefilter))
    
    if (has_raw_cols) {
      sample_missing <- trial_coverage_prefilter %>%
        filter(is.na(effort_condition) | is.na(difficulty_level)) %>%
        select(subject_id, task, run, trial_index, effort_condition, difficulty_level,
               raw_gf_trPer, raw_force_condition, raw_isOddball, raw_stimLev) %>%
        head(20)
      
      cat("### Sample of Trials with Missing Conditions (first 20)\n\n")
      cat("This table shows the raw source values that failed to match the expected patterns:\n\n")
      
      sample_missing %>%
        knitr::kable(
          caption = "Trials with missing effort_condition or difficulty_level, showing raw source values",
          digits = 3
        ) %>%
        kable_styling(
          bootstrap_options = c("striped", "hover", "condensed"),
          full_width = FALSE
        ) %>%
        print()
      
      cat("\n\n")
      
      # Summary of raw values for missing conditions
      effort_diagnostic <- trial_coverage_prefilter %>%
        filter(is.na(effort_condition)) %>%
        summarise(
          n = n(),
          has_gf_trPer = sum(!is.na(raw_gf_trPer)),
          has_force_condition = sum(!is.na(raw_force_condition)),
          unique_gf_trPer = paste(unique(na.omit(raw_gf_trPer)), collapse = ", "),
          unique_force_condition = paste(unique(na.omit(raw_force_condition)), collapse = ", "),
          .groups = "drop"
        )
      
      difficulty_diagnostic <- trial_coverage_prefilter %>%
        filter(is.na(difficulty_level)) %>%
        summarise(
          n = n(),
          has_isOddball = sum(!is.na(raw_isOddball)),
          has_stimLev = sum(!is.na(raw_stimLev)),
          unique_isOddball = paste(unique(na.omit(raw_isOddball)), collapse = ", "),
          unique_stimLev = paste(unique(na.omit(raw_stimLev)), collapse = ", "),
          .groups = "drop"
        )
      
      cat("### Diagnostic: Why Conditions Are Missing\n\n")
      
      if (effort_diagnostic$n > 0) {
        cat("**Effort condition missing for", effort_diagnostic$n, "trials:**\n")
        cat("- Trials with `gf_trPer`:", effort_diagnostic$has_gf_trPer, "\n")
        cat("- Trials with `force_condition`:", effort_diagnostic$has_force_condition, "\n")
        if (nchar(effort_diagnostic$unique_gf_trPer) > 0) {
          cat("- Unique `gf_trPer` values found:", effort_diagnostic$unique_gf_trPer, "\n")
          cat("  (Expected: 0.05 or 0.40)\n")
        }
        if (nchar(effort_diagnostic$unique_force_condition) > 0) {
          cat("- Unique `force_condition` values found:", effort_diagnostic$unique_force_condition, "\n")
          cat("  (Expected: 'Low_Force_5pct' or 'High_Force_40pct')\n")
        }
        cat("\n")
      }
      
      if (difficulty_diagnostic$n > 0) {
        cat("**Difficulty level missing for", difficulty_diagnostic$n, "trials:**\n")
        cat("- Trials with `isOddball`:", difficulty_diagnostic$has_isOddball, "\n")
        cat("- Trials with `stimLev`:", difficulty_diagnostic$has_stimLev, "\n")
        if (nchar(difficulty_diagnostic$unique_isOddball) > 0) {
          cat("- Unique `isOddball` values found:", difficulty_diagnostic$unique_isOddball, "\n")
          cat("  (Expected: 0 for Standard, 1 for Oddball)\n")
        }
        if (nchar(difficulty_diagnostic$unique_stimLev) > 0) {
          cat("- Unique `stimLev` values found:", difficulty_diagnostic$unique_stimLev, "\n")
          cat("  (Expected: 8/16/0.06/0.12 for Hard, 32/64/0.24/0.48 for Easy)\n")
        }
        cat("\n")
      }
    } else {
      # Fallback: show basic info without raw columns
      sample_missing <- trial_coverage_prefilter %>%
        filter(is.na(effort_condition) | is.na(difficulty_level)) %>%
        select(subject_id, task, run, trial_index, effort_condition, difficulty_level) %>%
        head(20)
      
      cat("### Sample of Trials with Missing Conditions (first 20)\n\n")
      
      sample_missing %>%
        knitr::kable(
          caption = "Trials with missing effort_condition or difficulty_level"
        ) %>%
        kable_styling(
          bootstrap_options = c("striped", "hover", "condensed"),
          full_width = FALSE
        ) %>%
        print()
      
      cat("\n\n")
      cat("**Note**: To see detailed diagnostics of why conditions are missing (showing raw source values),\n")
      cat("re-run with `recompute_raw_coverage = TRUE` to rebuild the cache with raw source values.\n\n")
    }
    
    cat("**Expected patterns:**\n")
    cat("- **Effort**: `gf_trPer` == 0.05 or 0.40, OR `force_condition` == 'Low_Force_5pct' or 'High_Force_40pct'\n")
    cat("- **Difficulty**: `isOddball` == 0 (Standard), OR `isOddball` == 1 with `stimLev` in [8,16,0.06,0.12] (Hard) or [32,64,0.24,0.48] (Easy)\n\n")
  }
}
```

## Condition Labeling Completeness (Prompt 4)

```{r condition-labeling-completeness-panel, results='asis'}
if (!is.null(trial_coverage_prefilter)) {
  cat("## Condition Labeling Completeness\n\n")
  cat("This panel shows the percentage of trials with missing effort or difficulty labels by subject and task.\n")
  cat("**Note**: Missing conditions are a data quality issue. Trials with missing conditions are excluded from condition-based analyses.\n\n")
  
  labeling_completeness <- trial_coverage_prefilter %>%
    group_by(subject_id, task) %>%
    summarise(
      n_trials = n_distinct(trial_id),
      n_unknown_effort = n_distinct(trial_id[is.na(effort_condition)]),
      n_unknown_difficulty = n_distinct(trial_id[is.na(difficulty_level)]),
      n_unknown_both = n_distinct(trial_id[is.na(effort_condition) & is.na(difficulty_level)]),
      .groups = "drop"
    ) %>%
    mutate(
      pct_unknown_effort = round(100 * n_unknown_effort / n_trials, 1),
      pct_unknown_difficulty = round(100 * n_unknown_difficulty / n_trials, 1),
      pct_unknown_both = round(100 * n_unknown_both / n_trials, 1),
      worst_offender = pct_unknown_effort > 20 | pct_unknown_difficulty > 20
    )
  
  cat("### Subject-Level Condition Labeling Completeness\n\n")
  
  labeling_completeness_sorted <- labeling_completeness %>%
    arrange(desc(pct_unknown_effort + pct_unknown_difficulty))
  
  labeling_completeness_sorted %>%
    select(subject_id, task, n_trials, 
           pct_unknown_effort, pct_unknown_difficulty, pct_unknown_both) %>%
    kable(
      col.names = c("Subject", "Task", "Total Trials", 
                   "% Missing Effort", "% Missing Difficulty", "% Missing Both"),
      caption = "Condition labeling completeness by subject and task (sorted by worst offenders)",
      digits = 1
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      full_width = FALSE
    ) %>%
    row_spec(which(labeling_completeness_sorted$worst_offender), 
             background = "#ffcccc") %>%
    print()
  
  # Summary by task
  cat("\n### Condition Labeling Completeness Summary by Task\n\n")
  
  labeling_completeness %>%
    group_by(task) %>%
    summarise(
      n_subjects = n(),
      mean_pct_unknown_effort = round(mean(pct_unknown_effort, na.rm = TRUE), 1),
      mean_pct_unknown_difficulty = round(mean(pct_unknown_difficulty, na.rm = TRUE), 1),
      n_worst_offenders = sum(worst_offender, na.rm = TRUE),
      .groups = "drop"
    ) %>%
    kable(
      col.names = c("Task", "N Subjects", "Mean % Missing Effort", 
                   "Mean % Missing Difficulty", "N Worst Offenders (>20%)"),
      caption = "Task-level summary of condition labeling completeness",
      digits = 1
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      full_width = FALSE
    ) %>%
    print()
  
  cat("\n**Worst offenders** (highlighted in red) are subjects with >20% missing effort or difficulty labels.\n\n")
}
```

## Condition Breakdown by Threshold (Cognitive-AUC gate)

```{r raw-status-condition-breakdown, results='asis'}
if (!is.null(threshold_sweep)) {
  # Filter out rows with missing conditions - these should not exist in the data
  # Using cognitive-AUC gate (gate_cog_auc) for this analysis
  sweep_gateC <- threshold_sweep %>%
    filter(gate == "gate_cog_auc", threshold %in% c(0.70, 0.80, 0.90)) %>%
    filter(!is.na(effort_condition), !is.na(difficulty_level)) %>%
    group_by(subject_id, task, effort_condition, difficulty_level, threshold) %>%
    summarise(n_trials_retained = sum(n_trials_retained, na.rm = TRUE), .groups = "drop") %>%
    mutate(
      threshold = as.factor(threshold),
      effort_condition = factor(effort_condition, levels = c("Low_5_MVC", "High_40_MVC")),
      difficulty_level = factor(difficulty_level, levels = c("Standard", "Easy", "Hard"))
    )

  # ADT and VDT separately
  for (task_name in c("ADT", "VDT")) {
    sub_df <- sweep_gateC %>%
      filter(task == task_name) %>%
      mutate(
        effort_label = case_when(
          effort_condition == "Low_5_MVC" ~ "Low",
          effort_condition == "High_40_MVC" ~ "High",
          TRUE ~ NA_character_
        ),
        condition = paste0(difficulty_level, " / ", effort_label)
      ) %>%
      select(subject_id, condition, threshold, n_trials_retained) %>%
      pivot_wider(names_from = threshold, values_from = n_trials_retained, values_fill = 0)

    if (nrow(sub_df) > 0) {
      cat("### ", task_name, " — Cognitive-AUC gate (gate_cog_auc) Trials at Thresholds 0.70 / 0.80 / 0.90\n\n")

      sub_df %>%
        arrange(subject_id, condition) %>%
        kable(
          caption = paste("Subject × condition breakdown for", task_name, "(Cognitive-AUC gate)"),
          col.names = c("Subject", "Condition", "0.70", "0.80", "0.90")
        ) %>%
        kable_styling(
          bootstrap_options = c("striped", "hover", "condensed"),
          full_width = TRUE,
          font_size = 10
        ) %>%
        print()

      cat("\n\n")
    }
  }
}
```

## Threshold Sensitivity Curves

```{r raw-status-threshold-sensitivity, fig.height=6}
if (!is.null(threshold_sweep)) {
  sweep_summary <- threshold_sweep %>%
    group_by(task, threshold, gate) %>%
    summarise(
      total_trials_retained = sum(n_trials_retained, na.rm = TRUE),
      .groups = "drop"
    ) %>%
    mutate(
      gate_label = dplyr::case_when(
        gate == "gate_A" ~ "Stimulus-locked gate: ITI + PreStim",
        gate == "gate_B" ~ "Total-AUC gate: Total AUC",
        gate == "gate_C" ~ "Cognitive-AUC gate: Cognitive AUC",
        TRUE ~ gate
      )
    )

  ggplot(sweep_summary, aes(x = threshold, y = total_trials_retained, color = gate_label)) +
    geom_line(linewidth = 1.1) +
    geom_point(size = 2) +
    facet_wrap(~ task, scales = "free_y") +
    scale_x_continuous(breaks = unique(sweep_summary$threshold)) +
    labs(
      title = "Threshold Sensitivity: Retained Trials vs Validity Threshold",
      x = "Validity Threshold",
      y = "Total Retained Trials",
      color = "Gate"
    ) +
    theme_minimal() +
    theme(legend.position = "bottom")
}
```

## Condition Balance (Effort × Difficulty, Cognitive AUC @ 0.80)

```{r raw-condition-balance, fig.height=7}
if (!is.null(trial_coverage_prefilter)) {
  thr_cog <- 0.80

  flags_cog <- gate_flags(trial_coverage_prefilter, thr_cog)
  trial_coverage_prefilter <- trial_coverage_prefilter %>%
    mutate(
      cog_ok_080 = flags_cog$gate_C
    )

  # Filter out rows with missing conditions - these should not exist in the data
  cond_counts <- trial_coverage_prefilter %>%
    filter(!is.na(effort_condition), !is.na(difficulty_level)) %>%
    group_by(subject_id, task, effort_condition, difficulty_level) %>%
    summarise(
      n_trials = n_distinct(trial_id),
      n_cog_retained_080 = n_distinct(trial_id[cog_ok_080]),
      .groups = "drop"
    )

  # Subject × condition heatmap for cognitive AUC @0.80
  cond_counts %>%
    mutate(
      condition = paste0(difficulty_level, " / ", ifelse(effort_condition == "Low_5_MVC", "Low", "High")),
      condition = factor(condition)
    ) %>%
    ggplot(aes(x = condition, y = subject_id, fill = n_cog_retained_080)) +
    geom_tile() +
    facet_wrap(~ task, scales = "free_y") +
    scale_fill_viridis_c(option = "C") +
    labs(
      title = "Cognitive AUC: Retained Trials per Subject × Condition (thr = 0.80)",
      x = "Condition (Difficulty / Effort)",
      y = "Subject",
      fill = "Retained\nTrials"
    ) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      strip.text = element_text(face = "bold")
    )
}
```

## THRESHOLD SWEEP (VALIDITY → RETAINED TRIALS)

```{r threshold-sweep-dashboard, fig.height=7}
if (!is.null(trial_coverage_prefilter)) {
  thr_grid <- threshold_grid

  mode_long <- purrr::map_dfr(thr_grid, function(thr) {
    gf <- gate_flags(trial_coverage_prefilter, thr)
    tibble(
      subject_id = trial_coverage_prefilter$subject_id,
      task = trial_coverage_prefilter$task,
      threshold = thr,
      baseline_prestim = gf$gate_A,
      total_auc = gf$gate_B,
      cognitive_auc = gf$gate_C
    )
  }) %>%
    tidyr::pivot_longer(cols = c(baseline_prestim, total_auc, cognitive_auc),
                        names_to = "mode", values_to = "retain")

  sweep_overall <- mode_long %>%
    group_by(task, threshold, mode) %>%
    summarise(n_trials_retained = sum(retain, na.rm = TRUE), .groups = "drop")

  ggplot(sweep_overall, aes(x = threshold, y = n_trials_retained, color = mode)) +
    geom_line(linewidth = 1.1) +
    geom_point(size = 2) +
    facet_wrap(~ task, scales = "free_y") +
    scale_x_continuous(breaks = thr_grid) +
    labs(
      title = "Threshold Sweep: Retained Trials vs Validity Threshold",
      x = "Validity Threshold",
      y = "Retained Trials",
      color = "Mode"
    ) +
    theme_minimal() +
    theme(legend.position = "bottom")
}
```

## Salvageability by Research Window (thr = 0.80)

```{r salvageability-dashboard, fig.height=7}
if (!is.null(trial_coverage_prefilter)) {
  thr <- 0.80
  gf <- gate_flags(trial_coverage_prefilter, thr)

  salvage_df <- trial_coverage_prefilter %>%
    mutate(
      baseline_ok = gf$gate_A,
      total_ok = gf$gate_B,
      cognitive_ok = gf$gate_C,
      category = dplyr::case_when(
        cognitive_ok ~ "Cognitive AUC usable",
        total_ok ~ "Total AUC usable only",
        baseline_ok ~ "Baseline+PreStim only",
        TRUE ~ "Unusable"
      ),
      salvage_total_not_cog = total_ok & !cognitive_ok,
      salvage_cog_not_total = cognitive_ok & !total_ok
    )

  # Stacked bars per subject (ordered by cognitive-usable trials)
  subj_order <- salvage_df %>%
    group_by(subject_id) %>%
    summarise(n_cog = sum(category == "Cognitive AUC usable"), .groups = "drop") %>%
    arrange(n_cog) %>%
    pull(subject_id)

  salvage_df %>%
    mutate(
      subject_id = factor(subject_id, levels = subj_order),
      category = factor(category,
                        levels = c("Unusable", "Baseline+PreStim only",
                                   "Total AUC usable only", "Cognitive AUC usable"))
    ) %>%
    group_by(subject_id, task, category) %>%
    summarise(n_trials = n_distinct(trial_id), .groups = "drop") %>%
    ggplot(aes(x = subject_id, y = n_trials, fill = category)) +
    geom_col() +
    facet_wrap(~ task, scales = "free_x") +
    labs(
      title = "Salvageability by Research Window (thr = 0.80)",
      x = "Subject",
      y = "Number of Trials",
      fill = "Category"
    ) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
      legend.position = "bottom"
    )
}
```

## Salvageability (Cognitive OK, Total Fails at 0.80)

```{r raw-status-salvageability}
if (!is.null(trial_coverage_prefilter)) {
  # Use gate_flags to define cognitive vs total usability at thr = 0.80
  thr <- 0.80
  gf <- gate_flags(trial_coverage_prefilter, thr)

  salvage_tbl <- trial_coverage_prefilter %>%
    mutate(
      baseline_ok_080 = gf$gate_A,
      total_ok_080 = gf$gate_B,
      cognitive_ok_080 = gf$gate_C,
      salvage_cognitive_only_080 = cognitive_ok_080 & !total_ok_080
    ) %>%
    group_by(subject_id, task) %>%
    summarise(
      n_trials = n(),
      n_cognitive_ok_only = sum(salvage_cognitive_only_080, na.rm = TRUE),
      n_total_ok = sum(total_ok_080, na.rm = TRUE),
      .groups = "drop"
    ) %>%
    arrange(subject_id, task)

  salvage_tbl %>%
    kable(
      caption = "Salvageability at 0.80: trials where Cognitive AUC window is usable but Total AUC window is not",
      col.names = c("Subject", "Task", "Total Trials", "Cognitive-Only Trials (OK Cognitive, Fail Total)", "Total AUC OK Trials")
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      full_width = FALSE
    )
}
```

## Automated Warnings (Minimum Per Cell & Asymmetry)

```{r raw-status-warnings, results='asis'}
if (!is.null(trial_coverage_prefilter) && !is.null(threshold_sweep)) {
  min_per_cell <- params$min_trials_per_cell %||% 10

  # Helper: total trials per subject × task × effort × difficulty
  total_by_cell <- trial_coverage_prefilter %>%
    group_by(subject_id, task, effort_condition, difficulty_level) %>%
    summarise(total_trials = n(), .groups = "drop")

  # Cognitive-AUC gate at thresholds 0.70 and 0.80
  gateC_70_80 <- threshold_sweep %>%
    filter(gate == "gate_C", threshold %in% c(0.70, 0.80)) %>%
    select(subject_id, task, effort_condition, difficulty_level, threshold, n_trials_retained) %>%
    pivot_wider(names_from = threshold, values_from = n_trials_retained, names_prefix = "thr_") %>%
    left_join(total_by_cell, by = c("subject_id", "task", "effort_condition", "difficulty_level"))

  # 1) Minimum per-cell warning at 0.80
  low_cells <- gateC_70_80 %>%
    filter(!is.na(thr_0.8), thr_0.8 < min_per_cell)

  if (nrow(low_cells) > 0) {
    cat("### Cells with <", min_per_cell, "Cognitive-AUC gate trials at threshold 0.80\n\n")

    low_cells %>%
      mutate(
        effort = as.character(effort_condition),
        difficulty = as.character(difficulty_level)
      ) %>%
      select(subject_id, task, effort, difficulty, thr_0.8, total_trials) %>%
      arrange(subject_id, task, effort, difficulty) %>%
      kable(
        col.names = c("Subject", "Task", "Effort", "Difficulty", "Cognitive-AUC gate Trials @0.80", "Total Trials"),
        caption = paste("Cells with fewer than", min_per_cell, "Cognitive-AUC gate trials at 0.80")
      ) %>%
      kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE) %>%
      print()

    cat("\n\n")
  }

  # 2) Instability between 0.70 and 0.80 (>30% drop)
  unstable_cells <- gateC_70_80 %>%
    filter(!is.na(thr_0.7), !is.na(thr_0.8), thr_0.7 > 0) %>%
    mutate(drop_frac = (thr_0.7 - thr_0.8) / thr_0.7) %>%
    filter(drop_frac > 0.30)

  if (nrow(unstable_cells) > 0) {
    cat("### Cells with >30% trial loss when moving Cognitive-AUC gate from 0.70 → 0.80\n\n")

    unstable_cells %>%
      mutate(
        effort = as.character(effort_condition),
        difficulty = as.character(difficulty_level),
        drop_pct = round(100 * drop_frac, 1)
      ) %>%
      select(subject_id, task, effort, difficulty, thr_0.7, thr_0.8, drop_pct) %>%
      arrange(desc(drop_pct)) %>%
      kable(
        col.names = c("Subject", "Task", "Effort", "Difficulty", "Trials @0.70", "Trials @0.80", "% Drop"),
        caption = "Cells with unstable retention between thresholds 0.70 and 0.80 (Cognitive-AUC gate)"
      ) %>%
      kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE) %>%
      print()

    cat("\n\n")
  }

  # 3) Effort asymmetry: High effort missingness > 2 × Low effort (Cognitive-AUC gate @0.80)
  gateC_080_effort <- gateC_70_80 %>%
    filter(!is.na(thr_0.8)) %>%
    group_by(subject_id, task, effort_condition) %>%
    summarise(
      retained_080 = sum(thr_0.8, na.rm = TRUE),
      total_trials = sum(total_trials, na.rm = TRUE),
      .groups = "drop"
    ) %>%
    mutate(
      missing_rate = ifelse(total_trials > 0, 1 - retained_080 / total_trials, NA_real_)
    )

  asymmetry <- gateC_080_effort %>%
    filter(!is.na(effort_condition)) %>%
    select(subject_id, task, effort_condition, missing_rate) %>%
    pivot_wider(names_from = effort_condition, values_from = missing_rate) %>%
    mutate(
      low_miss = `Low_5_MVC`,
      high_miss = `High_40_MVC`,
      asymmetry_flag = !is.na(low_miss) & !is.na(high_miss) & high_miss > 2 * low_miss
    ) %>%
    filter(asymmetry_flag)

  if (nrow(asymmetry) > 0) {
    cat("### Subjects with >2× higher missingness in High vs Low effort (Cognitive-AUC gate @0.80)\n\n")

    asymmetry %>%
      transmute(
        subject_id, task,
        low_missing_pct = round(100 * low_miss, 1),
        high_missing_pct = round(100 * high_miss, 1)
      ) %>%
      arrange(desc(high_missing_pct)) %>%
      kable(
        col.names = c("Subject", "Task", "% Missing (Low Effort)", "% Missing (High Effort)"),
        caption = "Effort asymmetry in missingness (High vs Low, Cognitive-AUC gate @0.80)"
      ) %>%
      kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE) %>%
      print()

    cat("\n\n")
  }
}
```

## Trial Completeness Classes

```{r trial-completeness-classes, results='asis'}
if (!is.null(trial_coverage_prefilter)) {
  thr <- 0.80
  
  # Compute binary flags for each epoch/window
  # Note: For windows not directly computed in build_trial_coverage (squeeze, post-target early),
  # we use approximations based on available windows and response timing
  completeness_flags <- trial_coverage_prefilter %>%
    mutate(
      # Baseline (-0.5 to 0)
      pass_baseline = !is.na(valid_prop_baseline_500ms) & valid_prop_baseline_500ms >= thr,
      
      # Prestim (3.25 to 3.75)
      pass_prestim = !is.na(valid_prop_prestim) & valid_prop_prestim >= thr,
      
      # Baseline + prestim combined (Stimulus-locked gate)
      pass_baseline_prestim = pass_baseline & pass_prestim,
      
      # Squeeze (0 to 3.0) - approximation: if total AUC passes and covers 0-3.0 range
      # We check if total_auc window passes AND response_onset >= 3.0 (ensuring coverage up to 3.0)
      # OR if we have valid ITI (which covers -3.0 to 0) and prestim (3.25-3.75), 
      # then the gap 0-3.0 is likely covered
      pass_squeeze = (!is.na(valid_prop_total_auc) & valid_prop_total_auc >= thr & 
                      !is.na(response_onset) & response_onset >= 3.0) |
                     (pass_baseline_prestim & !is.na(valid_prop_iti_full) & valid_prop_iti_full >= thr),
      
      # Post-target early (4.65 to 5.65) - fixed window
      # Check if cognitive AUC window passes AND response_onset >= 5.65
      # This ensures the cognitive window (4.65 to response_onset) fully covers the early window (4.65 to 5.65)
      pass_posttarget_early = !is.na(valid_prop_cognitive_auc) & 
        valid_prop_cognitive_auc >= thr & 
        !is.na(response_onset) & response_onset >= 5.65,
      
      # Response-locked cognitive (4.65 to response_onset) - Cognitive-AUC gate cognitive component
      pass_cognitive_resplocked = !is.na(valid_prop_cognitive_auc) & 
        valid_prop_cognitive_auc >= thr & 
        !is.na(response_onset),
      
      # Total AUC (0 to response_onset) - Total-AUC gate total component
      pass_total_auc = !is.na(valid_prop_total_auc) & 
        valid_prop_total_auc >= thr & 
        !is.na(response_onset),
      
      # Total-AUC gate (baseline+prestim + total AUC)
      pass_gate_b = pass_baseline_prestim & pass_total_auc,
      
      # Cognitive-AUC gate (baseline+prestim + cognitive AUC)
      pass_gate_c = pass_baseline_prestim & pass_cognitive_resplocked
    ) %>%
    mutate(
      # Define completeness classes
      completeness_class = case_when(
        # Class 1: Full (passes Total-AUC gate and Cognitive-AUC gate)
        pass_gate_b & pass_gate_c ~ "Class1_Full",
        
        # Class 2: Cognitive-early only (passes baseline+prestim and 4.65–5.65 but fails response-locked)
        pass_baseline_prestim & pass_posttarget_early & !pass_cognitive_resplocked ~ "Class2_CognitiveEarlyOnly",
        
        # Class 3: Baseline+prestim only
        pass_baseline_prestim & !pass_posttarget_early & !pass_total_auc ~ "Class3_BaselinePrestimOnly",
        
        # Class 4: Has pupil but fails baseline
        has_any_pupil & !pass_baseline ~ "Class4_FailsBaseline",
        
        # Additional classes for completeness
        has_any_pupil & pass_baseline & !pass_prestim ~ "Class5_BaselineOnly",
        has_any_pupil & !pass_baseline_prestim & pass_total_auc ~ "Class6_TotalAUCOnly",
        has_any_pupil & !pass_baseline_prestim & pass_cognitive_resplocked ~ "Class7_CognitiveAUCOnly",
        
        # No pupil data at all
        !has_any_pupil ~ "Class0_NoPupilData",
        
        TRUE ~ "Class8_Other"
      )
    )
  
  # Summary by subject-task
  completeness_summary <- completeness_flags %>%
    group_by(subject_id, task, completeness_class) %>%
    summarise(n_trials = n_distinct(trial_id), .groups = "drop") %>%
    pivot_wider(names_from = completeness_class, values_from = n_trials, values_fill = 0) %>%
    arrange(subject_id, task)
  
  # Overall totals
  completeness_totals <- completeness_flags %>%
    group_by(completeness_class) %>%
    summarise(n_trials = n_distinct(trial_id), .groups = "drop") %>%
    arrange(desc(n_trials))
  
  cat("## Trial Completeness Classes\n\n")
  cat("This section classifies trials based on which epochs/windows have valid pupil data at threshold 0.80.\n\n")
  cat("**Epochs checked:**\n")
  cat("- Baseline (-0.5 to 0s)\n")
  cat("- Squeeze (0 to 3.0s)\n")
  cat("- Prestim (3.25 to 3.75s)\n")
  cat("- Post-target early (4.65 to 5.65s)\n")
  cat("- Response-locked cognitive (4.65 to response_onset)\n")
  cat("- Total AUC (0 to response_onset)\n\n")
  
  cat("**Completeness Classes:**\n")
  cat("- **Class 1: Full** - Passes Total-AUC gate (baseline+prestim+Total AUC) and Cognitive-AUC gate (baseline+prestim+Cognitive AUC)\n")
  cat("- **Class 2: Cognitive-early only** - Passes baseline+prestim and 4.65–5.65s but fails response-locked cognitive window\n")
  cat("- **Class 3: Baseline+prestim only** - Passes baseline+prestim but fails both Total AUC and post-target early windows\n")
  cat("- **Class 4: Fails baseline** - Has pupil data but fails baseline window\n")
  cat("- **Class 5: Baseline only** - Passes baseline but fails prestim\n")
  cat("- **Class 6: Total AUC only** - Passes Total AUC but fails baseline+prestim\n")
  cat("- **Class 7: Cognitive AUC only** - Passes Cognitive AUC but fails baseline+prestim\n")
  cat("- **Class 0: No pupil data** - No valid pupil samples at all\n")
  cat("- **Class 8: Other** - Other combinations\n\n")
  
  cat("### Completeness Class Summary by Subject × Task\n\n")
  
  completeness_summary %>%
    kable(
      caption = "Trial completeness class counts by subject and task (threshold = 0.80)",
      digits = 0
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      full_width = TRUE,
      font_size = 9
    ) %>%
    scroll_box(width = "100%", height = "500px") %>%
    print()
  
  cat("\n### Overall Completeness Class Totals\n\n")
  
  completeness_totals %>%
    kable(
      col.names = c("Completeness Class", "Number of Trials"),
      caption = "Total trial counts by completeness class (threshold = 0.80)",
      digits = 0
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      full_width = FALSE
    ) %>%
    print()
  
  # Percentage breakdown
  total_trials_classified <- sum(completeness_totals$n_trials)
  completeness_pct <- completeness_totals %>%
    mutate(
      pct = round(100 * n_trials / total_trials_classified, 1)
    )
  
  cat("\n### Completeness Class Percentages\n\n")
  
  completeness_pct %>%
    kable(
      col.names = c("Completeness Class", "Number of Trials", "Percentage"),
      caption = "Percentage breakdown of trials by completeness class",
      digits = 1
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      full_width = FALSE
    ) %>%
    print()
}
```

## Fixed-Window Cognitive AUC Comparison (Salvage Analysis)

```{r cognitive-fixed-window-comparison, fig.height=10, results='asis'}
if (!is.null(trial_coverage_prefilter)) {
  # Define threshold grid for comparison
  threshold_grid_compare <- seq(0.50, 0.95, by = 0.05)
  
  # Helper function to compute gate flags for different cognitive window definitions
  gate_flags_cognitive_variants <- function(df, thr) {
    # Stimulus-locked gate (baseline + prestim) - same for all variants
    gate_A_base = !is.na(df$valid_prop_baseline_500ms) & !is.na(df$valid_prop_prestim) &
      df$valid_prop_baseline_500ms >= thr & df$valid_prop_prestim >= thr
    
    # Cognitive-AUC gate variants
    gate_C_original = gate_A_base & 
      !is.na(df$valid_prop_cognitive_auc) & 
      df$valid_prop_cognitive_auc >= thr &
      !is.na(df$response_onset)
    
    gate_C_fixed_1s = gate_A_base &
      !is.na(df$valid_prop_cognitive_fixed_1s) &
      df$valid_prop_cognitive_fixed_1s >= thr
    
    gate_C_fixed_1p5s = gate_A_base &
      !is.na(df$valid_prop_cognitive_fixed_1p5s) &
      df$valid_prop_cognitive_fixed_1p5s >= thr
    
    tibble(
      gate_C_original = gate_C_original,
      gate_C_fixed_1s = gate_C_fixed_1s,
      gate_C_fixed_1p5s = gate_C_fixed_1p5s
    )
  }
  
  # Compute retention across thresholds for each variant
  retention_comparison <- purrr::map_dfr(threshold_grid_compare, function(thr) {
    gf <- gate_flags_cognitive_variants(trial_coverage_prefilter, thr)
    
    tibble(
      threshold = thr,
      variant = c("Original (4.65 to response_onset)", 
                  "Fixed 1.0s (4.65-5.65s)", 
                  "Fixed 1.5s (4.65-6.15s)"),
      n_trials_retained = c(
        sum(gf$gate_C_original, na.rm = TRUE),
        sum(gf$gate_C_fixed_1s, na.rm = TRUE),
        sum(gf$gate_C_fixed_1p5s, na.rm = TRUE)
      )
    )
  })
  
  # Per-subject comparison at threshold 0.80
  thr_080 <- 0.80
  gf_080 <- gate_flags_cognitive_variants(trial_coverage_prefilter, thr_080)
  
  subject_comparison_080 <- trial_coverage_prefilter %>%
    mutate(
      gate_C_original_080 = gf_080$gate_C_original,
      gate_C_fixed_1s_080 = gf_080$gate_C_fixed_1s,
      gate_C_fixed_1p5s_080 = gf_080$gate_C_fixed_1p5s
    ) %>%
    group_by(subject_id, task) %>%
    summarise(
      n_original = n_distinct(trial_id[gate_C_original_080]),
      n_fixed_1s = n_distinct(trial_id[gate_C_fixed_1s_080]),
      n_fixed_1p5s = n_distinct(trial_id[gate_C_fixed_1p5s_080]),
      n_total = n_distinct(trial_id),
      .groups = "drop"
    ) %>%
    mutate(
      delta_1s = n_fixed_1s - n_original,
      delta_1p5s = n_fixed_1p5s - n_original,
      pct_gain_1s = round(100 * delta_1s / n_total, 1),
      pct_gain_1p5s = round(100 * delta_1p5s / n_total, 1)
    )
  
  # Overall retention curves
  p1 <- retention_comparison %>%
    ggplot(aes(x = threshold, y = n_trials_retained, color = variant, linetype = variant)) +
    geom_line(size = 1.2) +
    geom_point(size = 2) +
    scale_color_brewer(palette = "Set1", name = "Cognitive Window") +
    scale_linetype_manual(values = c("solid", "dashed", "dotted"), name = "Cognitive Window") +
    labs(
      title = "Overall Trial Retention: Original vs Fixed Cognitive Windows",
      subtitle = "Comparison across thresholds 0.50-0.95",
      x = "Validity Threshold",
      y = "Number of Trials Retained",
      caption = "Original: 4.65s to response_onset (variable length)\nFixed 1.0s: 4.65-5.65s (1.0s window)\nFixed 1.5s: 4.65-6.15s (1.5s window)"
    ) +
    theme_minimal() +
    theme(legend.position = "bottom")
  
  print(p1)
  
  # Per-subject deltas at 0.80
  p2 <- subject_comparison_080 %>%
    filter(n_total > 0) %>%
    ggplot(aes(x = reorder(paste(subject_id, task, sep = "_"), delta_1s), y = delta_1s)) +
    geom_col(aes(fill = delta_1s > 0), alpha = 0.7) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
    scale_fill_manual(values = c("red", "green"), labels = c("Loss", "Gain"), name = "") +
    labs(
      title = "Per-Subject Delta: Fixed 1.0s vs Original (threshold = 0.80)",
      subtitle = "Positive = more trials retained with fixed window",
      x = "Subject × Task",
      y = "Delta (Fixed 1.0s - Original)"
    ) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 7),
      legend.position = "bottom"
    )
  
  print(p2)
  
  p3 <- subject_comparison_080 %>%
    filter(n_total > 0) %>%
    ggplot(aes(x = reorder(paste(subject_id, task, sep = "_"), delta_1p5s), y = delta_1p5s)) +
    geom_col(aes(fill = delta_1p5s > 0), alpha = 0.7) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
    scale_fill_manual(values = c("red", "green"), labels = c("Loss", "Gain"), name = "") +
    labs(
      title = "Per-Subject Delta: Fixed 1.5s vs Original (threshold = 0.80)",
      subtitle = "Positive = more trials retained with fixed window",
      x = "Subject × Task",
      y = "Delta (Fixed 1.5s - Original)"
    ) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 7),
      legend.position = "bottom"
    )
  
  print(p3)
  
  # Summary table of deltas
  cat("\n### Summary: Retention Differences at Threshold 0.80\n\n")
  
  summary_deltas <- subject_comparison_080 %>%
    summarise(
      mean_delta_1s = mean(delta_1s, na.rm = TRUE),
      median_delta_1s = median(delta_1s, na.rm = TRUE),
      mean_delta_1p5s = mean(delta_1p5s, na.rm = TRUE),
      median_delta_1p5s = median(delta_1p5s, na.rm = TRUE),
      n_subjects_gain_1s = sum(delta_1s > 0, na.rm = TRUE),
      n_subjects_loss_1s = sum(delta_1s < 0, na.rm = TRUE),
      n_subjects_gain_1p5s = sum(delta_1p5s > 0, na.rm = TRUE),
      n_subjects_loss_1p5s = sum(delta_1p5s < 0, na.rm = TRUE),
      .groups = "drop"
    )
  
  summary_deltas %>%
    kable(
      col.names = c("Mean Δ (1.0s)", "Median Δ (1.0s)", "Mean Δ (1.5s)", "Median Δ (1.5s)",
                   "Subjects Gain (1.0s)", "Subjects Loss (1.0s)", 
                   "Subjects Gain (1.5s)", "Subjects Loss (1.5s)"),
      caption = "Summary of retention differences: Fixed windows vs Original (threshold = 0.80)",
      digits = 1
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      full_width = FALSE
    ) %>%
    print()
  
  # Overall deltas across thresholds
  overall_deltas <- retention_comparison %>%
    pivot_wider(names_from = variant, values_from = n_trials_retained) %>%
    mutate(
      delta_1s = `Fixed 1.0s (4.65-5.65s)` - `Original (4.65 to response_onset)`,
      delta_1p5s = `Fixed 1.5s (4.65-6.15s)` - `Original (4.65 to response_onset)`,
      pct_delta_1s = round(100 * delta_1s / `Original (4.65 to response_onset)`, 1),
      pct_delta_1p5s = round(100 * delta_1p5s / `Original (4.65 to response_onset)`, 1)
    )
  
  p4 <- overall_deltas %>%
    select(threshold, delta_1s, delta_1p5s) %>%
    pivot_longer(cols = starts_with("delta_"), names_to = "variant", values_to = "delta") %>%
    mutate(
      variant = case_when(
        variant == "delta_1s" ~ "Fixed 1.0s - Original",
        variant == "delta_1p5s" ~ "Fixed 1.5s - Original",
        TRUE ~ variant
      )
    ) %>%
    ggplot(aes(x = threshold, y = delta, color = variant)) +
    geom_line(size = 1.2) +
    geom_point(size = 2) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
    scale_color_brewer(palette = "Set2", name = "Comparison") +
    labs(
      title = "Overall Delta: Fixed Windows - Original Across Thresholds",
      subtitle = "Positive = more trials retained with fixed window",
      x = "Validity Threshold",
      y = "Delta (Fixed - Original)",
      caption = "Shows how much trial retention changes when using fixed vs response-locked windows"
    ) +
    theme_minimal() +
    theme(legend.position = "bottom")
  
  print(p4)
  
  cat("\n### Overall Deltas Across Thresholds\n\n")
  
  overall_deltas %>%
    select(threshold, delta_1s, delta_1p5s, pct_delta_1s, pct_delta_1p5s) %>%
    kable(
      col.names = c("Threshold", "Δ (1.0s)", "Δ (1.5s)", "% Δ (1.0s)", "% Δ (1.5s)"),
      caption = "Overall retention differences across thresholds",
      digits = 1
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      full_width = FALSE
    ) %>%
    print()
  
  cat("\n**Note**: This is a robustness/salvage tool for exploring alternative cognitive window definitions.\n")
  cat("Fixed windows may retain more trials for subjects with late responses or missing RT data.\n\n")
}
```

## Filter Funnel / Loss Reasons Dashboard

```{r filter-funnel-loss-reasons, results='asis'}
if (!is.null(trial_coverage_prefilter)) {
  # Helper function to label first failing reason at threshold t
  label_failure_reason <- function(df, thr) {
    df %>%
      mutate(
        fail_baseline500 = is.na(valid_prop_baseline_500ms) | valid_prop_baseline_500ms < thr,
        fail_prestim = is.na(valid_prop_prestim) | valid_prop_prestim < thr,
        fail_missing_rt = is.na(response_onset) | is.na(rt) | rt <= 0 | rt >= 5.0,
        fail_total_auc = is.na(valid_prop_total_auc) | valid_prop_total_auc < thr,
        fail_cognitive_auc = is.na(valid_prop_cognitive_auc) | valid_prop_cognitive_auc < thr,
        # Priority order: baseline500 -> prestim -> missing_rt -> cognitive_auc -> total_auc
        first_fail_reason = case_when(
          fail_baseline500 ~ "fail_baseline500",
          fail_prestim ~ "fail_prestim",
          fail_missing_rt ~ "fail_missing_rt",
          fail_cognitive_auc ~ "fail_cognitive_auc",
          fail_total_auc ~ "fail_total_auc",
          TRUE ~ "pass_all"
        )
      )
  }

  # Compute funnel and loss reasons for each threshold
  threshold_grid <- params$threshold_grid %||% c(0.50, 0.60, 0.70, 0.80, 0.90, 0.95)
  
  funnel_data <- purrr::map_dfr(threshold_grid, function(thr) {
    labeled <- label_failure_reason(trial_coverage_prefilter, thr)
    gf <- gate_flags(trial_coverage_prefilter, thr)
    
    labeled %>%
      mutate(
        threshold = thr,
        gate_A_pass = gf$gate_A,
        gate_C_pass = gf$gate_C
      ) %>%
      group_by(threshold, task) %>%
      summarise(
        total_trials = n(),
        gate_A_passed = sum(gate_A_pass, na.rm = TRUE),
        gate_C_passed = sum(gate_C_pass, na.rm = TRUE),
        n_fail_baseline500 = sum(first_fail_reason == "fail_baseline500", na.rm = TRUE),
        n_fail_prestim = sum(first_fail_reason == "fail_prestim", na.rm = TRUE),
        n_fail_missing_rt = sum(first_fail_reason == "fail_missing_rt", na.rm = TRUE),
        n_fail_cognitive_auc = sum(first_fail_reason == "fail_cognitive_auc", na.rm = TRUE),
        n_fail_total_auc = sum(first_fail_reason == "fail_total_auc", na.rm = TRUE),
        n_pass_all = sum(first_fail_reason == "pass_all", na.rm = TRUE),
        .groups = "drop"
      )
  })

  # Overall funnel (across all tasks)
  funnel_overall <- funnel_data %>%
    group_by(threshold) %>%
    summarise(
      total_trials = sum(total_trials),
      gate_A_passed = sum(gate_A_passed),
      gate_C_passed = sum(gate_C_passed),
      n_fail_baseline500 = sum(n_fail_baseline500),
      n_fail_prestim = sum(n_fail_prestim),
      n_fail_missing_rt = sum(n_fail_missing_rt),
      n_fail_cognitive_auc = sum(n_fail_cognitive_auc),
      n_fail_total_auc = sum(n_fail_total_auc),
      n_pass_all = sum(n_pass_all),
      .groups = "drop"
    )

  # Funnel plot: stage vs remaining trials, facet by task
  funnel_plot_data <- funnel_data %>%
    select(threshold, task, total_trials, gate_A_passed, gate_C_passed) %>%
    tidyr::pivot_longer(
      cols = c(total_trials, gate_A_passed, gate_C_passed),
      names_to = "stage",
      values_to = "n_trials"
    ) %>%
    mutate(
      stage = factor(stage, levels = c("total_trials", "gate_A_passed", "gate_C_passed"),
                    labels = c("Total Trials", "Stimulus-locked gate (Baseline+Prestim)", "Cognitive-AUC gate (Cognitive AUC)"))
    )

  p_funnel <- ggplot(funnel_plot_data, aes(x = stage, y = n_trials, fill = stage)) +
    geom_col(position = "dodge") +
    facet_wrap(~ task, scales = "free_y") +
    scale_fill_brewer(palette = "Set2") +
    labs(
      title = "Filter Funnel: Trials Remaining at Each Gate",
      x = "Filter Stage",
      y = "Number of Trials",
      fill = "Stage"
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

  print(p_funnel)

  # Stacked bar: first failing reason proportions, facet by task
  fail_reason_plot_data <- funnel_data %>%
    select(threshold, task, n_fail_baseline500, n_fail_prestim, n_fail_missing_rt,
           n_fail_cognitive_auc, n_fail_total_auc, n_pass_all) %>%
    tidyr::pivot_longer(
      cols = starts_with("n_"),
      names_to = "reason",
      values_to = "count"
    ) %>%
    mutate(
      reason = factor(
        reason,
        levels = c("n_fail_baseline500", "n_fail_prestim", "n_fail_missing_rt",
                   "n_fail_cognitive_auc", "n_fail_total_auc", "n_pass_all"),
        labels = c("Fail Baseline", "Fail Prestim", "Missing RT",
                   "Fail Cognitive AUC", "Fail Total AUC", "Pass All")
      )
    ) %>%
    group_by(threshold, task) %>%
    mutate(
      total = sum(count),
      prop = ifelse(total > 0, count / total, 0)
    )

  p_fail_reasons <- ggplot(fail_reason_plot_data, aes(x = threshold, y = prop, fill = reason)) +
    geom_col(position = "stack") +
    facet_wrap(~ task) +
    scale_fill_brewer(palette = "Spectral", direction = -1) +
    labs(
      title = "First Failing Reason by Threshold",
      x = "Threshold",
      y = "Proportion of Trials",
      fill = "Failure Reason"
    ) +
    theme_minimal()

  print(p_fail_reasons)

  # Summary table for threshold 0.80
  cat("\n### Loss Reasons Summary at Threshold 0.80\n\n")
  
  funnel_080 <- funnel_data %>%
    filter(threshold == 0.80) %>%
    mutate(
      pct_fail_baseline500 = round(100 * n_fail_baseline500 / total_trials, 1),
      pct_fail_prestim = round(100 * n_fail_prestim / total_trials, 1),
      pct_fail_missing_rt = round(100 * n_fail_missing_rt / total_trials, 1),
      pct_fail_cognitive_auc = round(100 * n_fail_cognitive_auc / total_trials, 1),
      pct_fail_total_auc = round(100 * n_fail_total_auc / total_trials, 1),
      pct_pass_all = round(100 * n_pass_all / total_trials, 1)
    ) %>%
    select(task, total_trials, gate_C_passed, pct_fail_baseline500, pct_fail_prestim,
           pct_fail_missing_rt, pct_fail_cognitive_auc, pct_fail_total_auc, pct_pass_all)

  funnel_080 %>%
    kable(
      col.names = c("Task", "Total Trials", "Cognitive-AUC gate Passed", "% Fail Baseline",
                   "% Fail Prestim", "% Missing RT", "% Fail Cognitive AUC",
                   "% Fail Total AUC", "% Pass All"),
      caption = "Loss reasons breakdown at threshold 0.80",
      digits = 1
    ) %>%
    kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE) %>%
    print()
  
  # Gate failure reasons by condition (Prompt 5)
  cat("\n## Gate Failure Reasons by Condition\n\n")
  cat("This section shows whether missingness (gate failures) is condition-dependent, which is critical for Chapter 2 validity.\n\n")
  
  failure_by_condition <- purrr::map_dfr(threshold_grid, function(thr) {
    labeled <- label_failure_reason(trial_coverage_prefilter, thr)
    
    # Filter out rows with missing conditions - these should not exist in the data
    labeled %>%
      filter(!is.na(effort_condition), !is.na(difficulty_level)) %>%
      mutate(
        effort_label = case_when(
          effort_condition == "Low_5_MVC" ~ "Low",
          effort_condition == "High_40_MVC" ~ "High",
          TRUE ~ NA_character_
        ),
        difficulty_label = as.character(difficulty_level)
      ) %>%
      group_by(threshold = thr, task, effort_label, difficulty_label, first_fail_reason) %>%
      summarise(n_trials = n_distinct(trial_id), .groups = "drop") %>%
      group_by(threshold, task, effort_label, difficulty_label) %>%
      mutate(
        total_in_cell = sum(n_trials),
        pct_fail_reason = round(100 * n_trials / total_in_cell, 1)
      ) %>%
      ungroup()
  })
  
  # Plot failure reasons by effort
  p_fail_effort <- failure_by_condition %>%
    filter(!is.na(effort_label), 
           first_fail_reason != "pass_all") %>%
    group_by(threshold, task, effort_label, first_fail_reason) %>%
    summarise(
      n_trials = sum(n_trials),
      total_in_effort = sum(total_in_cell),
      pct = round(100 * n_trials / total_in_effort, 1),
      .groups = "drop"
    ) %>%
    ggplot(aes(x = threshold, y = pct, color = first_fail_reason, linetype = first_fail_reason)) +
    geom_line(size = 1.1) +
    geom_point(size = 2) +
    facet_grid(task ~ effort_label) +
    scale_color_brewer(palette = "Set2", name = "Failure Reason") +
    scale_linetype_manual(values = c("solid", "dashed", "dotted", "dotdash", "longdash"), name = "Failure Reason") +
    labs(
      title = "Gate Failure Reasons by Effort Condition",
      subtitle = "Proportion of trials failing at each stage, stratified by Low vs High effort",
      x = "Validity Threshold",
      y = "Percentage of Trials",
      caption = "Shows if missingness is effort-dependent (critical for Chapter 2 validity)"
    ) +
    theme_minimal() +
    theme(legend.position = "bottom")
  
  print(p_fail_effort)
  
  # Plot failure reasons by difficulty
  p_fail_difficulty <- failure_by_condition %>%
    filter(!is.na(difficulty_label), difficulty_label != "Unknown",
           first_fail_reason != "pass_all") %>%
    group_by(threshold, task, difficulty_label, first_fail_reason) %>%
    summarise(
      n_trials = sum(n_trials),
      total_in_difficulty = sum(total_in_cell),
      pct = round(100 * n_trials / total_in_difficulty, 1),
      .groups = "drop"
    ) %>%
    ggplot(aes(x = threshold, y = pct, color = first_fail_reason, linetype = first_fail_reason)) +
    geom_line(size = 1.1) +
    geom_point(size = 2) +
    facet_grid(task ~ difficulty_label) +
    scale_color_brewer(palette = "Set2", name = "Failure Reason") +
    scale_linetype_manual(values = c("solid", "dashed", "dotted", "dotdash", "longdash"), name = "Failure Reason") +
    labs(
      title = "Gate Failure Reasons by Difficulty Level",
      subtitle = "Proportion of trials failing at each stage, stratified by difficulty",
      x = "Validity Threshold",
      y = "Percentage of Trials",
      caption = "Shows if missingness is difficulty-dependent"
    ) +
    theme_minimal() +
    theme(legend.position = "bottom")
  
  print(p_fail_difficulty)
  
  # Summary table at threshold 0.80
  cat("\n### Failure Reasons by Condition at Threshold 0.80\n\n")
  
  failure_summary_080 <- failure_by_condition %>%
    filter(threshold == 0.80, first_fail_reason != "pass_all") %>%
    filter(!is.na(effort_label), !is.na(difficulty_label)) %>%
    group_by(task, effort_label, difficulty_label, first_fail_reason) %>%
    summarise(n_trials = sum(n_trials), .groups = "drop") %>%
    group_by(task, effort_label, difficulty_label) %>%
    mutate(
      total_in_cell = sum(n_trials),
      pct = round(100 * n_trials / total_in_cell, 1)
    ) %>%
    ungroup() %>%
    arrange(task, effort_label, difficulty_label, desc(n_trials))
  
  failure_summary_080 %>%
    select(task, effort_label, difficulty_label, first_fail_reason, n_trials, pct) %>%
    kable(
      col.names = c("Task", "Effort", "Difficulty", "Failure Reason", "N Trials", "%"),
      caption = "Gate failure reasons by condition at threshold 0.80",
      digits = 1
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      full_width = FALSE
    ) %>%
    print()
  
  cat("\n**Interpretation**: If failure reasons differ substantially between Low vs High effort or across difficulty levels,\n")
  cat("this suggests Missing Not At Random (MNAR) bias, which could threaten Chapter 2 validity.\n\n")
}
```

## Threshold Knee-Point Diagnostic

```{r threshold-knee-point-diagnostic, fig.height=8, results='asis'}
if (!is.null(threshold_sweep)) {
  cat("This diagnostic identifies the 'knee' point where marginal retention loss per +0.05 threshold increase exceeds a set rate.\n")
  cat("This helps justify threshold choice (e.g., 0.70 vs 0.80) based on data rather than guessing.\n\n")
  
  # Define threshold grid
  threshold_grid_knee <- seq(0.50, 0.95, by = 0.05)
  
  # Compute retention for each gate × task × threshold
  retention_by_gate <- threshold_sweep %>%
    group_by(gate, task, threshold) %>%
    summarise(
      n_trials_retained = sum(n_trials_retained, na.rm = TRUE),
      .groups = "drop"
    ) %>%
    arrange(gate, task, threshold)
  
  # Compute marginal loss rate (loss per +0.05 threshold increase)
  knee_analysis <- retention_by_gate %>%
    group_by(gate, task) %>%
    arrange(threshold) %>%
    mutate(
      prev_threshold = lag(threshold),
      prev_retained = lag(n_trials_retained),
      marginal_loss = prev_retained - n_trials_retained,
      marginal_loss_rate = ifelse(!is.na(prev_retained) & prev_retained > 0,
                                 round(100 * marginal_loss / prev_retained, 1),
                                 NA_real_)
    ) %>%
    ungroup()
  
  # Find knee point: first threshold where marginal loss rate exceeds threshold (e.g., 10%)
  loss_rate_threshold <- 10  # 10% loss per +0.05 threshold increase
  
  knee_points <- knee_analysis %>%
    filter(!is.na(marginal_loss_rate), marginal_loss_rate >= loss_rate_threshold) %>%
    group_by(gate, task) %>%
    slice_min(threshold, n = 1) %>%
    ungroup() %>%
    select(gate, task, knee_threshold = threshold, knee_loss_rate = marginal_loss_rate)
  
  # Plot retention curves with knee points marked
  p_knee <- retention_by_gate %>%
    left_join(knee_points, by = c("gate", "task")) %>%
    mutate(
      gate_label = case_when(
        gate == "gate_A" ~ "Stimulus-locked gate: ITI + PreStim",
        gate == "gate_B" ~ "Total-AUC gate: Total AUC",
        gate == "gate_C" ~ "Cognitive-AUC gate: Cognitive AUC",
        TRUE ~ gate
      )
    ) %>%
    ggplot(aes(x = threshold, y = n_trials_retained)) +
    geom_line(size = 1.2, color = "steelblue") +
    geom_point(size = 2, color = "steelblue") +
    geom_vline(aes(xintercept = knee_threshold), linetype = "dashed", color = "red", alpha = 0.7) +
    facet_grid(gate_label ~ task, scales = "free_y") +
    scale_x_continuous(breaks = threshold_grid_knee) +
    labs(
      title = "Threshold Knee-Point Diagnostic",
      subtitle = paste("Red dashed line = knee point (first threshold with >=", loss_rate_threshold, "% loss per +0.05)"),
      x = "Validity Threshold",
      y = "Retained Trials",
      caption = "Knee point indicates where retention starts dropping rapidly"
    ) +
    theme_minimal() +
    theme(legend.position = "none")
  
  print(p_knee)
  
  # Marginal loss rate plot
  p_marginal <- knee_analysis %>%
    filter(!is.na(marginal_loss_rate)) %>%
    mutate(
      gate_label = case_when(
        gate == "gate_A" ~ "Stimulus-locked gate: ITI + PreStim",
        gate == "gate_B" ~ "Total-AUC gate: Total AUC",
        gate == "gate_C" ~ "Cognitive-AUC gate: Cognitive AUC",
        TRUE ~ gate
      )
    ) %>%
    ggplot(aes(x = threshold, y = marginal_loss_rate)) +
    geom_line(size = 1.2, color = "darkorange") +
    geom_point(size = 2, color = "darkorange") +
    geom_hline(yintercept = loss_rate_threshold, linetype = "dashed", color = "red", alpha = 0.7) +
    facet_grid(gate_label ~ task) +
    scale_x_continuous(breaks = threshold_grid_knee) +
    labs(
      title = "Marginal Loss Rate per +0.05 Threshold Increase",
      subtitle = paste("Red line =", loss_rate_threshold, "% threshold. Knee = first crossing."),
      x = "Validity Threshold",
      y = "Marginal Loss Rate (%)",
      caption = "Shows how quickly trials are lost as threshold increases"
    ) +
    theme_minimal()
  
  print(p_marginal)
  
  # Knee point summary table
  cat("\n### Recommended Thresholds (Knee Points) by Gate and Task\n\n")
  
  # Fill in missing combinations (no knee found = very stable)
  all_combos <- expand_grid(
    gate = c("gate_A", "gate_B", "gate_C"),
    task = c("ADT", "VDT")
  ) %>%
    mutate(
      gate_label = case_when(
        gate == "gate_A" ~ "Stimulus-locked gate: ITI + PreStim",
        gate == "gate_B" ~ "Total-AUC gate: Total AUC",
        gate == "gate_C" ~ "Cognitive-AUC gate: Cognitive AUC",
        TRUE ~ gate
      )
    )
  
  knee_summary_complete <- all_combos %>%
    left_join(knee_points, by = c("gate", "task")) %>%
    mutate(
      knee_threshold = ifelse(is.na(knee_threshold), "No knee (<10% loss)", as.character(knee_threshold)),
      knee_loss_rate = ifelse(is.na(knee_loss_rate), NA_real_, knee_loss_rate),
      recommendation = ifelse(is.na(knee_threshold) | knee_threshold == "No knee (<10% loss)",
                             "Stable across thresholds",
                             paste0("Consider threshold < ", knee_threshold))
    ) %>%
    select(gate_label, task, knee_threshold, knee_loss_rate, recommendation)
  
  knee_summary_complete %>%
    kable(
      col.names = c("Gate", "Task", "Knee Threshold", "Knee Loss Rate (%)", "Recommendation"),
      caption = paste("Knee point analysis: first threshold where marginal loss rate >=", loss_rate_threshold, "% per +0.05"),
      digits = 1
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      full_width = FALSE
    ) %>%
    print()
  
  cat("\n**Interpretation**: The knee point indicates where retention starts dropping rapidly.\n")
  cat("Thresholds below the knee are relatively stable; thresholds at or above the knee show accelerating loss.\n")
  cat("This helps justify threshold choice (e.g., 0.70 vs 0.80) based on data-driven criteria.\n\n")
}
```

## Time-Resolved Availability Curve

```{r time-resolved-availability}
if (!is.null(trial_coverage_prefilter)) {
  # Build time-binned availability from raw sample data
  availability_cache_file <- file.path(coverage_cache_dir, "pupil_time_availability.rds")
  
  build_time_availability <- function(files) {
    bin_width <- 0.05  # 50ms bins
    
    purrr::map_dfr(files, function(f) {
      df <- readr::read_csv(f, show_col_types = FALSE, progress = FALSE)
      
      if (!all(c("time", "pupil") %in% names(df))) {
        return(tibble())
      }
      
      df <- df %>%
        mutate(
          sub = if ("sub" %in% names(.)) sub else if ("subject_id" %in% names(.)) as.character(subject_id) else NA_character_,
          task = if ("task" %in% names(.)) {
            dplyr::if_else(task == "aud", "ADT",
                          dplyr::if_else(task == "vis", "VDT", as.character(task)))
          } else if ("task_modality" %in% names(.)) {
            dplyr::case_when(
              task_modality == "aud" ~ "ADT",
              task_modality == "vis" ~ "VDT",
              TRUE ~ as.character(task_modality)
            )
          } else NA_character_,
          run = if ("run" %in% names(.)) run else if ("run_num" %in% names(.)) run_num else NA_integer_,
          trial_index = dplyr::coalesce(
            if ("trial_index" %in% names(.)) trial_index else NA_integer_,
            if ("trial_in_run" %in% names(.)) trial_in_run else NA_integer_,
            if ("trial" %in% names(.)) trial else NA_integer_,
            if ("trial_num" %in% names(.)) trial_num else NA_integer_
          ),
          pupil = if ("pupil" %in% names(.)) pupil else NA_real_,
          gf_trPer = dplyr::coalesce(
            if ("gf_trPer" %in% names(.)) gf_trPer else NA_real_,
            if ("grip_targ_prop_mvc" %in% names(.)) grip_targ_prop_mvc else NA_real_
          ),
          force_condition = if ("force_condition" %in% names(.)) force_condition else NA_character_,
          stimLev = if ("stimLev" %in% names(.)) stimLev else if ("stim_level_index" %in% names(.)) stim_level_index else NA_real_,
          isOddball = if ("isOddball" %in% names(.)) isOddball else if ("stim_is_diff" %in% names(.)) as.integer(stim_is_diff) else NA_integer_,
          resp1RT = if ("resp1RT" %in% names(.)) resp1RT else if ("rt" %in% names(.)) rt else NA_real_
        ) %>%
        filter(!is.na(sub), !is.na(task), !is.na(run), !is.na(trial_index))
      
      if (nrow(df) == 0) {
        return(tibble())
      }
      
      df$pupil[df$pupil == 0] <- NA_real_
      
      # Bin time and compute availability per trial
      df %>%
        mutate(
          time_bin = round(time / bin_width) * bin_width,
          has_valid_pupil = !is.na(pupil)
        ) %>%
        group_by(sub, task, run, trial_index, time_bin) %>%
        summarise(
          has_valid = any(has_valid_pupil),
          .groups = "drop"
        ) %>%
        # Add condition labels (will join with trial_coverage_prefilter later)
        mutate(
          subject_id = as.character(sub)
        )
    })
  }
  
  if (file.exists(availability_cache_file) && !isTRUE(params$recompute_raw_coverage)) {
    time_availability <- readRDS(availability_cache_file)
  } else {
    merged_files <- flat_files_merged[grepl("_flat_merged\\.csv$", flat_files_merged)]
    files_to_use <- if (length(merged_files) > 0) merged_files else flat_files
    
    time_availability <- build_time_availability(files_to_use)
    saveRDS(time_availability, availability_cache_file)
  }
  
  if (!is.null(time_availability) && nrow(time_availability) > 0) {
    # Ensure time_availability is deduplicated (should be one row per trial+time_bin)
    time_availability <- time_availability %>%
      distinct(subject_id, task, run, trial_index, time_bin, .keep_all = TRUE)
    
    # Create unique trial-level lookup for condition labels
    trial_lookup <- trial_coverage_prefilter %>%
      select(subject_id, task, run, trial_index, effort_condition, difficulty_level) %>%
      distinct(subject_id, task, run, trial_index, .keep_all = TRUE)
    
    # Join condition labels (this should be 1:1 since trial_lookup is unique)
    time_availability <- time_availability %>%
      left_join(trial_lookup, by = c("subject_id", "task", "run", "trial_index"))
    
    # Aggregate availability across trials
    availability_summary <- time_availability %>%
      filter(time_bin >= -3.0 & time_bin <= 10.7, !is.na(effort_condition)) %>%
      group_by(task, time_bin, effort_condition) %>%
      summarise(
        n_trials = n_distinct(paste(subject_id, run, trial_index)),
        n_trials_with_valid = sum(has_valid),
        availability = mean(has_valid),
        .groups = "drop"
      )
    
    # Plot: availability vs time_bin, facet by task, color by effort
    # Time origin (t = 0) is the *grip gauge onset* (TrialST) from the task log.
    # Key events (seconds relative to TrialST):
    #   - 0.00: Grip gauge onset (TrialST)
    #   - 3.25: Fixation onset (fixST)
    #   - 3.75: Stimulus pair onset (A/V_ST)
    #   - 4.70: Response window start (Resp1ST; canonical)
    #   - 7.70: Response window end (Resp1ET; canonical)
    p_avail_effort <- ggplot(availability_summary, aes(x = time_bin, y = availability, color = effort_condition)) +
      geom_line(size = 1) +
      geom_vline(xintercept = 0.0,   linetype = "dashed", color = "gray50") +
      geom_vline(xintercept = 3.25,  linetype = "dashed", color = "gray50") +
      geom_vline(xintercept = 3.75,  linetype = "dashed", color = "gray50") +
      geom_vline(xintercept = 4.70,  linetype = "dashed", color = "blue",  alpha = 0.5) +
      geom_vline(xintercept = 7.70,  linetype = "dashed", color = "red",   alpha = 0.5) +
      facet_wrap(~ task) +
      scale_color_brewer(palette = "Set1", name = "Effort") +
      labs(
        title = "Time-Resolved Pupil Availability (Stimulus-Locked)",
        subtitle = paste(
          "Vertical lines:",
          "0=grip gauge onset (TrialST),",
          "3.25=fixation onset (fixST),",
          "3.75=stimulus pair onset (A/V_ST),",
          "4.70=response window start (Resp1ST),",
          "7.70=response window end (Resp1ET; canonical)"
        ),
        x = "Time (s)",
        y = "Proportion of Trials with Valid Pupil"
      ) +
      theme_minimal() +
      theme(legend.position = "bottom")
    
    print(p_avail_effort)
    
    # Same plot but colored by difficulty
    availability_summary_diff <- time_availability %>%
      filter(time_bin >= -3.0 & time_bin <= 10.7, !is.na(difficulty_level)) %>%
      group_by(task, time_bin, difficulty_level) %>%
      summarise(
        n_trials = n_distinct(paste(subject_id, run, trial_index)),
        n_trials_with_valid = sum(has_valid),
        availability = mean(has_valid),
        .groups = "drop"
      )
    
    p_avail_diff <- ggplot(availability_summary_diff, aes(x = time_bin, y = availability, color = difficulty_level)) +
      geom_line(size = 1) +
      geom_vline(xintercept = 0.0,   linetype = "dashed", color = "gray50") +
      geom_vline(xintercept = 3.25,  linetype = "dashed", color = "gray50") +
      geom_vline(xintercept = 3.75,  linetype = "dashed", color = "gray50") +
      geom_vline(xintercept = 4.70,  linetype = "dashed", color = "blue",  alpha = 0.5) +
      geom_vline(xintercept = 7.70,  linetype = "dashed", color = "red",   alpha = 0.5) +
      facet_wrap(~ task) +
      scale_color_brewer(palette = "Set2", name = "Difficulty") +
      labs(
        title = "Time-Resolved Pupil Availability by Difficulty",
        subtitle = paste(
          "Vertical lines:",
          "0=grip gauge onset (TrialST),",
          "3.25=fixation onset (fixST),",
          "3.75=stimulus pair onset (A/V_ST),",
          "4.70=response window start (Resp1ST),",
          "7.70=response window end (Resp1ET; canonical)"
        ),
        x = "Time (s)",
        y = "Proportion of Trials with Valid Pupil"
      ) +
      theme_minimal() +
      theme(legend.position = "bottom")
    
    print(p_avail_diff)
    
    # Response-locked availability (for trials with RT)
    # Create RT lookup (unique per trial)
    rt_lookup <- trial_coverage_prefilter %>%
      select(subject_id, task, run, trial_index, rt, response_onset) %>%
      distinct(subject_id, task, run, trial_index, .keep_all = TRUE)
    
    response_locked <- time_availability %>%
      left_join(rt_lookup, by = c("subject_id", "task", "run", "trial_index")) %>%
      filter(!is.na(response_onset), !is.na(rt), rt > 0, rt < 5.0, !is.na(effort_condition)) %>%
      mutate(
        t_resp = time_bin - response_onset
      ) %>%
      filter(t_resp >= -6.0 & t_resp <= 1.0) %>%
      group_by(task, t_resp, effort_condition) %>%
      summarise(
        n_trials = n_distinct(paste(subject_id, run, trial_index)),
        availability = mean(has_valid),
        .groups = "drop"
      )
    
    if (nrow(response_locked) > 0) {
      p_resp_locked <- ggplot(response_locked, aes(x = t_resp, y = availability, color = effort_condition)) +
        geom_line(size = 1) +
        geom_vline(xintercept = 0, linetype = "dashed", color = "red", alpha = 0.7) +
        facet_wrap(~ task) +
        scale_color_brewer(palette = "Set1", name = "Effort") +
        labs(
          title = "Response-Locked Pupil Availability",
          subtitle = "Time relative to response onset (0 = response)",
          x = "Time Relative to Response (s)",
          y = "Proportion of Trials with Valid Pupil"
        ) +
        theme_minimal() +
        theme(legend.position = "bottom")
      
      print(p_resp_locked)
    }
  }
}
```

# Dissertation Inclusion Decisions (Ch2 / Ch3)

## Subject Inclusion Decision Matrix

```{r inclusion-decision-matrix, include=FALSE}
# Build comprehensive inclusion decision matrix for Chapter 2 and Chapter 3 analyses

if (!is.null(trial_coverage_prefilter) && !is.null(threshold_sweep)) {
  # Load analysis-ready data to get overall_quality
  pupil_file <- file.path(analysis_ready_dir, "BAP_analysis_ready_PUPIL.csv")
  behav_file <- file.path(analysis_ready_dir, "BAP_analysis_ready_BEHAVIORAL.csv")
  
  has_pupil_ready <- file.exists(pupil_file)
  has_behav_ready <- file.exists(behav_file)
  
  # Get overall_quality from analysis-ready pupil data if available
  # If not available, compute from quality_iti and quality_prestim, or use raw files
  if (has_pupil_ready) {
    pupil_ready <- readr::read_csv(pupil_file, show_col_types = FALSE)
    
    if ("overall_quality" %in% names(pupil_ready)) {
      quality_lookup <- pupil_ready %>%
        select(subject_id, task, run, trial_index, overall_quality) %>%
        distinct(subject_id, task, run, trial_index, .keep_all = TRUE)
    } else if ("quality_iti" %in% names(pupil_ready) && "quality_prestim" %in% names(pupil_ready)) {
      # Compute overall_quality as mean of quality_iti and quality_prestim
      quality_lookup <- pupil_ready %>%
        mutate(overall_quality = (quality_iti + quality_prestim) / 2) %>%
        select(subject_id, task, run, trial_index, overall_quality) %>%
        distinct(subject_id, task, run, trial_index, .keep_all = TRUE)
    } else {
      # Try to get from raw flat files
      quality_lookup <- NULL
    }
    
    # Join quality to trial_coverage_prefilter
    if (!is.null(quality_lookup)) {
      trial_coverage_with_quality <- trial_coverage_prefilter %>%
        left_join(quality_lookup, by = c("subject_id", "task", "run", "trial_index"))
    } else {
      # If no quality data available, try to get from raw files
      # Check if raw files have overall_quality
      sample_file <- if (length(flat_files) > 0) flat_files[1] else NULL
      if (!is.null(sample_file) && file.exists(sample_file)) {
        sample_df <- readr::read_csv(sample_file, n_max = 100, show_col_types = FALSE)
        if ("overall_quality" %in% names(sample_df)) {
          # Extract quality from raw files (this is expensive, so we'll use a simplified approach)
          # For now, set to NA and note that quality tiers won't be available
          trial_coverage_with_quality <- trial_coverage_prefilter %>%
            mutate(overall_quality = NA_real_)
        } else {
          trial_coverage_with_quality <- trial_coverage_prefilter %>%
            mutate(overall_quality = NA_real_)
        }
      } else {
        trial_coverage_with_quality <- trial_coverage_prefilter %>%
          mutate(overall_quality = NA_real_)
      }
    }
  } else {
    # If analysis-ready not available, use a placeholder (all NA)
    trial_coverage_with_quality <- trial_coverage_prefilter %>%
      mutate(overall_quality = NA_real_)
  }
  
  # Get behavioral data for DDM-ready checks
  if (has_behav_ready) {
    behav_ready_raw <- readr::read_csv(behav_file, show_col_types = FALSE)
  } else if (file.exists(behavioral_file)) {
    behav_ready_raw <- readr::read_csv(behavioral_file, show_col_types = FALSE)
  } else {
    behav_ready_raw <- NULL
  }

  if (!is.null(behav_ready_raw)) {
    # Determine choice column if available
    choice_col <- dplyr::case_when(
      "resp_is_diff" %in% names(behav_ready_raw) ~ "resp_is_diff",
      "iscorr" %in% names(behav_ready_raw) ~ "iscorr",
      "accuracy" %in% names(behav_ready_raw) ~ "accuracy",
      TRUE ~ NA_character_
    )

    behav_ready <- behav_ready_raw %>%
      mutate(
        subject_id = if ("subject_id" %in% names(.)) as.character(subject_id) else
          if ("sub" %in% names(.)) as.character(sub) else NA_character_,
        task = if ("task" %in% names(.)) {
          dplyr::if_else(task == "aud", "ADT",
                         dplyr::if_else(task == "vis", "VDT", as.character(task)))
        } else if ("task_modality" %in% names(.)) {
          dplyr::case_when(
            task_modality == "aud" ~ "ADT",
            task_modality == "vis" ~ "VDT",
            TRUE ~ as.character(task_modality)
          )
        } else NA_character_,
        rt = if ("same_diff_resp_secs" %in% names(.)) same_diff_resp_secs else 
          if ("resp1RT" %in% names(.)) resp1RT else
          if ("rt" %in% names(.)) rt else NA_real_,
        choice = if (!is.na(choice_col) && choice_col %in% names(.)) as.numeric(.data[[choice_col]]) else NA_real_
      )
  } else {
    behav_ready <- NULL
  }
  
  # Define quality tiers
  assign_quality_tier <- function(overall_quality) {
    dplyr::case_when(
      is.na(overall_quality) ~ NA_character_,
      overall_quality >= 0.80 ~ "Tier1_High",
      overall_quality >= 0.60 ~ "Tier2_Medium",
      overall_quality >= 0.50 ~ "Tier3_Low",
      TRUE ~ "Fail"
    )
  }
  
  # Check if we have any quality data
  has_quality_data <- !all(is.na(trial_coverage_with_quality$overall_quality))
  
  if (!has_quality_data) {
    # If no quality data, assign all trials to a default tier for gate-based filtering
    # This allows gates to work but quality-tier filtering won't be meaningful
    trial_coverage_with_quality <- trial_coverage_with_quality %>%
      mutate(quality_tier = "Tier2_Medium")  # Default to medium so gates still work
  }
  
  # Gate flags at threshold 0.80
  t0 <- 0.80
  gf_080 <- gate_flags(trial_coverage_with_quality, t0)
  
  # Add quality tier and gate flags
  # Ensure trial_id exists (should already be there from trial_coverage_prefilter)
  if (!"trial_id" %in% names(trial_coverage_with_quality)) {
    trial_coverage_with_quality <- trial_coverage_with_quality %>%
      mutate(trial_id = paste(subject_id, task, run, trial_index, sep = ":"))
  }
  
  trial_coverage_with_quality <- trial_coverage_with_quality %>%
    mutate(
      quality_tier = assign_quality_tier(overall_quality),
      # DEPRECATED: Old nested gates (kept for backwards compatibility)
      gate_stimlocked_080 = gf_080$gate_A,
      gate_total_auc_080 = gf_080$gate_B,
      gate_cog_auc_080 = gf_080$gate_C,
      # NEW: Independent analysis-specific gates
      gate_stimlocked_080 = gf_080$gate_stimlocked,
      gate_total_auc_080 = gf_080$gate_total_auc,
      gate_cog_auc_080 = gf_080$gate_cog_auc
    )
  
  # Build inclusion matrix by subject × task
  inclusion_base <- trial_coverage_with_quality %>%
    group_by(subject_id, task) %>%
    summarise(
      n_trials_prefilter = n(),
      # DEPRECATED: Old nested gates
      n_gate_stimlocked_080 = sum(gate_stimlocked_080, na.rm = TRUE),
      n_gate_total_auc_080 = sum(gate_total_auc_080, na.rm = TRUE),
      n_gate_cog_auc_080 = sum(gate_cog_auc_080, na.rm = TRUE),
      # NEW: Analysis-specific gates
      n_gate_stimlocked_080 = sum(gate_stimlocked_080, na.rm = TRUE),
      n_gate_total_auc_080 = sum(gate_total_auc_080, na.rm = TRUE),
      n_gate_cog_auc_080 = sum(gate_cog_auc_080, na.rm = TRUE),
      .groups = "drop"
    )
  
  # Add quality-tier filtered counts (using cognitive-AUC gate for cognitive analyses)
  gate_counts_by_tier <- trial_coverage_with_quality %>%
    filter(gate_cog_auc_080) %>%
    group_by(subject_id, task, quality_tier) %>%
    summarise(n_gateC_tier = n_distinct(trial_id), .groups = "drop") %>%
    pivot_wider(names_from = quality_tier, values_from = n_gateC_tier, values_fill = 0)
  
  inclusion_base <- inclusion_base %>%
    left_join(gate_counts_by_tier, by = c("subject_id", "task"))
  
  # Compute Tier 1-2 and Tier 1-3 counts
  # Ensure tier columns exist even if some tiers are absent
  if (!"Tier1_High" %in% names(inclusion_base)) inclusion_base$Tier1_High <- 0L
  if (!"Tier2_Medium" %in% names(inclusion_base)) inclusion_base$Tier2_Medium <- 0L
  if (!"Tier3_Low" %in% names(inclusion_base)) inclusion_base$Tier3_Low <- 0L

  inclusion_base <- inclusion_base %>%
    mutate(
      Tier1_High = ifelse(is.na(Tier1_High), 0, Tier1_High),
      Tier2_Medium = ifelse(is.na(Tier2_Medium), 0, Tier2_Medium),
      Tier3_Low = ifelse(is.na(Tier3_Low), 0, Tier3_Low),
      n_gateC_tier12 = Tier1_High + Tier2_Medium,
      n_gateC_tier123 = Tier1_High + Tier2_Medium + Tier3_Low
    )
  
  # Chapter 2 Primary: GateC + Tier1-2 + >=45 trials + tertile intensity coverage
  # First, get intensity levels (stimLev) for tertile coverage check
  if (has_pupil_ready && "stimLev" %in% names(pupil_ready)) {
    intensity_lookup <- pupil_ready %>%
      select(subject_id, task, run, trial_index, stimLev, cognitive_auc) %>%
      distinct(subject_id, task, run, trial_index, .keep_all = TRUE)
    
    trial_coverage_with_intensity <- trial_coverage_with_quality %>%
      left_join(intensity_lookup, by = c("subject_id", "task", "run", "trial_index"))
  } else {
    # Fallback: use stimLev from trial_coverage_prefilter if available
    trial_coverage_with_intensity <- trial_coverage_with_quality %>%
      mutate(
        # If raw stimulus levels are not available, set to NA; tertile diagnostics will then skip
        stimLev = NA_real_,
        cognitive_auc = NA_real_
      )
  }
  
  # Check tertile intensity coverage for C2-primary
  # Only compute if cognitive_auc is available
  if (has_pupil_ready && "cognitive_auc" %in% names(pupil_ready) && 
      any(!is.na(trial_coverage_with_intensity$cognitive_auc))) {
    c2_primary_checks <- trial_coverage_with_intensity %>%
      filter(gate_cog_auc_080, quality_tier %in% c("Tier1_High", "Tier2_Medium"), !is.na(cognitive_auc)) %>%
      group_by(subject_id, task) %>%
      filter(n() >= 45) %>%  # First filter to subjects with enough trials
      mutate(
        tertile = ntile(cognitive_auc, 3),
        tertile_label = case_when(
          tertile == 1 ~ "Low",
          tertile == 2 ~ "Med",
          tertile == 3 ~ "High",
          TRUE ~ NA_character_
        )
      ) %>%
      group_by(subject_id, task, tertile_label) %>%
      summarise(
        n_per_tertile = n_distinct(trial_id),
        intensity_levels_present = n_distinct(stimLev, na.rm = TRUE),
        intensity_coverage = intensity_levels_present >= 3,  # >=3/4 levels
        .groups = "drop"
      ) %>%
      group_by(subject_id, task) %>%
      summarise(
        all_tertiles_covered = all(intensity_coverage),
        min_trials_per_tertile = min(n_per_tertile),
        .groups = "drop"
      )
  } else {
    # If cognitive_auc not available, create empty result
    c2_primary_checks <- inclusion_base %>%
      select(subject_id, task) %>%
      mutate(all_tertiles_covered = FALSE, min_trials_per_tertile = 0) %>%
      filter(FALSE)  # Empty but with correct structure
  }
  
  # Chapter 2 Secondary: GateC + Tier1-3 + >=30 trials + intensity coverage
  c2_secondary_checks <- trial_coverage_with_intensity %>%
    filter(gate_cog_auc_080, quality_tier %in% c("Tier1_High", "Tier2_Medium", "Tier3_Low")) %>%
    group_by(subject_id, task) %>%
    summarise(
      n_gateC_tier123 = n(),
      intensity_levels_present = n_distinct(stimLev, na.rm = TRUE),
      intensity_coverage = intensity_levels_present >= 3,  # >=3/4 levels overall
      .groups = "drop"
    )
  
  # Chapter 2 Effort manipulation: GateB + GateC + Tier1-2 + >=20 per effort
  c2_effort_checks <- trial_coverage_with_quality %>%
    filter(gate_total_auc_080, gate_cog_auc_080, quality_tier %in% c("Tier1_High", "Tier2_Medium")) %>%
    group_by(subject_id, task, effort_condition) %>%
    summarise(n_per_effort = n_distinct(trial_id), .groups = "drop") %>%
    group_by(subject_id, task) %>%
    summarise(
      min_effort_trials = min(n_per_effort, na.rm = TRUE),
      has_both_efforts = n_distinct(effort_condition, na.rm = TRUE) >= 2,
      .groups = "drop"
    )
  
  # Chapter 3 Behavior-only: DDM-ready trials (choice + RT in [0.2, 3.0])
  if (!is.null(behav_ready)) {
    # Ensure behav_ready has trial_id for consistent counting
    if (!"trial_id" %in% names(behav_ready)) {
      behav_ready <- behav_ready %>%
        mutate(
          run = if ("run" %in% names(.)) run else if ("run_num" %in% names(.)) run_num else 1L,
          trial_index = if ("trial_index" %in% names(.)) trial_index else 
            if ("trial_in_run" %in% names(.)) trial_in_run else
            if ("trial" %in% names(.)) trial else row_number(),
          trial_id = paste(subject_id, task, run, trial_index, sep = ":")
        )
    }
    
    c3_behavior_checks <- behav_ready %>%
      filter(
        !is.na(choice),
        !is.na(rt),
        rt >= 0.2,
        rt <= 3.0
      ) %>%
      group_by(subject_id, task) %>%
      summarise(
        n_ddm_ready = n_distinct(trial_id),
        .groups = "drop"
      ) %>%
      group_by(subject_id) %>%
      summarise(
        n_ddm_ready_total = sum(n_ddm_ready, na.rm = TRUE),
        n_ddm_ready_per_task = min(n_ddm_ready, na.rm = TRUE),
        .groups = "drop"
      )
  } else {
    c3_behavior_checks <- inclusion_base %>%
      select(subject_id) %>%
      distinct() %>%
      mutate(n_ddm_ready_total = 0, n_ddm_ready_per_task = 0)
  }
  
  # Chapter 3 Pupil: GateC + Tier1-2 + >=80 total + >=20 per effort
  c3_pupil_checks <- trial_coverage_with_quality %>%
    filter(gate_cog_auc_080, quality_tier %in% c("Tier1_High", "Tier2_Medium")) %>%
    group_by(subject_id, task, effort_condition) %>%
    summarise(n_per_effort = n_distinct(trial_id), .groups = "drop") %>%
    group_by(subject_id) %>%
    summarise(
      n_gateC_tier12_total = sum(n_per_effort, na.rm = TRUE),
      min_effort_trials = min(n_per_effort, na.rm = TRUE),
      has_both_efforts = n_distinct(effort_condition, na.rm = TRUE) >= 2,
      .groups = "drop"
    )
  
  # Combine all checks into inclusion matrix
  inclusion_matrix <- inclusion_base %>%
    left_join(c2_primary_checks %>% select(subject_id, task, all_tertiles_covered, min_trials_per_tertile), 
              by = c("subject_id", "task")) %>%
    left_join(c2_secondary_checks %>% select(subject_id, task, n_gateC_tier123, intensity_coverage), 
              by = c("subject_id", "task")) %>%
    left_join(c2_effort_checks %>% select(subject_id, task, min_effort_trials, has_both_efforts), 
              by = c("subject_id", "task")) %>%
    left_join(c3_behavior_checks %>% select(subject_id, n_ddm_ready_total, n_ddm_ready_per_task), 
              by = "subject_id") %>%
    left_join(c3_pupil_checks %>% select(subject_id, n_gateC_tier12_total, min_effort_trials_pupil = min_effort_trials, has_both_efforts_pupil = has_both_efforts), 
              by = "subject_id") %>%
    { 
      # Ensure columns exist before mutate (for robustness)
      if (!"n_gateC_tier123" %in% names(.)) .$n_gateC_tier123 <- 0L
      if (!"intensity_coverage" %in% names(.)) .$intensity_coverage <- NA
      .
    } %>%
    mutate(
      # C2 Primary: GateC Tier1-2 >=45 AND tertile intensity coverage
      c2_primary_pass = n_gateC_tier12 >= 45 & 
        !is.na(all_tertiles_covered) & all_tertiles_covered,
      c2_primary_fail_reason = case_when(
        c2_primary_pass ~ "",
        n_gateC_tier12 < 45 ~ "LOW_N_GATEC",
        is.na(all_tertiles_covered) | !all_tertiles_covered ~ "FAIL_TERTILE_INTENSITY",
        TRUE ~ "UNKNOWN"
      ),
      
      # C2 Secondary: GateC Tier1-3 >=30 AND intensity coverage
      c2_secondary_pass = n_gateC_tier123 >= 30 & 
        !is.na(intensity_coverage) & intensity_coverage,
      c2_secondary_fail_reason = case_when(
        c2_secondary_pass ~ "",
        n_gateC_tier123 < 30 ~ "LOW_N_GATEC",
        is.na(intensity_coverage) | !intensity_coverage ~ "FAIL_INTENSITY_COVERAGE",
        TRUE ~ "UNKNOWN"
      ),
      
      # C2 Effort: GateB + GateC Tier1-2 + >=20 per effort
      c2_effort_pass = !is.na(min_effort_trials) & min_effort_trials >= 20 & 
        !is.na(has_both_efforts) & has_both_efforts,
      c2_effort_fail_reason = case_when(
        c2_effort_pass ~ "",
        is.na(min_effort_trials) | min_effort_trials < 20 ~ "LOW_N_PER_EFFORT",
        is.na(has_both_efforts) | !has_both_efforts ~ "MISSING_EFFORT_LEVEL",
        TRUE ~ "UNKNOWN"
      ),
      
      # C3 Behavior: >=120 total OR >=60 per task
      c3_behavior_pass = (!is.na(n_ddm_ready_total) & n_ddm_ready_total >= 120) |
        (!is.na(n_ddm_ready_per_task) & n_ddm_ready_per_task >= 60),
      c3_behavior_fail_reason = case_when(
        c3_behavior_pass ~ "",
        is.na(n_ddm_ready_total) | (n_ddm_ready_total < 120 & n_ddm_ready_per_task < 60) ~ "LOW_N_DDMREADY",
        TRUE ~ "UNKNOWN"
      ),
      
      # C3 Pupil: GateC Tier1-2 >=80 total AND >=20 per effort
      c3_pupil_pass = !is.na(n_gateC_tier12_total) & n_gateC_tier12_total >= 80 &
        !is.na(min_effort_trials_pupil) & min_effort_trials_pupil >= 20 &
        !is.na(has_both_efforts_pupil) & has_both_efforts_pupil,
      c3_pupil_fail_reason = case_when(
        c3_pupil_pass ~ "",
        is.na(n_gateC_tier12_total) | n_gateC_tier12_total < 80 ~ "LOW_N_GATEC_TOTAL",
        is.na(min_effort_trials_pupil) | min_effort_trials_pupil < 20 ~ "LOW_N_PER_EFFORT",
        is.na(has_both_efforts_pupil) | !has_both_efforts_pupil ~ "MISSING_EFFORT_LEVEL",
        TRUE ~ "UNKNOWN"
      )
    )
  
  # Create summary columns
  inclusion_matrix <- inclusion_matrix %>%
    mutate(
      c2_primary_status = ifelse(c2_primary_pass, "PASS", "FAIL"),
      c2_secondary_status = ifelse(c2_secondary_pass, "PASS", "FAIL"),
      c2_effort_status = ifelse(c2_effort_pass, "PASS", "FAIL"),
      c3_behavior_status = ifelse(c3_behavior_pass, "PASS", "FAIL"),
      c3_pupil_status = ifelse(c3_pupil_pass, "PASS", "FAIL")
    )
}
```

```{r inclusion-matrix-display, results='asis'}
if (!is.null(inclusion_matrix)) {
  cat("This matrix shows which subjects qualify for each analysis based on the criteria defined in the Methods section.\n\n")
  
  # Display the matrix
  display_matrix <- inclusion_matrix %>%
    select(
      subject_id, task,
      n_trials_prefilter,
      n_gateC_tier12, n_gateC_tier123,
      c2_primary_status, c2_primary_fail_reason,
      c2_secondary_status, c2_secondary_fail_reason,
      c2_effort_status, c2_effort_fail_reason,
      c3_behavior_status, c3_behavior_fail_reason,
      c3_pupil_status, c3_pupil_fail_reason
    ) %>%
    arrange(c2_primary_status, c3_pupil_status, subject_id, task)
  
  display_matrix %>%
    kable(
      col.names = c(
        "Subject", "Task", "N Prefilter", "GateC T1-2", "GateC T1-3",
        "C2 Primary", "C2 Primary Reason", "C2 Secondary", "C2 Secondary Reason",
        "C2 Effort", "C2 Effort Reason", "C3 Behavior", "C3 Behavior Reason",
        "C3 Pupil", "C3 Pupil Reason"
      ),
      caption = "Subject Inclusion Decision Matrix: PASS/FAIL status for each analysis"
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      full_width = TRUE,
      font_size = 9
    ) %>%
    scroll_box(height = "600px") %>%
    print()
  
  # Summary counts
  cat("\n### Inclusion Summary\n\n")
  
  summary_counts <- inclusion_matrix %>%
    summarise(
      total_subject_tasks = n(),
      c2_primary_pass = sum(c2_primary_pass, na.rm = TRUE),
      c2_secondary_pass = sum(c2_secondary_pass, na.rm = TRUE),
      c2_effort_pass = sum(c2_effort_pass, na.rm = TRUE),
      c3_behavior_pass = sum(c3_behavior_pass, na.rm = TRUE),
      c3_pupil_pass = sum(c3_pupil_pass, na.rm = TRUE)
    )
  
  cat("**Total subject-task combinations:**", summary_counts$total_subject_tasks, "\n")
  cat("- **C2 Primary (Cognitive AUC tertiles)**:", summary_counts$c2_primary_pass, "PASS\n")
  cat("- **C2 Secondary (Continuous Cognitive AUC)**:", summary_counts$c2_secondary_pass, "PASS\n")
  cat("- **C2 Effort Manipulation**:", summary_counts$c2_effort_pass, "PASS\n")
  cat("- **C3 Behavior-Only DDM**:", summary_counts$c3_behavior_pass, "PASS\n")
  cat("- **C3 DDM with Pupil Predictors**:", summary_counts$c3_pupil_pass, "PASS\n\n")
}
```

```{r inclusion-csv-exports, include=FALSE}
# Export inclusion lists as CSVs

if (!is.null(inclusion_matrix)) {
  inclusion_output_dir <- file.path("quality_control", "output", "inclusion")
  dir.create(inclusion_output_dir, recursive = TRUE, showWarnings = FALSE)
  
  # C2 Primary
  c2_primary_list <- inclusion_matrix %>%
    filter(c2_primary_pass) %>%
    select(subject_id, task) %>%
    arrange(subject_id, task)
  
  # Always write file (even if empty) so downstream tools see consistent outputs
  write_csv(c2_primary_list, file.path(inclusion_output_dir, "C2_primary_subject_task.csv"))
  
  # C2 Secondary
  c2_secondary_list <- inclusion_matrix %>%
    filter(c2_secondary_pass) %>%
    select(subject_id, task) %>%
    arrange(subject_id, task)
  
  write_csv(c2_secondary_list, file.path(inclusion_output_dir, "C2_secondary_subject_task.csv"))
  
  # C3 Behavior
  c3_behavior_list <- inclusion_matrix %>%
    filter(c3_behavior_pass) %>%
    select(subject_id) %>%
    distinct() %>%
    arrange(subject_id)
  
  write_csv(c3_behavior_list, file.path(inclusion_output_dir, "C3_behavior_subject.csv"))
  
  # C3 Pupil
  c3_pupil_list <- inclusion_matrix %>%
    filter(c3_pupil_pass) %>%
    select(subject_id) %>%
    distinct() %>%
    arrange(subject_id)
  
  write_csv(c3_pupil_list, file.path(inclusion_output_dir, "C3_pupil_subject.csv"))

  # Also export the full inclusion matrix for LLM/QA consumption
  write_csv(inclusion_matrix,
            file.path(inclusion_output_dir, "subject_inclusion_matrix.csv"))
  
  cat("Inclusion lists exported to:", inclusion_output_dir, "\n")
}
```

## Analysis Feasibility Matrix (Prompt 7)

```{r analysis-feasibility-matrix, fig.height=10, results='asis'}
if (!is.null(inclusion_matrix) && !is.null(trial_coverage_prefilter)) {
  cat("This matrix shows subject-level eligibility for each analysis type with PASS/FAIL status and fail reasons.\n\n")
  
  # Build comprehensive feasibility matrix
  # Get gate flags at 0.80
  thr_feas <- 0.80
  gf_feas <- gate_flags(trial_coverage_prefilter, thr_feas)
  
  # Subject-level summaries
  feasibility_base <- trial_coverage_prefilter %>%
    mutate(
      gate_total_auc_080 = gf_feas$gate_B,
      gate_cog_auc_080 = gf_feas$gate_C,
      gate_C_fixed_1s_080 = !is.na(valid_prop_baseline_500ms) & !is.na(valid_prop_prestim) &
        valid_prop_baseline_500ms >= thr_feas & valid_prop_prestim >= thr_feas &
        !is.na(valid_prop_cognitive_fixed_1s) & valid_prop_cognitive_fixed_1s >= thr_feas
    ) %>%
    group_by(subject_id, task) %>%
    summarise(
      n_trials_total = n_distinct(trial_id),
      n_gate_B = n_distinct(trial_id[gate_total_auc_080]),
      n_gate_C = n_distinct(trial_id[gate_cog_auc_080]),
      n_gate_C_fixed_1s = n_distinct(trial_id[gate_C_fixed_1s_080]),
      n_unknown_effort = n_distinct(trial_id[is.na(effort_condition)]),
      n_unknown_difficulty = n_distinct(trial_id[is.na(difficulty_level)]),
      pct_unknown_effort = round(100 * n_unknown_effort / n_trials_total, 1),
      pct_unknown_difficulty = round(100 * n_unknown_difficulty / n_trials_total, 1),
      .groups = "drop"
    ) %>%
    left_join(
      inclusion_matrix %>% 
        select(subject_id, task, 
               c2_effort_pass, c2_effort_fail_reason,
               c2_primary_pass, c2_primary_fail_reason,
               c2_secondary_pass, c2_secondary_fail_reason,
               c3_behavior_pass, c3_behavior_fail_reason,
               c3_pupil_pass, c3_pupil_fail_reason),
      by = c("subject_id", "task")
    )
  
  # Check for effort asymmetry (from threshold_sweep if available)
  if (!is.null(threshold_sweep)) {
    effort_asymmetry <- threshold_sweep %>%
      filter(gate == "gate_C", threshold == thr_feas) %>%
      filter(!is.na(effort_condition)) %>%
      group_by(subject_id, task, effort_condition) %>%
      summarise(n_retained = sum(n_trials_retained, na.rm = TRUE), .groups = "drop") %>%
      group_by(subject_id, task) %>%
      summarise(
        has_asymmetry = n_distinct(effort_condition) >= 2 && 
          (max(n_retained, na.rm = TRUE) > 2 * min(n_retained, na.rm = TRUE)),
        .groups = "drop"
      )
    
    feasibility_base <- feasibility_base %>%
      left_join(effort_asymmetry, by = c("subject_id", "task"))
  } else {
    feasibility_base$has_asymmetry <- FALSE
  }
  
  # Define analysis eligibility
  feasibility_matrix <- feasibility_base %>%
    mutate(
      # C2 Effort Manipulation: Total-AUC gate + Cognitive-AUC gate, >=20 per effort
      c2_effort_manip_status = ifelse(c2_effort_pass, "PASS", "FAIL"),
      c2_effort_manip_reason = ifelse(c2_effort_pass, "",
        ifelse(!is.na(c2_effort_fail_reason), c2_effort_fail_reason,
          ifelse(n_gate_B < 20 | n_gate_C < 20, "LOW_TRIALS", "UNKNOWN"))),
      
      # C2 Trial-Level Coupling: Cognitive-AUC gate, >=45 trials (strict)
      c2_trial_coupling_status = ifelse(n_gate_C >= 45, "PASS", "FAIL"),
      c2_trial_coupling_reason = ifelse(n_gate_C >= 45, "",
        ifelse(n_gate_C < 30, "LOW_TRIALS", "INSUFFICIENT_TRIALS")),
      
      # C2 Subject-Level Coupling: Cognitive-AUC gate OR fixed-window cognitive (looser)
      c2_subject_coupling_status = ifelse(n_gate_C >= 30 | n_gate_C_fixed_1s >= 30, "PASS", "FAIL"),
      c2_subject_coupling_reason = ifelse(n_gate_C >= 30 | n_gate_C_fixed_1s >= 30, "",
        ifelse(n_gate_C < 20 & n_gate_C_fixed_1s < 20, "LOW_TRIALS", "INSUFFICIENT_TRIALS")),
      
      # C3 Pupil-Predictor: Cognitive-AUC gate strict, >=80 total, >=20 per effort
      c3_pupil_predictor_status = ifelse(c3_pupil_pass, "PASS", "FAIL"),
      c3_pupil_predictor_reason = ifelse(c3_pupil_pass, "",
        ifelse(!is.na(c3_pupil_fail_reason), c3_pupil_fail_reason,
          ifelse(n_gate_C < 80, "LOW_TRIALS", "UNKNOWN"))),
      
      # C3 Behavior-Only: No pupil gates, just DDM-ready trials
      c3_behavior_only_status = ifelse(c3_behavior_pass, "PASS", "FAIL"),
      c3_behavior_only_reason = ifelse(c3_behavior_pass, "",
        ifelse(!is.na(c3_behavior_fail_reason), c3_behavior_fail_reason, "LOW_TRIALS")),
      
      # Additional flags
      has_missing_conditions = pct_unknown_effort > 20 | pct_unknown_difficulty > 20,
      unstable_threshold = FALSE  # Could be computed from threshold sensitivity if needed
    )
  
  # Create display matrix
  display_feasibility <- feasibility_matrix %>%
    select(
      subject_id, task,
      c2_effort_manip_status, c2_effort_manip_reason,
      c2_trial_coupling_status, c2_trial_coupling_reason,
      c2_subject_coupling_status, c2_subject_coupling_reason,
      c3_pupil_predictor_status, c3_pupil_predictor_reason,
      c3_behavior_only_status, c3_behavior_only_reason
    ) %>%
    arrange(subject_id, task)
  
  display_feasibility %>%
    kable(
      col.names = c(
        "Subject", "Task",
        "C2 Effort Manip", "C2 Effort Reason",
        "C2 Trial Coupling", "C2 Trial Reason",
        "C2 Subject Coupling", "C2 Subject Reason",
        "C3 Pupil Predictor", "C3 Pupil Reason",
        "C3 Behavior Only", "C3 Behavior Reason"
      ),
      caption = "Analysis Feasibility Matrix: PASS/FAIL status for each analysis type with fail reasons"
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      full_width = TRUE,
      font_size = 8
    ) %>%
    row_spec(which(display_feasibility$c2_effort_manip_status == "FAIL" & 
                  display_feasibility$c2_trial_coupling_status == "FAIL" &
                  display_feasibility$c3_pupil_predictor_status == "FAIL"), 
             background = "#ffeeee") %>%
    scroll_box(height = "700px") %>%
    print()
  
  # Summary by analysis type
  cat("\n### Feasibility Summary by Analysis Type\n\n")
  
  feasibility_summary <- feasibility_matrix %>%
    summarise(
      total_subject_tasks = n(),
      c2_effort_manip_pass = sum(c2_effort_manip_status == "PASS", na.rm = TRUE),
      c2_trial_coupling_pass = sum(c2_trial_coupling_status == "PASS", na.rm = TRUE),
      c2_subject_coupling_pass = sum(c2_subject_coupling_status == "PASS", na.rm = TRUE),
      c3_pupil_predictor_pass = sum(c3_pupil_predictor_status == "PASS", na.rm = TRUE),
      c3_behavior_only_pass = sum(c3_behavior_only_status == "PASS", na.rm = TRUE),
      .groups = "drop"
    )
  
  feasibility_summary %>%
    kable(
      col.names = c("Total Subject-Tasks", "C2 Effort Manip", "C2 Trial Coupling", 
                   "C2 Subject Coupling", "C3 Pupil Predictor", "C3 Behavior Only"),
      caption = "Number of subject-task combinations passing each analysis type"
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      full_width = FALSE
    ) %>%
    print()
  
  cat("\n**Fail Reason Codes:**\n")
  cat("- `LOW_TRIALS`: Insufficient number of trials\n")
  cat("- `INSUFFICIENT_TRIALS`: Trials below minimum threshold\n")
  cat("- `LOW_N_GATEC`: Not enough Cognitive-AUC gate trials\n")
  cat("- `LOW_N_PER_EFFORT`: Not enough trials per effort level\n")
  cat("- `MISSING_EFFORT_LEVEL`: Missing one or both effort levels\n")
  cat("- `FAIL_TERTILE_INTENSITY`: Tertile intensity coverage failed\n")
  cat("- `FAIL_INTENSITY_COVERAGE`: Intensity level coverage failed\n")
  cat("- `UNKNOWN`: Reason not determined\n\n")
}
```

## Tertile Feasibility Diagnostics (Chapter 2 Primary)

```{r tertile-feasibility-diagnostics}
if (!is.null(trial_coverage_prefilter) && has_pupil_ready && 
    "cognitive_auc" %in% names(pupil_ready)) {
  
  # Get C2-primary candidate trials
  c2_primary_candidates <- trial_coverage_with_intensity %>%
    filter(gate_cog_auc_080, quality_tier %in% c("Tier1_High", "Tier2_Medium"), !is.na(cognitive_auc)) %>%
    group_by(subject_id, task) %>%
    filter(n() >= 45)  # Only subjects with enough trials
  
  if (nrow(c2_primary_candidates) > 0) {
    # Compute tertiles and intensity coverage
    tertile_diagnostics <- c2_primary_candidates %>%
      group_by(subject_id, task) %>%
      mutate(
        tertile = ntile(cognitive_auc, 3),
        tertile_label = factor(case_when(
          tertile == 1 ~ "Low",
          tertile == 2 ~ "Med",
          tertile == 3 ~ "High",
          TRUE ~ NA_character_
        ), levels = c("Low", "Med", "High"))
      ) %>%
      group_by(subject_id, task, tertile_label, stimLev) %>%
      summarise(n_trials = n_distinct(trial_id), .groups = "drop") %>%
      group_by(subject_id, task, tertile_label) %>%
      summarise(
        n_trials_tertile = sum(n_trials),
        intensity_levels_present = n_distinct(stimLev, na.rm = TRUE),
        intensity_coverage_pass = intensity_levels_present >= 3,
        .groups = "drop"
      )
    
    # Create heatmap: intensity levels × tertiles
    tertile_heatmap_data <- c2_primary_candidates %>%
      group_by(subject_id, task) %>%
      mutate(
        tertile = ntile(cognitive_auc, 3),
        tertile_label = factor(case_when(
          tertile == 1 ~ "Low",
          tertile == 2 ~ "Med",
          tertile == 3 ~ "High",
          TRUE ~ NA_character_
        ), levels = c("Low", "Med", "High"))
      ) %>%
      group_by(subject_id, task, tertile_label, stimLev) %>%
      summarise(n_trials = n_distinct(trial_id), .groups = "drop")
    
    # Summary table
    tertile_summary <- tertile_diagnostics %>%
      group_by(subject_id, task) %>%
      summarise(
        all_tertiles_covered = all(intensity_coverage_pass),
        min_trials_per_tertile = min(n_trials_tertile),
        .groups = "drop"
      )
    
    cat("### Tertile Intensity Coverage Summary\n\n")
    cat("For subjects with ≥45 Cognitive-AUC gate Tier1-2 trials, this shows whether each tertile has adequate intensity coverage (≥3/4 levels with ≥2 trials each).\n\n")
    
    tertile_summary %>%
      mutate(
        status = ifelse(all_tertiles_covered, "PASS", "FAIL"),
        fail_reason = ifelse(!all_tertiles_covered, "FAIL_TERTILE_INTENSITY_COVERAGE", "")
      ) %>%
      select(subject_id, task, status, min_trials_per_tertile, fail_reason) %>%
      arrange(status, subject_id, task) %>%
      kable(
        col.names = c("Subject", "Task", "Tertile Coverage", "Min Trials/Tertile", "Fail Reason"),
        caption = "Tertile intensity coverage check for Chapter 2 Primary analysis"
      ) %>%
      kable_styling(
        bootstrap_options = c("striped", "hover", "condensed"),
        full_width = FALSE
      ) %>%
      print()
    
    # Heatmap for a sample of subjects (worst cases first)
    if (nrow(tertile_heatmap_data) > 0) {
      worst_subjects <- tertile_summary %>%
        filter(!all_tertiles_covered) %>%
        arrange(min_trials_per_tertile) %>%
        head(12) %>%
        select(subject_id, task)
      
      if (nrow(worst_subjects) > 0) {
        heatmap_sample <- tertile_heatmap_data %>%
          inner_join(worst_subjects, by = c("subject_id", "task")) %>%
          mutate(subject_task = paste(subject_id, task, sep = "_"))
        
        p_tertile_heatmap <- ggplot(heatmap_sample, aes(x = factor(stimLev), y = tertile_label, fill = n_trials)) +
          geom_tile() +
          facet_wrap(~ subject_task, scales = "free_x") +
          scale_fill_gradient(low = "white", high = "darkblue", name = "Trials") +
          labs(
            title = "Tertile × Intensity Coverage (Worst 12 Cases)",
            subtitle = "Cells show trial counts. Each tertile needs ≥3 intensity levels with ≥2 trials.",
            x = "Stimulus Level (Intensity)",
            y = "Cognitive AUC Tertile"
          ) +
          theme_minimal() +
          theme(axis.text.x = element_text(angle = 45, hjust = 1))
        
        print(p_tertile_heatmap)
      }
    }
  } else {
    cat("**Note**: No subjects meet the minimum threshold (≥45 Cognitive-AUC gate Tier1-2 trials) for tertile diagnostics.\n")
    cat("Consider lowering the threshold or checking data quality.\n\n")
  }
}
```

## Missingness Model: Is Missingness Condition-Linked? (MNAR Risk Check)

```{r missingness-model, results='asis'}
if (!is.null(trial_coverage_prefilter)) {
  # Create trial-level dataset with pass/fail outcome
  missingness_data <- trial_coverage_with_quality %>%
    mutate(
      pass_gatec = as.integer(gate_cog_auc_080),
      effort_condition_f = factor(effort_condition),
      difficulty_level_f = factor(difficulty_level),
      task_f = factor(task)
    ) %>%
    filter(!is.na(effort_condition_f), !is.na(difficulty_level_f), !is.na(task_f))
  
  if (nrow(missingness_data) > 100) {  # Only run if enough data
    # Simple logistic model (fast, no need for full mixed model here)
    # Using glm for speed; could use glmer if needed
    tryCatch({
      model_missing <- glm(
        pass_gatec ~ effort_condition_f + difficulty_level_f + task_f,
        data = missingness_data,
        family = binomial
      )
      
      # Extract coefficients
      if (requireNamespace("broom", quietly = TRUE)) {
        coef_summary <- broom::tidy(model_missing, conf.int = TRUE, exponentiate = TRUE) %>%
          filter(term != "(Intercept)")
      } else {
        # Fallback: manual extraction
        coef_summary <- tibble(
          term = names(coef(model_missing))[-1],
          estimate = exp(coef(model_missing)[-1]),
          conf.low = exp(confint(model_missing)[-1, 1]),
          conf.high = exp(confint(model_missing)[-1, 2]),
          p.value = summary(model_missing)$coefficients[-1, 4]
        )
      }
      
      cat("### Logistic Model: Trial Passes Cognitive-AUC gate ~ Condition\n\n")
      cat("This model tests whether missingness (failing Cognitive-AUC gate) is systematically related to experimental conditions.\n")
      cat("**Interpretation**: If effort or difficulty significantly predicts missingness, this suggests Missing Not At Random (MNAR) bias.\n\n")
      
      coef_summary %>%
        mutate(
          term_clean = case_when(
            grepl("effort", term) ~ gsub("effort_condition_f", "Effort: ", term),
            grepl("difficulty", term) ~ gsub("difficulty_level_f", "Difficulty: ", term),
            grepl("task", term) ~ gsub("task_f", "Task: ", term),
            TRUE ~ term
          )
        ) %>%
        select(term_clean, estimate, conf.low, conf.high, p.value) %>%
        mutate(
          p_sig = ifelse(p.value < 0.001, "***",
                        ifelse(p.value < 0.01, "**",
                              ifelse(p.value < 0.05, "*", "")))
        ) %>%
        kable(
          col.names = c("Predictor", "Odds Ratio", "CI Lower", "CI Upper", "p-value", "Sig"),
          caption = "Logistic regression: Does condition predict Cognitive-AUC gate pass/fail?",
          digits = c(NA, 3, 3, 3, 4, NA)
        ) %>%
        kable_styling(
          bootstrap_options = c("striped", "hover", "condensed"),
          full_width = FALSE
        ) %>%
        print()
      
      # Predicted probabilities plot
      pred_data <- expand_grid(
        effort_condition_f = unique(missingness_data$effort_condition_f),
        difficulty_level_f = unique(missingness_data$difficulty_level_f),
        task_f = unique(missingness_data$task_f)
      ) %>%
        mutate(
          pred_prob = predict(model_missing, newdata = ., type = "response")
        )
      
      p_missing_pred <- ggplot(pred_data, aes(x = effort_condition_f, y = pred_prob, fill = difficulty_level_f)) +
        geom_col(position = "dodge") +
        facet_wrap(~ task_f) +
        scale_fill_brewer(palette = "Set2", name = "Difficulty") +
        labs(
          title = "Predicted Probability of Passing Cognitive-AUC gate by Condition",
          subtitle = "Higher values = better data quality. Systematic differences suggest MNAR bias.",
          x = "Effort Condition",
          y = "Predicted Pass Probability"
        ) +
        theme_minimal() +
        theme(legend.position = "bottom")
      
      print(p_missing_pred)
      
      # Flag significant effort effects
      effort_effects <- coef_summary %>%
        filter(grepl("effort", term), p.value < 0.05)
      
      if (nrow(effort_effects) > 0) {
        cat("\n### ⚠️ Warning: Effort-Linked Missingness Detected\n\n")
        cat("Significant effort effects on missingness suggest that High effort may cause more data loss.\n")
        cat("**Recommendation**: Run sensitivity analysis excluding subjects with effort asymmetry (as flagged in Automated Warnings section).\n\n")
      }
      
    }, error = function(e) {
      cat("**Note**: Could not fit missingness model. Error:", e$message, "\n\n")
    })
  } else {
    cat("**Note**: Insufficient data to fit missingness model.\n\n")
  }
}
```

## DDM-Ready QC (Chapter 3 Alignment)

```{r ddm-ready-qc, results='asis'}
if (!is.null(behav_ready)) {
  # DDM-ready trials: choice + RT in [0.2, 3.0]
  ddm_ready_trials <- behav_ready %>%
    filter(
      !is.na(choice),
      !is.na(rt),
      rt >= 0.2,
      rt <= 3.0
    ) %>%
    mutate(
      task = if ("task_modality" %in% names(.)) {
        dplyr::case_when(
          task_modality == "aud" ~ "ADT",
          task_modality == "vis" ~ "VDT",
          TRUE ~ as.character(task_modality)
        )
      } else task
    )
  
  if (nrow(ddm_ready_trials) > 0) {
    # Ensure trial_id exists for behavioral data
    if (!"trial_id" %in% names(ddm_ready_trials)) {
      ddm_ready_trials <- ddm_ready_trials %>%
        mutate(
          run = if ("run" %in% names(.)) run else if ("run_num" %in% names(.)) run_num else 1L,
          trial_index = if ("trial_index" %in% names(.)) trial_index else 
            if ("trial_in_run" %in% names(.)) trial_in_run else
            if ("trial" %in% names(.)) trial else row_number(),
          trial_id = paste(subject_id, task, run, trial_index, sep = ":")
        )
    }
    
    # Per subject × task summaries
    ddm_summary <- ddm_ready_trials %>%
      group_by(subject_id, task) %>%
      summarise(
        n_ddm_ready = n_distinct(trial_id),
        mean_rt = mean(rt, na.rm = TRUE),
        median_rt = median(rt, na.rm = TRUE),
        sd_rt = sd(rt, na.rm = TRUE),
        p25_rt = quantile(rt, 0.25, na.rm = TRUE),
        p75_rt = quantile(rt, 0.75, na.rm = TRUE),
        accuracy = mean(choice == 1, na.rm = TRUE),
        .groups = "drop"
      )
    
    # Per subject totals
    ddm_summary_total <- ddm_ready_trials %>%
      group_by(subject_id) %>%
      summarise(
        n_ddm_ready_total = n_distinct(trial_id),
        mean_rt_total = mean(rt, na.rm = TRUE),
        .groups = "drop"
      )
    
    cat("### DDM-Ready Trial Counts and RT Distributions\n\n")
    cat("**DDM-ready criteria**: Non-missing choice + RT in [0.2, 3.0] seconds\n\n")
    
    # RT distribution plots
    p_rt_dist <- ggplot(ddm_ready_trials, aes(x = rt)) +
      geom_histogram(bins = 50, fill = "steelblue", alpha = 0.7) +
      facet_wrap(~ task) +
      geom_vline(xintercept = c(0.2, 3.0), linetype = "dashed", color = "red", alpha = 0.5) +
      labs(
        title = "RT Distribution for DDM-Ready Trials",
        subtitle = "Red lines: RT filter boundaries [0.2, 3.0] seconds",
        x = "Response Time (seconds)",
        y = "Count"
      ) +
      theme_minimal()
    
    print(p_rt_dist)
    
    # RT by effort if available
    if ("effort_condition" %in% names(ddm_ready_trials) || 
        "grip_targ_prop_mvc" %in% names(ddm_ready_trials)) {
      ddm_ready_trials <- ddm_ready_trials %>%
        mutate(
          effort = if ("effort_condition" %in% names(.)) effort_condition else
            if ("grip_targ_prop_mvc" %in% names(.)) {
              ifelse(grip_targ_prop_mvc == 0.05, "Low", 
                    ifelse(grip_targ_prop_mvc == 0.40, "High", NA_character_))
            } else NA_character_
        )
      
      if (any(!is.na(ddm_ready_trials$effort))) {
        p_rt_effort <- ggplot(ddm_ready_trials %>% filter(!is.na(effort)), 
                              aes(x = rt, fill = effort)) +
          geom_histogram(bins = 50, alpha = 0.7, position = "identity") +
          facet_wrap(~ task) +
          scale_fill_brewer(palette = "Set1", name = "Effort") +
          labs(
            title = "RT Distribution by Effort Condition",
            x = "Response Time (seconds)",
            y = "Count"
          ) +
          theme_minimal() +
          theme(legend.position = "bottom")
        
        print(p_rt_effort)
      }
    }
    
    # Subject-level boxplots (flag extreme medians)
    ddm_summary_display <- ddm_summary %>%
      left_join(ddm_summary_total, by = "subject_id") %>%
      mutate(
        c3_behavior_pass = n_ddm_ready_total >= 120 | n_ddm_ready >= 60,
        rt_extreme = median_rt < 0.3 | median_rt > 2.5
      )
    
    cat("### DDM-Ready Summary by Subject × Task\n\n")
    
    ddm_summary_display %>%
      select(subject_id, task, n_ddm_ready, mean_rt, median_rt, accuracy, c3_behavior_pass, rt_extreme) %>%
      arrange(c3_behavior_pass, subject_id, task) %>%
      kable(
        col.names = c("Subject", "Task", "N DDM-Ready", "Mean RT", "Median RT", "Accuracy", "C3 Behavior Pass", "Extreme RT"),
        caption = "DDM-ready trial counts and RT statistics",
        digits = 3
      ) %>%
      kable_styling(
        bootstrap_options = c("striped", "hover", "condensed"),
        full_width = FALSE
      ) %>%
      print()
    
    # Boxplot of RT by subject
    if (nrow(ddm_ready_trials) > 0) {
      # Sample subjects for visualization (show worst cases)
      worst_subjects <- ddm_summary_display %>%
        filter(!c3_behavior_pass | rt_extreme) %>%
        arrange(n_ddm_ready) %>%
        head(20) %>%
        pull(subject_id) %>%
        unique()
      
      if (length(worst_subjects) > 0) {
        p_rt_boxplot <- ggplot(
          ddm_ready_trials %>% filter(subject_id %in% worst_subjects),
          aes(x = reorder(subject_id, rt, median), y = rt)
        ) +
          geom_boxplot(outlier.alpha = 0.3) +
          geom_hline(yintercept = c(0.2, 3.0), linetype = "dashed", color = "red", alpha = 0.5) +
          coord_flip() +
          labs(
            title = "RT Distribution by Subject (Worst Cases)",
            subtitle = "Red lines: RT filter boundaries. Subjects with extreme medians or low N flagged.",
            x = "Subject ID",
            y = "Response Time (seconds)"
          ) +
          theme_minimal()
        
        print(p_rt_boxplot)
      }
    }
  } else {
    cat("**Note**: No DDM-ready trials found. Check behavioral data file.\n\n")
  }
} else {
  cat("**Note**: Behavioral data not available for DDM-ready QC.\n\n")
}
```

## Sensitivity Analysis Panel

```{r sensitivity-analysis-panel, results='asis', fig.height=8}
# Comprehensive sensitivity analysis across thresholds and minimum trial requirements
#
# METHODOLOGY:
# 1. For each gate threshold (0.50, 0.60, 0.70, 0.80, 0.90, 0.95):
#    - Apply gate_flags() to determine which trials pass Cognitive-AUC gate at that threshold
#    - Cognitive-AUC gate requires: baseline_500ms >= threshold AND prestim >= threshold AND cognitive_auc >= threshold
# 2. For each minimum trial requirement (20, 30, 45, 60, 80):
#    - Count how many subjects (across all tasks) have >= min_trials Cognitive-AUC gate trials
#    - Count total subject-task combinations passing
#    - Sum total trials retained across all passing subjects
# 3. Generate visualizations and summary statistics
#
# This analysis helps answer:
# - How sensitive is subject inclusion to threshold choice?
# - What minimum trial requirement balances power with inclusion?
# - What threshold × min_trials combination maximizes usable data?

if (!is.null(trial_coverage_prefilter) && !is.null(threshold_sweep)) {
  cat("## Sensitivity Analysis: Threshold and Minimum Trial Robustness\n\n")
  cat("This panel shows how subject inclusion changes across different gate thresholds and minimum trial requirements.\n\n")
  cat("**Methodology**: For each threshold × minimum trial combination, we count:\n")
  cat("- Number of unique subjects with ≥min_trials Cognitive-AUC gate trials\n")
  cat("- Number of subject-task combinations passing\n")
  cat("- Total trials retained across all passing subjects\n\n")
  
  # Define parameter grids
  gate_thresholds <- params$threshold_grid %||% c(0.50, 0.60, 0.70, 0.80, 0.90, 0.95)
  min_trials_options <- c(20, 30, 45, 60, 80)
  
  cat("**Parameter Grid**:\n")
  cat("- Gate thresholds:", paste(gate_thresholds, collapse = ", "), "\n")
  cat("- Minimum trial requirements:", paste(min_trials_options, collapse = ", "), "\n\n")
  
  # For each combination, compute subject counts passing Cognitive-AUC gate
  # This uses the gate_flags() function defined earlier in the report
  sensitivity_summary <- purrr::map_dfr(
    gate_thresholds,
    function(gate_threshold) {
      # Apply gate flags at this threshold
      gf <- gate_flags(trial_coverage_prefilter, gate_threshold)
      
      purrr::map_dfr(
        min_trials_options,
        function(min_trials) {
          # Filter to trials passing Cognitive-AUC gate, then count per subject-task
          result <- trial_coverage_prefilter %>%
            mutate(gate_C = gf$gate_C) %>%
            filter(gate_C) %>%
            group_by(subject_id, task) %>%
            summarise(n_trials = n_distinct(trial_id), .groups = "drop") %>%
            filter(n_trials >= min_trials)
          
          if (nrow(result) > 0) {
            # Compute summary statistics
            result %>%
              summarise(
                gate_threshold = gate_threshold,
                min_trials = min_trials,
                n_subjects_passing = n_distinct(subject_id),
                n_subject_tasks_passing = n(),
                total_trials_retained = sum(n_trials),
                mean_trials_per_subject_task = round(mean(n_trials), 1),
                .groups = "drop"
              )
          } else {
            # No subjects pass this combination
            tibble(
              gate_threshold = gate_threshold,
              min_trials = min_trials,
              n_subjects_passing = 0L,
              n_subject_tasks_passing = 0L,
              total_trials_retained = 0L,
              mean_trials_per_subject_task = 0
            )
          }
        }
      )
    }
  )
  
  # Plot 1: Subject count heatmap
  p_sens_heatmap <- sensitivity_summary %>%
    ggplot(aes(x = factor(gate_threshold), y = factor(min_trials), fill = n_subjects_passing)) +
    geom_tile(color = "white", size = 0.5) +
    scale_fill_viridis_c(option = "C", name = "Subjects\nPassing", direction = 1) +
    labs(
      title = "Subject Count Passing Cognitive-AUC gate Across Thresholds and Minimum Trial Requirements",
      subtitle = "Each cell shows number of unique subjects with ≥min_trials Cognitive-AUC gate trials at given threshold",
      x = "Gate Threshold",
      y = "Minimum Trials Required"
    ) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      plot.title = element_text(size = 12, face = "bold")
    )
  
  print(p_sens_heatmap)
  
  # Plot 2: Line plot showing subject count vs threshold for different min_trials
  p_sens_lines_subjects <- sensitivity_summary %>%
    mutate(
      min_trials_label = paste0("Min ", min_trials, " trials")
    ) %>%
    ggplot(aes(x = gate_threshold, y = n_subjects_passing, 
               color = min_trials_label, linetype = min_trials_label)) +
    geom_line(size = 1.2) +
    geom_point(size = 2.5) +
    scale_color_brewer(palette = "Set2", name = "Minimum Trials") +
    scale_linetype_manual(values = c("solid", "dashed", "dotted", "dotdash", "longdash"), name = "Minimum Trials") +
    labs(
      title = "Subject Count Across Thresholds",
      subtitle = "How many subjects pass at each threshold, for different minimum trial requirements",
      x = "Gate Threshold",
      y = "Number of Subjects Passing"
    ) +
    theme_minimal() +
    theme(legend.position = "bottom")
  
  print(p_sens_lines_subjects)
  
  # Plot 3: Total trials retained vs threshold
  p_sens_lines_trials <- sensitivity_summary %>%
    filter(min_trials == 45) %>%  # Focus on C2-primary-like criteria
    ggplot(aes(x = gate_threshold, y = total_trials_retained)) +
    geom_line(size = 1.5, color = "steelblue") +
    geom_point(size = 3, color = "steelblue") +
    labs(
      title = "Total Trials Retained Across Thresholds (Min 45 Trials)",
      subtitle = "Total Cognitive-AUC gate trials across all subjects meeting minimum requirement",
      x = "Gate Threshold",
      y = "Total Trials Retained"
    ) +
    theme_minimal()
  
  print(p_sens_lines_trials)
  
  # Summary table
  cat("\n### Sensitivity Summary Table\n\n")
  cat("This table shows how subject and trial counts change across different threshold and minimum trial combinations.\n\n")
  
  sensitivity_summary %>%
    arrange(gate_threshold, min_trials) %>%
    mutate(
      pct_subjects = round(100 * n_subjects_passing / max(n_subjects_passing, na.rm = TRUE), 1)
    ) %>%
    select(gate_threshold, min_trials, n_subjects_passing, n_subject_tasks_passing, 
           total_trials_retained, mean_trials_per_subject_task) %>%
    kable(
      col.names = c("Gate Threshold", "Min Trials", "Subjects Passing", 
                   "Subject-Tasks Passing", "Total Trials", "Mean Trials/Subject-Task"),
      caption = "Sensitivity analysis: subject and trial counts across parameter combinations",
      digits = 1
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      full_width = FALSE
    ) %>%
    print()
  
  # Key insights
  cat("\n### Key Insights\n\n")
  
  # Compare 0.70 vs 0.80 at min_trials = 45
  sens_070 <- sensitivity_summary %>%
    filter(gate_threshold == 0.70, min_trials == 45)
  sens_080 <- sensitivity_summary %>%
    filter(gate_threshold == 0.80, min_trials == 45)
  
  if (nrow(sens_070) > 0 && nrow(sens_080) > 0) {
    subject_loss <- sens_070$n_subjects_passing - sens_080$n_subjects_passing
    trial_loss <- sens_070$total_trials_retained - sens_080$total_trials_retained
    pct_subject_loss <- round(100 * subject_loss / sens_070$n_subjects_passing, 1)
    pct_trial_loss <- round(100 * trial_loss / sens_070$total_trials_retained, 1)
    
    cat("- **Moving from threshold 0.70 → 0.80 (min 45 trials)**:\n")
    cat("  - Subject loss:", subject_loss, "subjects (", pct_subject_loss, "%)\n")
    cat("  - Trial loss:", trial_loss, "trials (", pct_trial_loss, "%)\n\n")
  }
  
  # Find most permissive combination that still gives reasonable counts
  best_combo <- sensitivity_summary %>%
    filter(n_subjects_passing >= 10, total_trials_retained >= 500) %>%
    arrange(desc(n_subjects_passing), desc(total_trials_retained)) %>%
    slice(1)
  
  if (nrow(best_combo) > 0) {
    cat("- **Recommended permissive combination**: Threshold =", best_combo$gate_threshold, 
        ", Min trials =", best_combo$min_trials, "\n")
    cat("  - Provides", best_combo$n_subjects_passing, "subjects with", 
        best_combo$total_trials_retained, "total trials\n\n")
  }
  
  # Find strictest combination that still gives usable counts
  strict_combo <- sensitivity_summary %>%
    filter(n_subjects_passing >= 5, total_trials_retained >= 200) %>%
    arrange(gate_threshold, desc(min_trials)) %>%
    slice(1)
  
  if (nrow(strict_combo) > 0) {
    cat("- **Recommended strict combination**: Threshold =", strict_combo$gate_threshold, 
        ", Min trials =", strict_combo$min_trials, "\n")
    cat("  - Provides", strict_combo$n_subjects_passing, "subjects with", 
        strict_combo$total_trials_retained, "total trials\n\n")
  }
  
  cat("**Interpretation**: Use this analysis to choose threshold and minimum trial requirements that balance\n")
  cat("data quality (higher threshold) with sample size (more subjects passing).\n\n")
  
} else {
  cat("## Sensitivity Analysis Panel\n\n")
  cat("Sensitivity analysis requires `trial_coverage_prefilter` and `threshold_sweep` data.\n")
  cat("These should be available if the report was generated with `recompute_raw_coverage = TRUE`.\n\n")
}
```

# Analysis-Ready Data Statistics (Post-Filter)

## Loading Analysis-Ready Data

```{r load-analysis-ready}
# Check if analysis-ready files exist
pupil_file <- file.path(analysis_ready_dir, "BAP_analysis_ready_PUPIL.csv")
behav_file <- file.path(analysis_ready_dir, "BAP_analysis_ready_BEHAVIORAL.csv")

has_pupil_data <- file.exists(pupil_file)
has_behav_data <- file.exists(behav_file)

if (has_pupil_data) {
  pupil_data <- read_csv(pupil_file, show_col_types = FALSE)
  cat("**Pupil data loaded:**", nrow(pupil_data), "trials\n")
  cat("**Subjects in pupil data:**", length(unique(pupil_data$subject_id)), "\n")
} else {
  cat("**WARNING:** Pupil analysis-ready file not found. Run the pipeline first.\n")
  pupil_data <- NULL
}

if (has_behav_data) {
  behav_data <- read_csv(behav_file, show_col_types = FALSE)
  cat("**Behavioral data loaded:**", nrow(behav_data), "trials\n")
  cat("**Subjects in behavioral data:**", length(unique(behav_data$subject_id)), "\n")
} else {
  cat("**WARNING:** Behavioral analysis-ready file not found. Run the pipeline first.\n")
  behav_data <- NULL
}
```

## Subject-Level Statistics

```{r subject-level-stats}
if (!is.null(pupil_data)) {
  subject_stats <- pupil_data %>%
    group_by(subject_id) %>%
    summarise(
      n_trials = n(),
      n_runs = length(unique(run)),
      tasks = paste(sort(unique(task)), collapse = ", "),
      n_tasks = length(unique(task)),
      # Trials by task
      trials_adt = sum(task == "ADT", na.rm = TRUE),
      trials_vdt = sum(task == "VDT", na.rm = TRUE),
      # Trials by effort
      trials_low = sum(effort_condition == "Low_5_MVC", na.rm = TRUE),
      trials_high = sum(effort_condition == "High_40_MVC", na.rm = TRUE),
      # Trials by difficulty
      trials_standard = sum(difficulty_level == "Standard", na.rm = TRUE),
      trials_easy = sum(difficulty_level == "Easy", na.rm = TRUE),
      trials_hard = sum(difficulty_level == "Hard", na.rm = TRUE),
      # Quality metrics
      mean_quality_iti = mean(quality_iti, na.rm = TRUE),
      mean_quality_prestim = mean(quality_prestim, na.rm = TRUE),
      # AUC metrics
      has_total_auc = sum(!is.na(total_auc)),
      has_cognitive_auc = sum(!is.na(cognitive_auc)),
      mean_total_auc = mean(total_auc, na.rm = TRUE),
      mean_cognitive_auc = mean(cognitive_auc, na.rm = TRUE),
      .groups = "drop"
    ) %>%
    arrange(subject_id)
  
  cat("**Subjects with pupil data:**", nrow(subject_stats), "\n")
} else {
  subject_stats <- NULL
}
```

### Subject-Level Statistics Table

```{r subject-stats-table}
if (!is.null(subject_stats)) {
  subject_stats %>%
    select(
      subject_id, n_trials, n_runs, tasks, 
      trials_adt, trials_vdt,
      trials_low, trials_high,
      mean_quality_iti, mean_quality_prestim
    ) %>%
    kable(
      col.names = c("Subject", "Trials", "Runs", "Tasks", "ADT", "VDT", "Low", "High", "Quality ITI", "Quality PreStim"),
      caption = "Subject-level statistics for all subjects with pupil data",
      digits = 3
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      full_width = TRUE,
      font_size = 10
    ) %>%
    scroll_box(height = "500px")
}
```

## Subject Report Cards (Prompt 8)

```{r subject-report-cards, results='asis', fig.height=6}
if (!is.null(trial_coverage_prefilter) && !is.null(threshold_sweep)) {
  cat("Compact diagnostic panels for each subject showing key metrics at a glance.\n\n")
  
  # Get expected trials if available
  if (exists("expected_trials_by_task") && !is.null(expected_trials_by_task) && nrow(expected_trials_by_task) > 0) {
    expected_trials_lookup <- expected_trials_by_task
  } else {
    expected_trials_lookup <- trial_coverage_prefilter %>%
      group_by(subject_id, task) %>%
      summarise(expected_trials = n_distinct(trial_id), .groups = "drop")
  }
  
  # Get gate flags at multiple thresholds (compute on full dataset)
  gates_070 <- gate_flags(trial_coverage_prefilter, 0.70)
  gates_080 <- gate_flags(trial_coverage_prefilter, 0.80)
  gates_090 <- gate_flags(trial_coverage_prefilter, 0.90)
  
  # Build subject-level summary
  # Ensure gate flags match the number of rows in trial_coverage_prefilter
  if (nrow(trial_coverage_prefilter) != length(gates_080$gate_C)) {
    stop(sprintf("Size mismatch: trial_coverage_prefilter has %d rows but gates_080 has %d elements. This suggests trial_coverage_prefilter is not truly row-level.", 
                 nrow(trial_coverage_prefilter), length(gates_080$gate_C)))
  }
  
  subject_cards <- trial_coverage_prefilter %>%
    mutate(
      gate_A_070 = gates_070$gate_A,
      gate_B_070 = gates_070$gate_B,
      gate_C_070 = gates_070$gate_C,
      gate_stimlocked_080 = gates_080$gate_A,
      gate_total_auc_080 = gates_080$gate_B,
      gate_cog_auc_080 = gates_080$gate_C,
      gate_A_090 = gates_090$gate_A,
      gate_B_090 = gates_090$gate_B,
      gate_C_090 = gates_090$gate_C
    ) %>%
    group_by(subject_id, task) %>%
    summarise(
      n_trials_total = n_distinct(trial_id),
      n_trials_any_pupil = n_distinct(trial_id[!is.na(valid_prop_baseline_500ms)]),
      n_gate_A_070 = n_distinct(trial_id[gate_A_070]),
      n_gate_B_070 = n_distinct(trial_id[gate_B_070]),
      n_gate_C_070 = n_distinct(trial_id[gate_C_070]),
      n_gate_stimlocked_080 = n_distinct(trial_id[gate_stimlocked_080]),
      n_gate_total_auc_080 = n_distinct(trial_id[gate_total_auc_080]),
      n_gate_cog_auc_080 = n_distinct(trial_id[gate_cog_auc_080]),
      n_gate_A_090 = n_distinct(trial_id[gate_A_090]),
      n_gate_B_090 = n_distinct(trial_id[gate_B_090]),
      n_gate_C_090 = n_distinct(trial_id[gate_C_090]),
      pct_unknown_effort = round(100 * n_distinct(trial_id[is.na(effort_condition)]) / n_distinct(trial_id), 1),
      pct_unknown_difficulty = round(100 * n_distinct(trial_id[is.na(difficulty_level)]) / n_distinct(trial_id), 1),
      .groups = "drop"
    ) %>%
    left_join(expected_trials_lookup, by = c("subject_id", "task")) %>%
    mutate(
      expected_trials = ifelse(is.na(expected_trials), n_trials_total, expected_trials)
    )
  
  # Get condition balance for heatmap (filter out rows with missing conditions)
  # Compute gates on the filtered data to ensure size match
  trial_coverage_filtered <- trial_coverage_prefilter %>%
    filter(!is.na(effort_condition), !is.na(difficulty_level))
  
  gates_080_filtered <- gate_flags(trial_coverage_filtered, 0.80)
  
  cond_balance <- trial_coverage_filtered %>%
    mutate(
      gate_cog_auc_080 = gates_080_filtered$gate_C,
      effort_label = case_when(
        effort_condition == "Low_5_MVC" ~ "Low",
        effort_condition == "High_40_MVC" ~ "High",
        TRUE ~ NA_character_
      ),
      difficulty_label = as.character(difficulty_level)
    ) %>%
    filter(gate_cog_auc_080, !is.na(effort_label)) %>%
    group_by(subject_id, task, effort_label, difficulty_label) %>%
    summarise(n_retained = n_distinct(trial_id), .groups = "drop")
  
  # Create collapsible panels per subject
  subjects_list <- subject_cards %>%
    arrange(subject_id, task) %>%
    distinct(subject_id, task) %>%
    mutate(subject_task_id = paste(subject_id, task, sep = "_"))
  
  for (i in 1:nrow(subjects_list)) {
    subj <- subjects_list$subject_id[i]
    task_name <- subjects_list$task[i]
    subj_task_id <- subjects_list$subject_task_id[i]
    
    card_data <- subject_cards %>%
      filter(subject_id == subj, task == task_name)
    
    if (nrow(card_data) == 0) next
    
    cat("<details>\n")
    cat("<summary><strong>", subj, "—", task_name, "</strong></summary>\n\n")
    
    # Trial counts
    cat("**Trial Counts:**\n")
    cat("- Expected:", card_data$expected_trials, "\n")
    cat("- With any pupil:", card_data$n_trials_any_pupil, "\n")
    cat("- Total recorded:", card_data$n_trials_total, "\n\n")
    
    # Retention at gates
    cat("**Retention at Gates (0.70 / 0.80 / 0.90):**\n")
    cat("- Stimulus-locked gate (Baseline+Prestim):", card_data$n_gate_A_070, "/", card_data$n_gate_stimlocked_080, "/", card_data$n_gate_A_090, "\n")
    cat("- Total-AUC gate (Total AUC):", card_data$n_gate_B_070, "/", card_data$n_gate_total_auc_080, "/", card_data$n_gate_B_090, "\n")
    cat("- Cognitive-AUC gate (Cognitive AUC):", card_data$n_gate_C_070, "/", card_data$n_gate_cog_auc_080, "/", card_data$n_gate_C_090, "\n\n")
    
    # Condition balance heatmap
    cat("**Condition Balance (Cognitive-AUC gate @ 0.80):**\n")
    cond_subj <- cond_balance %>%
      filter(subject_id == subj, task == task_name)
    
    if (nrow(cond_subj) > 0) {
      cond_heatmap <- cond_subj %>%
        ggplot(aes(x = difficulty_label, y = effort_label, fill = n_retained)) +
        geom_tile() +
        scale_fill_viridis_c(option = "C", name = "Trials") +
        labs(
          title = paste(subj, task_name, "— Cognitive-AUC gate @ 0.80"),
          x = "Difficulty",
          y = "Effort"
        ) +
        theme_minimal() +
        theme(plot.title = element_text(size = 10))
      
      print(cond_heatmap)
    } else {
      cat("No Cognitive-AUC gate trials at 0.80 threshold.\n\n")
    }
    
    # Missing condition labels (data quality issue)
    cat("**Missing Condition Labels (Data Quality Issue):**\n")
    cat("- Missing effort condition:", card_data$pct_unknown_effort, "% of trials\n")
    cat("- Missing difficulty level:", card_data$pct_unknown_difficulty, "% of trials\n")
    if (card_data$pct_unknown_effort > 0 || card_data$pct_unknown_difficulty > 0) {
      cat("- **Warning**: Trials with missing conditions should be investigated and fixed in the source data.\n")
    }
    cat("\n")
    
    # Time-resolved availability sparklines (simplified - would need time_availability data)
    cat("**Note**: Time-resolved availability sparklines would require cached time_availability data.\n")
    cat("See 'Time-Resolved Availability Curve' section for full plots.\n\n")
    
    cat("</details>\n\n")
  }
  
  cat("\n**Instructions**: Click on each subject-task to expand and view detailed diagnostics.\n\n")
}
```

## Effective Valid Seconds in Each Window (Prompt 9)

```{r effective-valid-seconds, results='asis'}
if (!is.null(trial_coverage_prefilter)) {
  cat("This section computes valid_seconds (n_valid_samples / sampling_rate) and missing_gap_max (largest contiguous missing gap) for each trial and window.\n")
  cat("This helps decide whether a trial that fails 0.80 is still usable for certain windows.\n\n")
  
  # Assume sampling rate (typically 1000 Hz for EyeLink, but check your data)
  sampling_rate <- 1000  # Hz - adjust if needed
  
  # Compute valid seconds and missing gaps for each window
  # This requires sample-level data, so we'll compute from proportions and window durations
  valid_seconds_data <- trial_coverage_prefilter %>%
    mutate(
      # Window durations (in seconds)
      baseline_duration = 0.5,  # -0.5 to 0
      prestim_duration = 0.5,   # 3.25 to 3.75
      iti_duration = 3.0,        # -3.0 to 0
      total_auc_duration = ifelse(!is.na(response_onset) & response_onset > 0, response_onset, NA_real_),
      cognitive_auc_duration = ifelse(!is.na(response_onset) & response_onset > 4.65, response_onset - 4.65, NA_real_),
      
      # Valid seconds = proportion * duration
      valid_seconds_baseline = ifelse(!is.na(valid_prop_baseline_500ms), 
                                     valid_prop_baseline_500ms * baseline_duration, NA_real_),
      valid_seconds_prestim = ifelse(!is.na(valid_prop_prestim),
                                    valid_prop_prestim * prestim_duration, NA_real_),
      valid_seconds_iti = ifelse(!is.na(valid_prop_iti_full),
                                valid_prop_iti_full * iti_duration, NA_real_),
      valid_seconds_total_auc = ifelse(!is.na(valid_prop_total_auc) & !is.na(total_auc_duration),
                                       valid_prop_total_auc * total_auc_duration, NA_real_),
      valid_seconds_cognitive_auc = ifelse(!is.na(valid_prop_cognitive_auc) & !is.na(cognitive_auc_duration),
                                          valid_prop_cognitive_auc * cognitive_auc_duration, NA_real_),
      
      # Missing seconds = (1 - proportion) * duration
      missing_seconds_baseline = baseline_duration - valid_seconds_baseline,
      missing_seconds_prestim = prestim_duration - valid_seconds_prestim,
      missing_seconds_iti = iti_duration - valid_seconds_iti,
      missing_seconds_total_auc = ifelse(!is.na(total_auc_duration), 
                                         total_auc_duration - valid_seconds_total_auc, NA_real_),
      missing_seconds_cognitive_auc = ifelse(!is.na(cognitive_auc_duration),
                                             cognitive_auc_duration - valid_seconds_cognitive_auc, NA_real_)
    )
  
  # For missing_gap_max, we'd need sample-level data to find contiguous gaps
  # For now, approximate as: if proportion < 1.0, assume one large gap = missing_seconds
  valid_seconds_data <- valid_seconds_data %>%
    mutate(
      missing_gap_max_baseline = ifelse(!is.na(valid_prop_baseline_500ms) & valid_prop_baseline_500ms < 1.0,
                                       missing_seconds_baseline, 0),
      missing_gap_max_prestim = ifelse(!is.na(valid_prop_prestim) & valid_prop_prestim < 1.0,
                                      missing_seconds_prestim, 0),
      missing_gap_max_iti = ifelse(!is.na(valid_prop_iti_full) & valid_prop_iti_full < 1.0,
                                   missing_seconds_iti, 0),
      missing_gap_max_total_auc = ifelse(!is.na(valid_prop_total_auc) & valid_prop_total_auc < 1.0,
                                         missing_seconds_total_auc, 0),
      missing_gap_max_cognitive_auc = ifelse(!is.na(valid_prop_cognitive_auc) & valid_prop_cognitive_auc < 1.0,
                                             missing_seconds_cognitive_auc, 0)
    )
  
  # Summary per subject
  cat("### Valid Seconds Summary by Subject\n\n")
  
  valid_seconds_summary_subject <- valid_seconds_data %>%
    group_by(subject_id, task) %>%
    summarise(
      n_trials = n_distinct(trial_id),
      mean_valid_seconds_baseline = mean(valid_seconds_baseline, na.rm = TRUE),
      mean_valid_seconds_prestim = mean(valid_seconds_prestim, na.rm = TRUE),
      mean_valid_seconds_cognitive_auc = mean(valid_seconds_cognitive_auc, na.rm = TRUE),
      mean_missing_gap_max_cognitive_auc = mean(missing_gap_max_cognitive_auc, na.rm = TRUE),
      pct_trials_valid_baseline_080 = round(100 * sum(valid_seconds_baseline >= 0.4, na.rm = TRUE) / n_distinct(trial_id), 1),
      pct_trials_valid_prestim_080 = round(100 * sum(valid_seconds_prestim >= 0.4, na.rm = TRUE) / n_distinct(trial_id), 1),
      pct_trials_valid_cognitive_080 = round(100 * sum(valid_seconds_cognitive_auc >= 0.8, na.rm = TRUE) / n_distinct(trial_id), 1),
      .groups = "drop"
    )
  
  valid_seconds_summary_subject %>%
    kable(
      col.names = c("Subject", "Task", "N Trials", "Mean Valid Sec (Baseline)", 
                   "Mean Valid Sec (Prestim)", "Mean Valid Sec (Cognitive AUC)",
                   "Mean Max Gap (Cognitive AUC)", "% Valid Baseline (≥0.4s)",
                   "% Valid Prestim (≥0.4s)", "% Valid Cognitive (≥0.8s)"),
      caption = "Valid seconds summary by subject and task",
      digits = 2
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      full_width = FALSE
    ) %>%
    print()
  
  # Summary by condition
  cat("\n### Valid Seconds Summary by Condition (Cognitive-AUC gate @ 0.80)\n\n")
  
  # Filter valid_seconds_data first, then compute gates to ensure size match
  valid_seconds_filtered <- valid_seconds_data %>%
    filter(!is.na(effort_condition), !is.na(difficulty_level))
  
  gates_080_valid_seconds <- gate_flags(valid_seconds_filtered, 0.80)
  
  valid_seconds_summary_condition <- valid_seconds_filtered %>%
    mutate(
      gate_cog_auc_080 = gates_080_valid_seconds$gate_C,
      effort_label = case_when(
        effort_condition == "Low_5_MVC" ~ "Low",
        effort_condition == "High_40_MVC" ~ "High",
        TRUE ~ NA_character_
      ),
      difficulty_label = as.character(difficulty_level)
    ) %>%
    filter(gate_cog_auc_080, !is.na(effort_label)) %>%
    group_by(task, effort_label, difficulty_label) %>%
    summarise(
      n_trials = n_distinct(trial_id),
      mean_valid_seconds_cognitive = mean(valid_seconds_cognitive_auc, na.rm = TRUE),
      median_valid_seconds_cognitive = median(valid_seconds_cognitive_auc, na.rm = TRUE),
      mean_missing_gap_max = mean(missing_gap_max_cognitive_auc, na.rm = TRUE),
      pct_trials_valid_100 = round(100 * sum(valid_seconds_cognitive_auc >= cognitive_auc_duration * 0.99, na.rm = TRUE) / n_distinct(trial_id), 1),
      .groups = "drop"
    )
  
  valid_seconds_summary_condition %>%
    kable(
      col.names = c("Task", "Effort", "Difficulty", "N Trials", 
                   "Mean Valid Sec", "Median Valid Sec", "Mean Max Gap",
                   "% Valid ≥99%"),
      caption = "Valid seconds summary by condition (Cognitive-AUC gate @ 0.80)",
      digits = 2
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      full_width = FALSE
    ) %>%
    print()
  
  cat("\n**Interpretation**: Valid seconds shows the actual usable duration in each window.\n")
  cat("Trials with valid_seconds close to window duration (e.g., ≥0.8s for 1s cognitive window) may still be usable even if they fail a strict 0.80 proportion threshold.\n")
  cat("Missing_gap_max helps identify trials with large contiguous gaps that might be problematic.\n\n")
}
```

## Overall Trial Statistics

```{r overall-trial-stats}
if (!is.null(pupil_data)) {
  overall_stats <- list(
    "Total Trials" = nrow(pupil_data),
    "Total Subjects" = length(unique(pupil_data$subject_id)),
    "Trials by Task" = table(pupil_data$task),
    "Trials by Effort" = table(pupil_data$effort_condition),
    "Trials by Difficulty" = table(pupil_data$difficulty_level),
    "Trials with Total AUC" = sum(!is.na(pupil_data$total_auc)),
    "Trials with Cognitive AUC" = sum(!is.na(pupil_data$cognitive_auc))
  )
  
  # Create condition combinations
  # Ensure trial_id exists (create from available columns if needed)
  condition_stats <- pupil_data %>%
    mutate(
      trial_id = if ("trial_id" %in% names(.)) trial_id else
        if (all(c("subject_id", "task", "run", "trial_index") %in% names(.))) {
          paste(subject_id, task, run, trial_index, sep = ":")
        } else {
          # Fallback: use row number if trial identifiers not available
          paste0("trial_", row_number())
        }
    ) %>%
    filter(!grepl("Standard", difficulty_level)) %>%
    mutate(condition = paste0(difficulty_level, " / ", 
                             ifelse(effort_condition == "Low_5_MVC", "Low", "High"))) %>%
    group_by(task, condition) %>%
    summarise(n_trials = n_distinct(trial_id), .groups = "drop")
}
```

### Overall Statistics Summary

```{r overall-stats-table}
if (!is.null(pupil_data)) {
  stats_df <- tibble(
    Metric = c(
      "Total Trials",
      "Total Subjects",
      "Trials with Total AUC",
      "Trials with Cognitive AUC",
      "Mean Quality (ITI)",
      "Mean Quality (PreStim)"
    ),
    Value = c(
      nrow(pupil_data),
      length(unique(pupil_data$subject_id)),
      sum(!is.na(pupil_data$total_auc)),
      sum(!is.na(pupil_data$cognitive_auc)),
      round(mean(pupil_data$quality_iti, na.rm = TRUE), 3),
      round(mean(pupil_data$quality_prestim, na.rm = TRUE), 3)
    )
  )
  
  stats_df %>%
    kable(
      caption = "Overall statistics for analysis-ready pupil data",
      col.names = c("Metric", "Value")
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover"),
      full_width = FALSE
    )
}
```

### Trial Distribution by Task

```{r task-distribution}
if (!is.null(pupil_data)) {
  task_dist <- pupil_data %>%
    group_by(task) %>%
    summarise(
      n_trials = n(),
      n_subjects = length(unique(subject_id)),
      pct = round(100 * n() / nrow(pupil_data), 1),
      .groups = "drop"
    )
  
  task_dist %>%
    kable(
      caption = "Trial distribution by task",
      col.names = c("Task", "N Trials", "N Subjects", "%")
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover"),
      full_width = FALSE
    )
}
```

### Trial Distribution by Condition

```{r condition-distribution}
if (!is.null(condition_stats)) {
  condition_wide <- condition_stats %>%
    pivot_wider(names_from = task, values_from = n_trials, values_fill = 0)

  # Build column names dynamically based on tasks present
  col_names <- c("Condition", setdiff(names(condition_wide), "condition"))

  condition_wide %>%
    kable(
      caption = "Trial distribution by condition and task (Easy/Hard trials only)",
      col.names = col_names
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover"),
      full_width = FALSE
    )
}
```

# Quality Control Summary

## Quality Metrics Distribution

```{r quality-distribution}
if (!is.null(pupil_data)) {
  quality_summary <- pupil_data %>%
    summarise(
      mean_quality_iti = mean(quality_iti, na.rm = TRUE),
      sd_quality_iti = sd(quality_iti, na.rm = TRUE),
      min_quality_iti = min(quality_iti, na.rm = TRUE),
      max_quality_iti = max(quality_iti, na.rm = TRUE),
      mean_quality_prestim = mean(quality_prestim, na.rm = TRUE),
      sd_quality_prestim = sd(quality_prestim, na.rm = TRUE),
      min_quality_prestim = min(quality_prestim, na.rm = TRUE),
      max_quality_prestim = max(quality_prestim, na.rm = TRUE),
      trials_below_80_iti = sum(quality_iti < 0.80, na.rm = TRUE),
      trials_below_80_prestim = sum(quality_prestim < 0.80, na.rm = TRUE),
      trials_below_80_either = sum(quality_iti < 0.80 | quality_prestim < 0.80, na.rm = TRUE)
    )
  
  cat("**Quality Threshold:** 80% valid data required\n")
  cat("**Trials below threshold (ITI):**", quality_summary$trials_below_80_iti, "\n")
  cat("**Trials below threshold (PreStim):**", quality_summary$trials_below_80_prestim, "\n")
  cat("**Trials below threshold (either):**", quality_summary$trials_below_80_either, "\n")
  cat("**Note:** All trials in analysis-ready data should meet the 80% threshold\n")
}
```

### Quality Metrics Summary Table

```{r quality-summary-table}
if (!is.null(pupil_data)) {
  quality_summary %>%
    select(
      mean_quality_iti, sd_quality_iti, min_quality_iti, max_quality_iti,
      mean_quality_prestim, sd_quality_prestim, min_quality_prestim, max_quality_prestim
    ) %>%
    pivot_longer(everything(), names_to = "metric", values_to = "value") %>%
    separate(metric, into = c("stat", "period"), sep = "_", extra = "merge") %>%
    pivot_wider(names_from = stat, values_from = value) %>%
    kable(
      caption = "Quality metrics summary (all trials in analysis-ready data)",
      col.names = c("Period", "Mean", "SD", "Min", "Max"),
      digits = 3
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover"),
      full_width = FALSE
    )
}
```

## Quality Distribution Plots

```{r quality-plots}
if (!is.null(pupil_data)) {
  quality_long <- pupil_data %>%
    select(quality_iti, quality_prestim) %>%
    pivot_longer(everything(), names_to = "metric", values_to = "quality") %>%
    mutate(metric = case_when(
      metric == "quality_iti" ~ "ITI Baseline",
      metric == "quality_prestim" ~ "Pre-Stimulus"
    ))
  
  p1 <- ggplot(quality_long, aes(x = quality, fill = metric)) +
    geom_histogram(bins = 30, alpha = 0.7, position = "identity") +
    geom_vline(xintercept = 0.80, linetype = "dashed", color = "red", linewidth = 1) +
    labs(
      title = "Pupil Quality Metrics Distribution",
      subtitle = "80% threshold shown (all trials should be >= 0.80)",
      x = "Quality (proportion valid)",
      y = "Frequency",
      fill = "Metric"
    ) +
    scale_fill_manual(values = c("ITI Baseline" = "steelblue", "Pre-Stimulus" = "coral")) +
    theme_minimal() +
    theme(legend.position = "bottom")
  
  print(p1)
}
```

# Feature Extraction Summary

## AUC Metrics

```{r auc-summary}
if (!is.null(pupil_data)) {
  auc_summary <- pupil_data %>%
    summarise(
      # Total AUC
      total_auc_mean = mean(total_auc, na.rm = TRUE),
      total_auc_sd = sd(total_auc, na.rm = TRUE),
      total_auc_min = min(total_auc, na.rm = TRUE),
      total_auc_max = max(total_auc, na.rm = TRUE),
      total_auc_missing = sum(is.na(total_auc)),
      total_auc_valid = sum(!is.na(total_auc)),
      # Cognitive AUC
      cognitive_auc_mean = mean(cognitive_auc, na.rm = TRUE),
      cognitive_auc_sd = sd(cognitive_auc, na.rm = TRUE),
      cognitive_auc_min = min(cognitive_auc, na.rm = TRUE),
      cognitive_auc_max = max(cognitive_auc, na.rm = TRUE),
      cognitive_auc_missing = sum(is.na(cognitive_auc)),
      cognitive_auc_valid = sum(!is.na(cognitive_auc))
    )
  
  cat("**Total AUC:**\n")
  cat("  Mean:", round(auc_summary$total_auc_mean, 3), "\n")
  cat("  SD:", round(auc_summary$total_auc_sd, 3), "\n")
  cat("  Valid trials:", auc_summary$total_auc_valid, "/", nrow(pupil_data), "\n")
  
  cat("\n**Cognitive AUC:**\n")
  cat("  Mean:", round(auc_summary$cognitive_auc_mean, 3), "\n")
  cat("  SD:", round(auc_summary$cognitive_auc_sd, 3), "\n")
  cat("  Valid trials:", auc_summary$cognitive_auc_valid, "/", nrow(pupil_data), "\n")
}
```

### AUC Metrics Summary Table

```{r auc-summary-table}
if (!is.null(pupil_data)) {
  auc_table <- tibble(
    Metric = c("Total AUC", "Cognitive AUC"),
    Mean = c(
      round(auc_summary$total_auc_mean, 3),
      round(auc_summary$cognitive_auc_mean, 3)
    ),
    SD = c(
      round(auc_summary$total_auc_sd, 3),
      round(auc_summary$cognitive_auc_sd, 3)
    ),
    Min = c(
      round(auc_summary$total_auc_min, 3),
      round(auc_summary$cognitive_auc_min, 3)
    ),
    Max = c(
      round(auc_summary$total_auc_max, 3),
      round(auc_summary$cognitive_auc_max, 3)
    ),
    Valid = c(
      auc_summary$total_auc_valid,
      auc_summary$cognitive_auc_valid
    ),
    Missing = c(
      auc_summary$total_auc_missing,
      auc_summary$cognitive_auc_missing
    )
  )
  
  auc_table %>%
    kable(
      caption = "AUC metrics summary (Zenon et al. 2014 method)",
      col.names = c("Metric", "Mean", "SD", "Min", "Max", "Valid Trials", "Missing Trials")
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover"),
      full_width = FALSE
    )
}
```

## AUC by Condition

```{r auc-by-condition}
if (!is.null(pupil_data)) {
  auc_by_condition <- pupil_data %>%
    filter(!grepl("Standard", difficulty_level)) %>%
    mutate(condition = paste0(difficulty_level, " / ", 
                             ifelse(effort_condition == "Low_5_MVC", "Low", "High"))) %>%
    group_by(task, condition) %>%
    summarise(
      n_trials = n(),
      total_auc_mean = mean(total_auc, na.rm = TRUE),
      total_auc_se = sd(total_auc, na.rm = TRUE) / sqrt(n()),
      cognitive_auc_mean = mean(cognitive_auc, na.rm = TRUE),
      cognitive_auc_se = sd(cognitive_auc, na.rm = TRUE) / sqrt(n()),
      .groups = "drop"
    )
}
```

### AUC by Condition Table

```{r auc-condition-table}
if (!is.null(auc_by_condition)) {
  auc_by_condition %>%
    select(task, condition, n_trials, total_auc_mean, cognitive_auc_mean) %>%
    kable(
      caption = "Mean AUC by condition and task (Easy/Hard trials only)",
      col.names = c("Task", "Condition", "N Trials", "Total AUC (Mean)", "Cognitive AUC (Mean)"),
      digits = 3
    ) %>%
    kable_styling(
      bootstrap_options = c("striped", "hover"),
      full_width = FALSE
    )
}
```


# Visualizations

## Pupil Waveform Plots

The following plots show baseline-corrected pupil traces across conditions for ADT and VDT tasks. These plots are generated by `plot_pupil_waveforms.R` and include:

- Condition-specific averages (Easy/Low, Easy/High, Hard/Low, Hard/High)
- Smoothed lines with confidence intervals (GAM smoothing)
- Event markers: Trial onset, Target onset, Response
- Timeline bars: Baseline window, Total AUC window, Cognitive AUC window

```{r waveform-plots, fig.height=10, out.width="100%"}
# Check if waveform plot exists
waveform_file <- file.path(output_dir, "Figure3_Pupil_Waveforms_ADT_VDT.png")

if (file.exists(waveform_file)) {
  # Include the plot
  knitr::include_graphics(waveform_file)
} else {
  cat("**Note:** Waveform plots not found. Run `plot_pupil_waveforms.R` to generate them.\n")
  cat("Expected location:", waveform_file, "\n")
  
  # Try to generate the plot if data is available
  if (!is.null(pupil_data)) {
    cat("\n**Generating waveform plot from current data...**\n")
    tryCatch({
      source("visualization/plot_pupil_waveforms.R", local = TRUE)
      
      if (file.exists(waveform_file)) {
        knitr::include_graphics(waveform_file)
      }
    }, error = function(e) {
      cat("Error generating waveform plot:", e$message, "\n")
    })
  }
}
```

# Data Flow Summary

## Pipeline Stages

1. **Raw Flat Files**: Sample-level data from MATLAB pipeline
2. **Merged Flat Files**: Sample-level data merged with behavioral data
3. **Analysis-Ready Files**: Trial-level summaries with quality filtering (≥80% valid data)

## Filtering Steps

1. **Quality Filtering**: Only trials with ≥80% valid data in ITI and pre-stimulus periods
2. **Subject Inclusion**: All subjects with data are included (no run-based filtering)

## Output Files

- **Analysis-Ready Data**: `data/analysis_ready/BAP_analysis_ready_PUPIL.csv`
- **Behavioral Data**: `data/analysis_ready/BAP_analysis_ready_BEHAVIORAL.csv`
- **QC Reports**: `02_pupillometry_analysis/quality_control/output/`
- **Visualizations**: `06_visualization/publication_figures/`

# Summary

This report provides a comprehensive overview of the pupil data available in the BAP study. Key findings:

- **Total Subjects**: `r if(exists("subject_stats") && !is.null(subject_stats)) nrow(subject_stats) else if(exists("pupil_data") && !is.null(pupil_data)) length(unique(pupil_data$subject_id)) else "N/A"`
- **Total Trials**: `r if(exists("pupil_data") && !is.null(pupil_data)) nrow(pupil_data) else "N/A"`
- **Quality Threshold**: 80% valid data required
- **Primary Metrics**: Total AUC and Cognitive AUC (Zenon et al. 2014 method)

For detailed documentation, see:
- `02_pupillometry_analysis/README.md`
- `02_pupillometry_analysis/PIPELINE_GUIDE.md`
- `02_pupillometry_analysis/feature_extraction/AUC_CALCULATION_METHOD.md`

---

*Report generated on `r format(Sys.time(), '%Y-%m-%d %H:%M:%S')`*

# QC EXPORTS (CSV)

```{r qc-exports, include=FALSE}
if (isTRUE(params$export_qc_csvs)) {

  export_dir <- params$export_dir %||% "quality_control/exports"
  dir.create(export_dir, recursive = TRUE, showWarnings = FALSE)

  export_thresholds <- params$export_thresholds %||% c(0.50,0.60,0.70,0.80,0.90)
  export_default_thr <- params$export_default_thr %||% 0.60

  safe_write_csv <- function(df, path) {
    if (is.null(df) || (is.data.frame(df) && nrow(df) == 0)) return(invisible(FALSE))
    readr::write_csv(df, path)
    invisible(TRUE)
  }

  safe_write_csv_gz <- function(df, path) {
    if (is.null(df) || (is.data.frame(df) && nrow(df) == 0)) return(invisible(FALSE))
    readr::write_csv(df, path)  # write_csv supports .gz extension
    invisible(TRUE)
  }

  # ------------------------------------------------------------
  # 00) Data inventory (what files exist per subject/task)
  # ------------------------------------------------------------
  if (exists("file_inventory")) {
    inv <- file_inventory %>%
      dplyr::arrange(subject_id, file_type, filename)
    safe_write_csv(inv, file.path(export_dir, "00_data_inventory_file_inventory.csv"))
  }

  # ------------------------------------------------------------
  # Helper: gate flags at a given threshold (uses your existing function if present)
  # ------------------------------------------------------------
  if (!exists("gate_flags")) {
    stop("gate_flags() not found in environment. Ensure RAW coverage section ran before exports.")
  }
  if (!exists("trial_coverage_trial") || is.null(trial_coverage_trial)) {
    stop("trial_coverage_trial is missing. Ensure RAW coverage section ran before exports.")
  }

  # Use strict trial-level table (one row per trial_id)
  trial_cov <- trial_coverage_trial %>%
    dplyr::mutate(
      effort_condition = as.character(effort_condition),
      difficulty_level = as.character(difficulty_level),
      is_effort_unknown = is.na(effort_condition) | effort_condition %in% c(""),
      is_difficulty_unknown = is.na(difficulty_level) | difficulty_level %in% c("")
    )

  # ------------------------------------------------------------
  # 01) Trial-level prefilter coverage (1 row per trial) + gate flags at multiple thresholds
  # ------------------------------------------------------------
  gate_cols <- purrr::map_dfc(export_thresholds, function(thr) {
    gf <- gate_flags(trial_cov, thr)
    tibble::tibble(
      !!paste0("gateA_", sprintf("%03d", round(thr*100))) := gf$gate_A,
      !!paste0("gateB_", sprintf("%03d", round(thr*100))) := gf$gate_B,
      !!paste0("gateC_", sprintf("%03d", round(thr*100))) := gf$gate_C
    )
  })

  # Select columns that exist (using any_of to avoid errors if columns are missing)
  cols_to_select <- c(
    "subject_id", "task", "run", "trial_index", "trial_id",
    "effort_condition", "difficulty_level", "gf_trPer", "stimLev", "isOddball",
    "rt", "response_onset",
    "recording_max_time", "last_valid_time", "has_response_window",
    "valid_prop_baseline_500ms", "n_rows_baseline_500ms",
    "valid_prop_iti_full", "n_rows_iti_full",
    "valid_prop_prestim", "n_rows_prestim",
    "valid_prop_total_auc", "n_rows_total_auc",
    "valid_prop_cognitive_auc", "n_rows_cognitive_auc",
    "is_effort_unknown", "is_difficulty_unknown"
  )
  
  trial_cov_export <- dplyr::bind_cols(trial_cov, gate_cols) %>%
    dplyr::select(
      dplyr::any_of(cols_to_select),
      dplyr::starts_with("gateA_"), dplyr::starts_with("gateB_"), dplyr::starts_with("gateC_")
    ) %>%
    dplyr::arrange(subject_id, task, run, trial_index)

  safe_write_csv_gz(trial_cov_export, file.path(export_dir, "01_trial_coverage_prefilter.csv.gz"))

  # Small export audit for the report/logs
  n_trials_export  <- nrow(trial_cov_export)
  n_subjects_export <- dplyr::n_distinct(trial_cov_export$subject_id)
  n_subj_task_pairs <- dplyr::n_distinct(interaction(trial_cov_export$subject_id, trial_cov_export$task, drop = TRUE))
  message(
    sprintf(
      "QC export audit — trials: %d, subjects: %d, subject-task pairs: %d",
      n_trials_export, n_subjects_export, n_subj_task_pairs
    )
  )

  # ------------------------------------------------------------
  # 02) Subject-task "report card" (decision-grade summary at multiple thresholds)
  #     Uses strict trial-level table and adds baseline-only gate variants
  # ------------------------------------------------------------
  subject_task_summary <- purrr::map_dfr(export_thresholds, function(thr) {
    gf <- gate_flags(trial_cov, thr)

    tmp <- trial_cov %>%
      dplyr::mutate(
        gateA = gf$gate_A,
        gateB = gf$gate_B,
        gateC = gf$gate_C,
        thr = thr,
        has_rt = !is.na(rt),
        gateC_baseline_only = !is.na(valid_prop_baseline_500ms) &
          valid_prop_baseline_500ms >= thr &
          !is.na(valid_prop_cognitive_auc) &
          valid_prop_cognitive_auc >= thr &
          has_response_window & has_rt,
        gateB_baseline_only = !is.na(valid_prop_baseline_500ms) &
          valid_prop_baseline_500ms >= thr &
          !is.na(valid_prop_total_auc) &
          valid_prop_total_auc >= thr &
          has_response_window & has_rt
      ) %>%
      dplyr::group_by(subject_id, task, thr) %>%
      dplyr::summarise(
        n_trials_prefilter = dplyr::n_distinct(trial_id),

        n_has_response_window = sum(has_response_window, na.rm = TRUE),
        pct_has_response_window = dplyr::if_else(
          n_trials_prefilter > 0,
          n_has_response_window / n_trials_prefilter,
          NA_real_
        ),

        n_effort_unknown = sum(is_effort_unknown, na.rm = TRUE),
        n_difficulty_unknown = sum(is_difficulty_unknown, na.rm = TRUE),

        n_gateA = sum(gateA, na.rm = TRUE),
        n_gateB = sum(gateB, na.rm = TRUE),
        n_gateC = sum(gateC, na.rm = TRUE),
        n_gateC_baseline_only = sum(gateC_baseline_only, na.rm = TRUE),
        n_gateB_baseline_only = sum(gateB_baseline_only, na.rm = TRUE),

        med_last_valid_time = median(last_valid_time, na.rm = TRUE),
        p10_last_valid_time = stats::quantile(last_valid_time, 0.10, na.rm = TRUE, names = FALSE),
        p90_last_valid_time = stats::quantile(last_valid_time, 0.90, na.rm = TRUE, names = FALSE),

        med_valid_baseline = median(valid_prop_baseline_500ms, na.rm = TRUE),
        med_valid_prestim  = median(valid_prop_prestim, na.rm = TRUE),
        med_valid_total    = median(valid_prop_total_auc, na.rm = TRUE),
        med_valid_cognitive= median(valid_prop_cognitive_auc, na.rm = TRUE),

        .groups = "drop"
      )

    tmp
  }) %>%
    dplyr::rename(threshold = thr) %>%
    dplyr::arrange(subject_id, task, threshold)

  safe_write_csv(subject_task_summary, file.path(export_dir, "02_subject_task_report_card.csv"))

  # ------------------------------------------------------------
  # 03) Threshold sweep long (subject x task x effort x difficulty x threshold x gate)
  # ------------------------------------------------------------
  if (exists("threshold_sweep") && !is.null(threshold_sweep)) {
    sweep_long <- threshold_sweep %>%
      dplyr::arrange(subject_id, task, effort_condition, difficulty_level, threshold, gate)
    safe_write_csv_gz(sweep_long, file.path(export_dir, "03_threshold_sweep_long.csv.gz"))
  } else if (exists("threshold_sweep_cache_file") && file.exists(threshold_sweep_cache_file)) {
    sweep_long <- readRDS(threshold_sweep_cache_file) %>%
      dplyr::arrange(subject_id, task, effort_condition, difficulty_level, threshold, gate)
    safe_write_csv_gz(sweep_long, file.path(export_dir, "03_threshold_sweep_long.csv.gz"))
  }

  # ------------------------------------------------------------
  # 04) Threshold sweep subject summary (sum across conditions)
  # ------------------------------------------------------------
  if (exists("sweep_long") && !is.null(sweep_long)) {
    # Base subject × task × threshold summary from sweep_long (one row per gate)
    sweep_subject_long <- sweep_long %>%
      dplyr::group_by(subject_id, task, threshold, gate) %>%
      dplyr::summarise(
        n_trials_retained = sum(n_trials_retained, na.rm = TRUE),
        .groups = "drop"
      ) %>%
      dplyr::arrange(subject_id, task, threshold, gate)

    # Pivot to wide format so each subject × task × threshold is a single row
    sweep_subject_wide <- sweep_subject_long %>%
      tidyr::pivot_wider(
        names_from = gate,
        values_from = n_trials_retained,
        names_prefix = "n_trials_"
      )

    # Chapter 2 feasibility dashboard metrics, computed from strict trial-level table
    ch2_feasibility <- purrr::map_dfr(export_thresholds, function(thr) {
      df_thr <- trial_coverage_trial %>%
        dplyr::mutate(
          baseline_ok   = !is.na(valid_prop_baseline_500ms)   & valid_prop_baseline_500ms   >= thr,
          prestim_ok    = !is.na(valid_prop_prestim)          & valid_prop_prestim          >= thr,
          cognitive_ok  = !is.na(valid_prop_cognitive_auc)    & valid_prop_cognitive_auc    >= thr,
          has_rt        = !is.na(rt),
          has_resp_win  = isTRUE(has_response_window),
          gateC_T = baseline_ok & prestim_ok & cognitive_ok,
          gateC_baseline_only_T = baseline_ok & cognitive_ok & has_resp_win & has_rt
        )

      # Known-label trials under baseline-only Cognitive-AUC gate
      df_known <- df_thr %>%
        dplyr::filter(
          gateC_baseline_only_T,
          !is.na(effort_condition),
          effort_condition != "",
          effort_condition != "Unknown",
          !is.na(difficulty_level),
          difficulty_level != "",
          difficulty_level != "Unknown"
        )

      # Per-subject, per-task metrics
      by_subj_task <- df_thr %>%
        dplyr::group_by(subject_id, task) %>%
        dplyr::summarise(
          n_gateC_T = sum(gateC_T, na.rm = TRUE),
          n_gateC_baseline_only_T = sum(gateC_baseline_only_T, na.rm = TRUE),
          .groups = "drop"
        )

      known_counts <- df_known %>%
        dplyr::group_by(subject_id, task) %>%
        dplyr::summarise(
          n_known_labels_in_gateC_baseline_only_T = dplyr::n_distinct(trial_id),
          .groups = "drop"
        )

      # Cell counts across effort × difficulty (all known labels)
      cell_counts <- df_known %>%
        dplyr::group_by(subject_id, task, effort_condition, difficulty_level) %>%
        dplyr::summarise(
          n_cell = dplyr::n_distinct(trial_id),
          .groups = "drop"
        )

      min_cell <- cell_counts %>%
        dplyr::group_by(subject_id, task) %>%
        dplyr::summarise(
          min_cell_count = min(n_cell, na.rm = TRUE),
          .groups = "drop"
        )

      # 2x2 cell counts: effort × {Easy, Hard}
      cell_counts_2x2 <- df_known %>%
        dplyr::filter(difficulty_level %in% c("Easy", "Hard")) %>%
        dplyr::group_by(subject_id, task, effort_condition, difficulty_level) %>%
        dplyr::summarise(
          n_cell = dplyr::n_distinct(trial_id),
          .groups = "drop"
        )

      min_cell_2x2 <- cell_counts_2x2 %>%
        dplyr::group_by(subject_id, task) %>%
        dplyr::summarise(
          min_cell_count_2x2 = min(n_cell, na.rm = TRUE),
          .groups = "drop"
        )

      feas <- by_subj_task %>%
        dplyr::left_join(known_counts, by = c("subject_id", "task")) %>%
        dplyr::left_join(min_cell, by = c("subject_id", "task")) %>%
        dplyr::left_join(min_cell_2x2, by = c("subject_id", "task")) %>%
        dplyr::mutate(
          threshold = thr,
          n_known_labels_in_gateC_baseline_only_T = n_known_labels_in_gateC_baseline_only_T %||% 0L,
          feasible_for_ch2_continuous = n_known_labels_in_gateC_baseline_only_T >= 30,
          feasible_for_ch2_tertiles  = n_known_labels_in_gateC_baseline_only_T >= 45
        )

      feas
    })

    sweep_subject <- sweep_subject_wide %>%
      dplyr::left_join(
        ch2_feasibility,
        by = c("subject_id", "task", "threshold")
      ) %>%
      dplyr::arrange(subject_id, task, threshold)

    safe_write_csv(sweep_subject, file.path(export_dir, "04_threshold_sweep_subject_summary.csv"))
  }

  # ------------------------------------------------------------
  # 05) Loss reasons long (trial-level, for Total-AUC gate and Cognitive-AUC gate)
  #      - Overall by threshold × gate × first_fail_reason
  #      - Plus readable breakdown by task and condition (known labels only)
  # ------------------------------------------------------------
  label_failure_flags <- function(df, thr) {
    df %>%
      dplyr::mutate(
        fail_baseline500 = is.na(valid_prop_baseline_500ms) | valid_prop_baseline_500ms < thr,
        fail_prestim     = is.na(valid_prop_prestim)        | valid_prop_prestim        < thr,
        fail_missing_rt  = is.na(rt) | rt <= 0 | rt >= 5.0,
        fail_total_auc   = is.na(valid_prop_total_auc)      | valid_prop_total_auc      < thr,
        fail_cognitive_auc = is.na(valid_prop_cognitive_auc) | valid_prop_cognitive_auc < thr
      )
  }

  compute_first_fail <- function(df_flags, gate) {
    # Gate-specific first-fail logic, ordered to show where trials "bottleneck"
    if (gate == "gateC") {
      df_flags %>%
        dplyr::mutate(
          gate = "gateC",
          first_fail_reason = dplyr::case_when(
            fail_baseline500   ~ "fail_baseline500",
            fail_prestim       ~ "fail_prestim",
            fail_missing_rt    ~ "fail_missing_rt",
            fail_cognitive_auc ~ "fail_cognitive_auc",
            fail_total_auc     ~ "fail_total_auc",
            TRUE               ~ "pass"
          )
        )
    } else if (gate == "gateB") {
      df_flags %>%
        dplyr::mutate(
          gate = "gateB",
          first_fail_reason = dplyr::case_when(
            fail_baseline500 ~ "fail_baseline500",
            fail_prestim     ~ "fail_prestim",
            fail_missing_rt  ~ "fail_missing_rt",
            fail_total_auc   ~ "fail_total_auc",
            TRUE             ~ "pass"
          )
        )
    } else {
      df_flags
    }
  }

  loss_reasons_long <- purrr::map_dfr(export_thresholds, function(thr) {
    # Work on strict trial-level table to ensure 1 row per trial
    df_flags <- label_failure_flags(trial_cov, thr)

    # Overall summaries (by threshold × gate × first_fail_reason)
    overall_gateC <- compute_first_fail(df_flags, "gateC") %>%
      dplyr::group_by(threshold = !!thr, gate, first_fail_reason) %>%
      dplyr::summarise(
        n_trials = dplyr::n_distinct(trial_id),
        .groups = "drop"
      )

    overall_gateB <- compute_first_fail(df_flags, "gateB") %>%
      dplyr::group_by(threshold = !!thr, gate, first_fail_reason) %>%
      dplyr::summarise(
        n_trials = dplyr::n_distinct(trial_id),
        .groups = "drop"
      )

    overall <- dplyr::bind_rows(overall_gateC, overall_gateB) %>%
      dplyr::group_by(threshold, gate) %>%
      dplyr::mutate(
        total_trials_gate = sum(n_trials, na.rm = TRUE),
        prop_trials = ifelse(
          total_trials_gate > 0,
          n_trials / total_trials_gate,
          NA_real_
        ),
        # No task / condition stratification for these rows
        task = NA_character_,
        effort_condition = NA_character_,
        difficulty_level = NA_character_,
        level = "overall"
      ) %>%
      dplyr::ungroup() %>%
      dplyr::select(-total_trials_gate)

    # Stratified summaries by task × effort × difficulty (known labels only)
    df_labeled <- df_flags %>%
      dplyr::filter(
        !is.na(effort_condition),
        effort_condition != "",
        effort_condition != "Unknown",
        !is.na(difficulty_level),
        difficulty_level != "",
        difficulty_level != "Unknown"
      )

    strat_gateC <- compute_first_fail(df_labeled, "gateC") %>%
      dplyr::group_by(
        threshold = !!thr, gate, task, effort_condition, difficulty_level, first_fail_reason
      ) %>%
      dplyr::summarise(
        n_trials = dplyr::n_distinct(trial_id),
        .groups = "drop"
      )

    strat_gateB <- compute_first_fail(df_labeled, "gateB") %>%
      dplyr::group_by(
        threshold = !!thr, gate, task, effort_condition, difficulty_level, first_fail_reason
      ) %>%
      dplyr::summarise(
        n_trials = dplyr::n_distinct(trial_id),
        .groups = "drop"
      )

    stratified <- dplyr::bind_rows(strat_gateC, strat_gateB) %>%
      dplyr::group_by(threshold, gate, task, effort_condition, difficulty_level) %>%
      dplyr::mutate(
        total_trials_cell = sum(n_trials, na.rm = TRUE),
        prop_trials = ifelse(
          total_trials_cell > 0,
          n_trials / total_trials_cell,
          NA_real_
        ),
        level = "by_task_condition"
      ) %>%
      dplyr::ungroup() %>%
      dplyr::select(-total_trials_cell)

    dplyr::bind_rows(overall, stratified)
  }) %>%
    dplyr::arrange(threshold, gate, level, task, effort_condition, difficulty_level, first_fail_reason)

  safe_write_csv(loss_reasons_long, file.path(export_dir, "05_loss_reasons_long.csv"))

  # ------------------------------------------------------------
  # 06-07) Time availability exports (stimulus-locked and response-locked), long format
  # ------------------------------------------------------------
  # These objects should exist if the time-resolved availability section ran.
  # If not, we still export nothing rather than failing hard.

  if (exists("availability_summary") && !is.null(availability_summary)) {
    avail_effort <- availability_summary %>%
      dplyr::mutate(stratifier = "effort", level = as.character(effort_condition)) %>%
      dplyr::select(task, time_bin, stratifier, level, n_trials, availability)
  } else {
    avail_effort <- tibble::tibble()
  }

  if (exists("availability_summary_diff") && !is.null(availability_summary_diff)) {
    avail_diff <- availability_summary_diff %>%
      dplyr::mutate(stratifier = "difficulty", level = as.character(difficulty_level)) %>%
      dplyr::select(task, time_bin, stratifier, level, n_trials, availability)
  } else {
    avail_diff <- tibble::tibble()
  }

  avail_stim_locked <- dplyr::bind_rows(avail_effort, avail_diff) %>%
    dplyr::arrange(task, stratifier, level, time_bin)

  safe_write_csv(avail_stim_locked, file.path(export_dir, "06_availability_stimulus_locked_long.csv"))

  if (exists("response_locked") && !is.null(response_locked)) {
    resp_effort <- response_locked %>%
      dplyr::mutate(stratifier = "effort", level = as.character(effort_condition)) %>%
      dplyr::select(task, t_resp, stratifier, level, n_trials, availability)
  } else {
    resp_effort <- tibble::tibble()
  }

  if (exists("response_locked_diff") && !is.null(response_locked_diff)) {
    resp_diff <- response_locked_diff %>%
      dplyr::mutate(stratifier = "difficulty", level = as.character(difficulty_level)) %>%
      dplyr::select(task, t_resp, stratifier, level, n_trials, availability)
  } else {
    resp_diff <- tibble::tibble()
  }

  avail_resp_locked <- dplyr::bind_rows(resp_effort, resp_diff) %>%
    dplyr::arrange(task, stratifier, level, t_resp)

  safe_write_csv(avail_resp_locked, file.path(export_dir, "07_availability_response_locked_long.csv"))

  # ------------------------------------------------------------
  # 08) Analysis-ready subject stats (post-filter summary)
  # ------------------------------------------------------------
  if (exists("subject_stats") && !is.null(subject_stats)) {
    safe_write_csv(subject_stats, file.path(export_dir, "08_analysis_ready_subject_stats.csv"))
  }

  message("QC exports written to: ", export_dir)
}

```

## Data Integrity Checks

```{r data-integrity-checks, include=FALSE}
if (isTRUE(params$export_qc_csvs)) {
  export_dir <- params$export_dir %||% "quality_control/exports"

  # ------------------------------------------------------------
  # 1) 01_trial_coverage_prefilter: trial_id must be unique
  # ------------------------------------------------------------
  path01 <- file.path(export_dir, "01_trial_coverage_prefilter.csv.gz")
  if (file.exists(path01)) {
    df01 <- readr::read_csv(path01, show_col_types = FALSE, progress = FALSE)

    if (!"trial_id" %in% names(df01)) {
      stop("Data integrity check failed: 01_trial_coverage_prefilter.csv.gz is missing 'trial_id'.")
    }

    if (any(duplicated(df01$trial_id))) {
      dup_ids <- df01$trial_id[duplicated(df01$trial_id)] %>% unique()
      stop(
        "Data integrity check failed: 01_trial_coverage_prefilter.csv.gz has duplicated trial_id. ",
        "Examples: ", paste(utils::head(dup_ids, 5), collapse = ", "), "."
      )
    }
  }

  # ------------------------------------------------------------
  # 2–3) 04_threshold_sweep_subject_summary: gate counts must not exceed total trials
  # ------------------------------------------------------------
  path04 <- file.path(export_dir, "04_threshold_sweep_subject_summary.csv")
  if (file.exists(path04) && exists("trial_coverage_trial") && !is.null(trial_coverage_trial)) {
    sweep04 <- readr::read_csv(path04, show_col_types = FALSE, progress = FALSE)

    base_trials <- trial_coverage_trial %>%
      dplyr::group_by(subject_id, task) %>%
      dplyr::summarise(
        n_trials_total = dplyr::n_distinct(trial_id),
        .groups = "drop"
      )

    sweep04_chk <- sweep04 %>%
      dplyr::left_join(base_trials, by = c("subject_id", "task"))

    if (any(is.na(sweep04_chk$n_trials_total))) {
      bad_rows <- sweep04_chk %>% dplyr::filter(is.na(n_trials_total))
      stop(
        "Data integrity check failed: 04_threshold_sweep_subject_summary.csv contains subject-task combinations ",
        "not found in trial_coverage_trial. Examples: ",
        paste(
          utils::head(
            paste0(bad_rows$subject_id, ":", bad_rows$task),
            5
          ),
          collapse = ", "
        ),
        "."
      )
    }

    gate_cols <- c(
      "n_trials_gateA", "n_trials_gateB", "n_trials_gateC",
      "n_gateC_T", "n_gateC_baseline_only_T",
      "n_known_labels_in_gateC_baseline_only_T"
    )
    gate_cols <- intersect(gate_cols, names(sweep04_chk))

    if (length(gate_cols) > 0) {
      bad_counts <- sweep04_chk %>%
        dplyr::filter(dplyr::if_any(dplyr::all_of(gate_cols), ~ .x > n_trials_total))

      if (nrow(bad_counts) > 0) {
        example <- bad_counts[1, , drop = FALSE]
        stop(
          "Data integrity check failed: gate counts in 04_threshold_sweep_subject_summary.csv exceed total trials ",
          "for at least one subject-task-threshold. Example: subject_id=",
          example$subject_id, ", task=", example$task, ", threshold=", example$threshold, "."
        )
      }
    }
  }

  # ------------------------------------------------------------
  # 4) 05_loss_reasons_long: totals must equal total trial count at each threshold × gate
  # ------------------------------------------------------------
  path05 <- file.path(export_dir, "05_loss_reasons_long.csv")
  if (file.exists(path05) && exists("trial_coverage_trial") && !is.null(trial_coverage_trial)) {
    loss <- readr::read_csv(path05, show_col_types = FALSE, progress = FALSE)

    required_cols <- c("threshold", "gate", "first_fail_reason", "n_trials", "level")
    if (!all(required_cols %in% names(loss))) {
      missing_cols <- setdiff(required_cols, names(loss))
      stop(
        "Data integrity check failed: 05_loss_reasons_long.csv is missing required column(s): ",
        paste(missing_cols, collapse = ", "),
        "."
      )
    }

    total_trials <- nrow(trial_coverage_trial)

    overall <- loss %>%
      dplyr::filter(level == "overall") %>%
      dplyr::group_by(threshold, gate) %>%
      dplyr::summarise(
        n_sum = sum(n_trials, na.rm = TRUE),
        .groups = "drop"
      )

    bad_totals <- overall %>%
      dplyr::filter(n_sum != total_trials)

    if (nrow(bad_totals) > 0) {
      example <- bad_totals[1, , drop = FALSE]
      stop(
        "Data integrity check failed: loss reasons totals in 05_loss_reasons_long.csv do not match total trial count ",
        "for threshold=", example$threshold, ", gate=", example$gate, 
        ". Expected ", total_trials, " trials, got ", example$n_sum, "."
      )
    }
  }
}
```

